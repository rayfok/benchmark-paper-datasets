What is the seed lexicon?
What are the results?
How are relations used to propagate polarity?
How big is the Japanese data?
What are labels available in dataset for supervision?
How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?
How does their model learn using mostly raw data?
How big is seed lexicon used for training?
How large is raw corpus used for training?

Does the paper report macro F1?
How is the annotation experiment evaluated?
What are the aesthetic emotions formalized?

Do they report results only on English data?
How do the various social phenomena examined manifest in different types of communities?
What patterns do they observe about how user engagement varies with the characteristics of a community?
How did the select the 300 Reddit communities for comparison?
How do the authors measure how temporally dynamic a community is?
How do the authors measure how distinctive a community is?

What data is the language model pretrained on?
What baselines is the proposed model compared against?
How is the clinical text structuring task defined?
What are the specific tasks being unified?
Is all text in this dataset a question, or are there unrelated sentences in between questions?
How many questions are in the dataset?
What is the perWhat are the tasks evaluated?
Are there privacy concerns with clinical data?
How they introduce domain-specific features into pre-trained language model?
How big is QA-CTS task dataset?
How big is dataset of pathology reports collected from Ruijing Hospital?
What are strong baseline models in specific tasks?

What aspects have been compared between various language models?
what classic language models are mentioned in the paper?
What is a commonly used evaluation metric for language models?

Which dataset do they use a starting point in generating fake reviews?
Do they use a pretrained NMT model to help generating reviews?
How does using NMT ensure generated reviews stay on topic?
What kind of model do they use for detection?
Does their detection tool work better than human detection?
How many reviews in total (both generated and true) do they evaluate on Amazon Mechanical Turk?

Which baselines did they compare?
How many attention layers are there in their model?
Is the explanation from saliency map correct?

How is embedding quality assessed?
What are the three measures of bias which are reduced in experiments?
What are the probabilistic observations which contribute to the more robust algorithm?

What turn out to be more important high volume or high quality data?
How much is model improved by massive data and how much by quality?
What two architectures are used?

Does this paper target European or Brazilian Portuguese?
What were the word embeddings trained on?
Which word embeddings are analysed?

Did they experiment on this dataset?
How is quality of the citation measured?
How big is the dataset?

Do they evaluate only on English datasets?
Do the authors mention any possible confounds in this study?
How is the intensity of the PTSD established?
How is LIWC incorporated into this system?
How many twitter users are surveyed using the clinically validated survey?
Which clinically validated survey tools are used?

Did they experiment with the dataset?
What is the size of this dataset?
Do they list all the named entity types present?

how is quality measured?
how many languages exactly is the sentiment lexica for?
what sentiment sources do they compare with?

Is the method described in this work a clustering-based method?
How are the different senses annotated/labeled?
Was any extrinsic evaluation carried out?

Does the model use both spectrogram images and raw waveforms as features?
Is the performance compared against a baseline model?
What is the accuracy reported by state-of-the-art methods?

Which vision-based approaches does this approach outperform?
What baseline is used for the experimental setup?
Which languages are used in the multi-lingual caption model?

Did they experiment on all the tasks?
What models did they compare to?
What datasets are used in training?

Which GAN do they use?
Do they evaluate grammaticality of generated text?
Which corpora do they use?

Do they report results only on English datasets?
How do the authors define or exemplify 'incorrect words'?
How many vanilla transformers do they use after applying an embedding layer?
Do they test their approach on a dataset without incomplete data?
Should their approach be applied only when dealing with incomplete data?
By how much do they outperform other models in the sentiment in intent classification tasks?

What is the sample size of people used to measure user satisfaction?
What are all the metrics to measure user engagement?
What the system designs introduced?
Do they specify the model they use for Gunrock?
Do they gather explicit user satisfaction data on Gunrock?
How do they correlate user backstory queries to user satisfaction?

Do the authors report only on English?
What is the baseline for the experiments?
Which experiments are perfomed?

Is ROUGE their only baseline?
what language models do they use?
what questions do they ask human judges?

What misbehavior is identified?
What is the baseline used?
Which attention mechanisms do they compare?

Which paired corpora did they use in the other experiment?
By how much does their system outperform the lexicon-based models?
Which lexicon-based models did they compare with?
How many comments were used?
How many articles did they have?
What news comment dataset was used?

By how much do they outperform standard BERT?
What dataset do they use?
How do they combine text representations with the knowledge graph embeddings?

What is the algorithm used for the classification tasks?
Is the outcome of the LDA analysis evaluated in any way?
What is the corpus used in the study?

What are the traditional methods to identifying important attributes?
What do you use to calculate word/sub-word embeddings
What user generated text data do you use?

Did they propose other metrics?
Which real-world datasets did they use?
How did they obtain human intuitions?

What are the country-specific drivers of international development rhetoric?
Is the dataset multilingual?
How are the main international development topics that states raise identified?

What experiments do the authors present to validate their system?
How does the conversation layer work?
What components is the QnAMaker composed of?

How they measure robustness in experiments?
Is new method inferior in terms of robustness to MIRAs in experiments?
What experiments with large-scale features are performed?

Which ASR system(s) is used in this work?
What are the series of simple models?
Over which datasets/corpora is this work evaluated?

Is the semantic hierarchy representation used for any task?
What are the corpora used for the task?
Is the model evaluated?

What new metrics are suggested to track progress?
What intrinsic evaluation metrics are used?
What experimental results suggest that using less than 50% of the available training examples might result in overfitting?

What multimodality is available in the dataset?
What are previously reported models?
How better is accuracy of new model compared to previously reported models?

How does the scoring model work?
How does the active learning model work?
Which neural network architectures are employed?

What are the key points in the role of script knowledge that can be studied?
Did the annotators agreed and how much?
How many subjects have been used to create the annotations?

What datasets are used to evaluate this approach?
How is this approach used to detect incorrect facts?
Can this adversarial approach be used to directly improve model accuracy?

what are the advantages of the proposed model?
what are the state of the art approaches?
what datasets were used?

How was the dataset collected?
What are the benchmark models?
How was the corpus annotated?

What models other than standalone BERT is new model compared to?
How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?
What are three downstream task datasets?
What is dataset for word probing task?

How fast is the model compared to baselines?
How big is the performance difference between this method and the baseline?
What datasets used for evaluation?
what are the mentioned cues?

How did the author's work rank among other submissions on the challenge?
What approaches without reinforcement learning have been tried?
What classification approaches were experimented for this task?
Did classification models perform better than previous regression one?

What are the main sources of recall errors in the mapping?
Do they look for inconsistencies between different languages' annotations in UniMorph?
Do they look for inconsistencies between different UD treebanks?
Which languages do they validate on?

Does the paper evaluate any adjustment to improve the predicion accuracy of face and audio features?
How is face and audio data analysis evaluated?
What is the baseline method for the task?
What are the emotion detection tools used for audio and face input?

what amounts of size were used on german-english?
what were their experimental results in the low-resource dataset?
what are the methods they compare with in the korean-english dataset?
what pitfalls are mentioned in the paper?

Does the paper report the results of previous models applied to the same tasks?
How is the quality of the discussion evaluated?
What is the technique used for text analysis and mining?
What are the causal mapping methods employed?

What is the previous work's model?
What dataset is used?
How big is the dataset?
How is the dataset collected?
Was each text augmentation technique experimented individually?
What models do previous work use?
Does the dataset contain content from various social media platforms?
What dataset is used?

How they demonstrate that language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment?
Are language-specific and language-neutral components disjunctive?
How they show that mBERT representations can be split into a language-specific component and a language-neutral component?
What challenges this work presents that must be solved to build better language-neutral representations?

What is the performance of their system?
What evaluation metrics are used?
What is the source of the dialogues?
What pretrained LM is used?

What approaches they propose?
What faithfulness criteria does they propose?
Which are three assumptions in current approaches for defining faithfulness?
Which are key points in guidelines for faithfulness evaluation?

Did they use the state-of-the-art model to analyze the attention?
What is the performance of their model?
How many layers are there in their model?
Did they compare with gradient-based methods?

What MC abbreviate for?
how much of improvement the adaptation model can get?
what is the architecture of the baseline model?
What is the exact performance on SQUAD?

What are their correlation results?
What dataset do they use?
What simpler models do they look at?
What linguistic quality aspects are addressed?

What benchmark datasets are used for the link prediction task?
What are state-of-the art models for this task?
How better does HAKE model peform than state-of-the-art methods?
How are entities mapped onto polar coordinate system?

What additional techniques are incorporated?
What dataset do they use?
Do they compare to other models?
What is the architecture of the system?
How long are expressions in layman's language?
What additional techniques could be incorporated to further improve accuracy?
What programming language is target language?
What dataset is used to measure accuracy?

Is text-to-image synthesis trained is suppervized or unsuppervized manner?
What challenges remain unresolved?
What is the conclusion of comparison of proposed solution?
What is typical GAN architecture for each text-to-image synhesis group?

Where do they employ feature-wise sigmoid gating?
Which model architecture do they use to obtain representations?
Which downstream sentence-level tasks do they evaluate on?
Which similarity datasets do they use?

Are there datasets with relation tuples annotated, how big are datasets available?
Which one of two proposed approaches performed better in experiments?
What is previous work authors reffer to?
How higher are F1 scores compared to previous work?

what were the baselines?
what is the supervised model they developed?
what is the size of this built corpus?
what crowdsourcing platform is used?

Which deep learning model performed better?
By how much did the results improve?
What was their performance on the dataset?
How large is the dataset?

Did the authors use crowdsourcing platforms?
How was the dataset collected?
What language do the agents talk in?
What evaluation metrics did the authors look at?
What data did they use?

Do the authors report results only on English data?
How is the accuracy of the system measured?
How is an incoming claim used to retrieve similar factchecked claims?
What existing corpus is used for comparison in these experiments?
What are the components in the factchecking algorithm?

What is the baseline?
What dataset was used in the experiment?
Did they use any crowdsourcing platform?
How was the dataset annotated?
What is the source of the proposed dataset?

How many label options are there in the multi-label task?
What is the interannotator agreement of the crowd sourced users?
Who are the experts?
Who is the crowd in these experiments?
How do you establish the ground truth of who won a debate?

How much better is performance of proposed method than state-of-the-art methods in experiments?
What further analysis is done?
What seven state-of-the-art methods are used for comparison?
What three datasets are used to measure performance?
How does KANE capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner?
What are recent works on knowedge graph embeddings authors mention?

Do they report results only on English data?
Do the authors mention any confounds to their study?
What baseline model is used?
What stylistic features are used to detect drunk texts?
Is the data acquired under distant supervision verified by humans at any stage?
What hashtags are used for distant supervision?
Do the authors equate drunk tweeting with drunk texting?

What corpus was the source of the OpenIE extractions?
What is the accuracy of the proposed technique?
Is an entity linking process used?
Are the OpenIE extractions all triples?
What method was used to generate the OpenIE extractions?
Can the method answer multi-hop questions?
What was the textual source to which OpenIE was applied?
What OpenIE method was used to generate the extractions?
Is their method capable of multi-hop reasoning?

Do the authors offer any hypothesis about why the dense mode outperformed the sparse one?
What evaluation is conducted?
Which corpus of synsets are used?
What measure of semantic similarity is used?

Which retrieval system was used for baselines?

What word embeddings were used?
What type of errors were produced by the BLSTM-CNN-CRF system?
How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?

What supplemental tasks are used for multitask learning?
Is the improvement actually coming from using an RNN?
How much performance gap between their approach and the strong handcrafted method?
What is a strong feature-based method?
Did they experimnet in other languages?

Do they use multi-attention heads?
How big is their model?
How is their model different from BERT?

What datasets were used?
How did they do compared to other teams?

Which tested technique was the worst performer?
How many emotions do they look at?
What are the baseline benchmarks?
What is the size of this dataset?
How many annotators were there?

Can SCRF be used to pretrain the model?

What conclusions are drawn from the syntactic analysis?
What type of syntactic analysis is performed?
How is it demonstrated that the correct gender and number information is injected using this system?
Which neural machine translation system is used?
What are the components of the black-box context injection system?

What normalization techniques are mentioned?
What features do they experiment with?
Which architecture is their best model?
What kind of spontaneous speech is used?

What approach did previous models use for multi-span questions?
How they use sequence tagging to answer multi-span questions?
What is difference in peformance between proposed model and state-of-the art on other question types?
What is the performance of proposed model on entire DROP dataset?
What is the previous model that attempted to tackle multi-span questions as a part of its design?

How much more data does the model trained using XR loss have access to, compared to the fully supervised model?
Does the system trained only using XR loss outperform the fully supervised neural system?
How accurate is the aspect based sentiment classifier trained only using the XR loss?
How is the expectation regularization loss defined?

What were the non-neural baselines used for the task?

Which publicly available NLU dataset is used?
What metrics other than entity tagging are compared?

Do they provide decision sequences as supervision while training models?
What are the models evaluated on?
How do they train models in this setup?
What commands does their setup provide to models seeking information?

What models do they propose?
Are all tweets in English?
How large is the dataset?
What is the results of multimodal compared to unimodal models?
What is author's opinion on why current multimodal models cannot outperform models analyzing only text?
What metrics are used to benchmark the results?
How is data collected, manual collection or Twitter api?
How many tweats does MMHS150k contains, 150000?
What unimodal detection models were used?
What different models for multimodal detection were proposed?
What annotations are available in the dataset - tweat used hate speach or not?

What were the evaluation metrics used?
What were their performance results?
By how much did they outperform the other methods?
Which popular clustering methods did they experiment with?
What datasets did they use?

Does pre-training on general text corpus improve performance?
What neural configurations are explored?
Are the Transformers masked?
How is this problem evaluated?
What datasets do they use?

What evaluation metrics were used?
Where did the real production data come from?
What feedback labels are used?

What representations for textual documents do they use?
Which dataset(s) do they use?
How do they evaluate knowledge extraction performance?

What is CamemBERT trained on?
Which tasks does CamemBERT not improve on?
What is the state of the art?
How much better was results of CamemBERT than previous results on these tasks?
Was CamemBERT compared against multilingual BERT on these tasks?
How long was CamemBERT trained?
What data is used for training CamemBERT?

What are the state of the art measures?
What controversial topics are experimented with?
What datasets did they use?
What social media platform is observed?
How many languages do they experiment with?

What is the current SOTA for sentiment analysis on Twitter at the time of writing?
What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?
What are the metrics to evaluate sentiment analysis on Twitter?

How many sentence transformations on average are available per unique sentence in dataset?
What annotations are available in the dataset?
How are possible sentence transformations represented in dataset, as new sentences?
What are all 15 types of modifications ilustrated in the dataset?
Is this dataset publicly available?
Are some baseline models trained on this dataset?
Do they do any analysis of of how the modifications changed the starting set of sentences?
How do they introduce language variation?
Do they use external resources to make modifications to sentences?

How big is dataset domain-specific embedding are trained on?
How big is unrelated corpus universal embedding is traned on?
How better are state-of-the-art results than this model?

What were their results on the three datasets?
What was the baseline?
Which datasets did they use?
Are results reported only on English datasets?
Which three Twitter sentiment classification datasets are used for experiments?
What semantic rules are proposed?

Which knowledge graph completion tasks do they experiment with?
Apart from using desired properties, do they evaluate their LAN approach in some other way?
Do they evaluate existing methods in terms of desired properties?

How does the model differ from Generative Adversarial Networks?
What is the dataset used to train the model?
What is the performance of the model?
Is the model evaluated against a CNN baseline?

Does the model proposed beat the baseline models for all the values of the masking parameter tested?
Has STES been previously used in the literature to evaluate similar tasks?
What are the baseline models mentioned in the paper?

What was the performance of both approaches on their dataset?
What kind of settings do the utterances come from?
What genres are covered?
Do they experiment with cross-genre setups?
Which of the two speech recognition models works better overall on CN-Celeb?
By how much is performance on CN-Celeb inferior to performance on VoxCeleb?

On what datasets is the new model evaluated on?
How do the authors measure performance?
Does the new objective perform better than the original objective bert is trained on?
Are other pretrained language models also evaluated for contextual augmentation?
Do the authors report performance of conditional bert on tasks without data augmentation?

Do they cover data augmentation papers?
What is the latest paper covered by this survey?
Do they survey visual question generation work?
Do they survey multilingual aspects?
What learning paradigms do they cover in this survey?
What are all the input modalities considered in prior work in question generation?
Do they survey non-neural methods for question generation?

What is their model?
Do they evaluate on NER data sets?

What previously proposed methods is this method compared against?
How is effective word score calculated?
How is tweet subjectivity measured?

Why is supporting fact supervision necessary for DMN?
What does supporting fact supervision mean?
What changes they did on input module?
What improvements they did for DMN?
How does the model circumvent the lack of supporting facts during training?
Does the DMN+ model establish state-of-the-art ?

Is this style generator compared to some baseline?
How they perform manual evaluation, what is criteria?
What metrics are used for automatic evaluation?
How they know what are content words?
How they model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions?

Do they report results only on English data?
What insights into the relationship between demographics and mental health are provided?
What model is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter?
How do this framework facilitate demographic inference from social media?
What types of features are used from each data type?
How is the data annotated?
Where does the information on individual-level demographics come from?
What is the source of the user interaction data?
What is the source of the textual data?
What is the source of the visual data?

Is there an online demo of their system?
Do they perform manual evaluation?
Do they compare against Noraset et al. 2017?
What is a sememe?

What data did they use?
What is the state of the art?
What language tasks did they experiment on?

What result from experiments suggest that natural language based agents are more robust?
How better is performance of natural language based agents in experiments?
How much faster natural language agents converge in performed experiments?
What experiments authors perform?
How is state to learn and complete tasks represented via natural language?

How does the model compare with the MMR baseline?

Does the paper discuss previous models which have been applied to the same task?
Which datasets are used in the paper?
How does the parameter-free model work?
How do they quantify moral relevance?
Which fine-grained moral dimension examples do they showcase?
Which dataset sources to they use to demonstrate moral sentiment through history?

How well did the system do?
How is the information extracted?

What are some guidelines in writing input vernacular so model can generate
How much is proposed model better in perplexity and BLEU score than typical UMT models?
What dataset is used for training?

What were the evaluation metrics?
What were the baseline systems?
Which dialog datasets did they experiment with?
What KB is used?

At which interval do they extract video and audio frames?
Do they use pretrained word vectors for dialogue context embedding?
Do they train a different training method except from scheduled sampling?

Is the web interface publicly accessible?
Is the Android application publicly available?
What classifier is used for emergency categorization?
What classifier is used for emergency detection?
Do the tweets come from any individual?
How many categories are there?
What was the baseline?
Are the tweets specific to a region?

Do they release MED?
What NLI models do they analyze?
How do they define upward and downward reasoning?
What is monotonicity reasoning?

What other relations were found in the datasets?
How does the ensemble annotator extract the final label?
How were dialogue act labels defined?
How many models were used?

Do they compare their neural network against any other model?
Do they annotate their own dataset or use an existing one?
Does their neural network predict a single offset in a recording?
What kind of neural network architecture do they use?

How are aspects identified in aspect extraction?

What web and user-generated NER datasets are used for the analysis?

Which unlabeled data do they pretrain with?
How many convolutional layers does their model have?
Do they explore how much traning data is needed for which magnitude of improvement for WER?

How are character representations from various languages joint?
On which dataset is the experiment conducted?

Do they train their own RE model?
How big are the datasets?
What languages do they experiment on?
What datasets are used?

How better does auto-completion perform when using both language and vision than only language?
How big is data provided by this research?
How they complete a user query prefix conditioned upon an image?

Did the collection process use a WoZ method?
By how much did their model outperform the baseline?
What baselines did they compare their model with?
What was the performance of their model?
What evaluation metrics are used?
Did the authors use a crowdsourcing platform?
How were the navigation instructions collected?
What language is the experiment done in?

What additional features are proposed for future work?
What are their initial results on this task?
What datasets did the authors use?

How many linguistic and semantic features are learned?
How is morphology knowledge implemented in the method?
How does the word segmentation method work?
Is the word segmentation method independently evaluated?

Do they normalize the calculated intermediate output hypotheses to compensate for the incompleteness?
How many layers do they use in their best performing network?
Do they just sum up all the loses the calculate to end up with one single loss?
Does their model take more time to train than regular transformer models?

Are agglutinative languages used in the prediction of both prefixing and suffixing languages?
What is an example of a prefixing language?
How is the performance on the task evaluated?
What are the tree target languages studied in the paper?

Is the model evaluated against any baseline?
Does the paper report the accuracy of the model?
How is the performance of the model evaluated?
What are the different bilingual models employed?
How does the well-resourced language impact the quality of the output?

what are the baselines?
did they outperform previous methods?
what language pairs are explored?
what datasets were used?

How is order of binomials tracked across time?
What types of various community texts have been investigated for exploring global structure of binomials?
Are there any new finding in analasys of trinomials that was not present binomials?
What new model is proposed for binomial lists?
How was performance of previously proposed rules at very large scale?
What previously proposed rules for predicting binoial ordering are used?
What online text resources are used to test binomial lists?

How do they model a city description using embeddings?
How do they obtain human judgements?
Which clustering method do they use to cluster city description embeddings?

Does this approach perform better in the multi-domain or single-domain setting?
What are the performance metrics used?
Which datasets are used to evaluate performance?

How does the automatic theorem prover infer the relation?
If these model can learn the first-order logic on artificial language, why can't it lear for natural language?
How many samples did they generate for the artificial language?

Which of their training domains improves performance the most?
Do they fine-tune their model on the end task?

Why does not the approach from English work on other languages?
How do they measure grammaticality?
Which model do they use to convert between masculine-inflected and feminine-inflected sentences?

What is the performance achieved by the model described in the paper?
What is the best performance achieved by supervised models?
What is the size of the datasets employed?
What are the baseline models?

What evaluation metrics are used?
What datasets did they use?

Why does their model do better than prior models?

What is the difference in recall score between the systems?
What is their f1 score and recall?
How many layers does their system have?
Which news corpus is used?
How large is the dataset they used?

Which coreference resolution systems are tested?

How big is improvement in performances of proposed model over state of the art?
What two large datasets are used for evaluation?
What context modelling methods are evaluated?
What are two datasets models are tested on?

How big is the improvement over the state-of-the-art results?
Is the model evaluated against other Aspect-Based models?

Is the baseline a non-heirarchical model like BERT?

Do they build a model to recognize discourse relations on their dataset?
Which inter-annotator metric do they use?
How high is the inter-annotator agreement?
How are resources adapted to properties of Chinese text?

How better are results compared to baseline models?
What models that rely only on claim-specific linguistic features are used as baselines?
How is pargmative and discourse context added to the dataset?
What annotations are available in the dataset?

How big is dataset used for training/testing?
Is there any example where geometric property is visible for context similarity between words?
What geometric properties do embeddings display?
How accurate is model trained on text exclusively?

What was their result on Stance Sentiment Emotion Corpus?
What performance did they obtain on the SemEval dataset?
What are the state-of-the-art systems?
How is multi-tasking performed?
What are the datasets used for training?
How many parameters does the model have?
What is the previous state-of-the-art model?
What is the previous state-of-the-art performance?

How can the classifier facilitate the annotation task for human annotators?
What recommendations are made to improve the performance in future?
What type of errors do the classifiers use?
What neural classifiers are used?
What is the hashtags does the hashtag-based baseline use?
What languages are included in the dataset?
What dataset is used for this study?
What proxies for data annotation were used in previous datasets?

What are the supported natural commands?
What is the size of their collected dataset?
Did they compare against other systems?
What intents does the paper explore?

What kind of features are used by the HMM models, and how interpretable are those?
What kind of information do the HMMs learn that the LSTMs don't?
Which methods do the authors use to reach the conclusion that LSTMs and HMMs learn complementary information?
How large is the gap in performance between the HMMs and the LSTMs?

Do they report results only on English data?
Which publicly available datasets are used?
What embedding algorithm and dimension size are used?
What data are the embeddings trained on?
how much was the parameter difference between their model and previous methods?
how many parameters did their model use?
which datasets were used?
what was their system's f1 performance?
what was the baseline?

What datasets were used?
What language pairs did they experiment with?

How much more coverage is in the new dataset?
How was coverage measured?
How was quality measured?
How was the corpus obtained?
How are workers trained?
What is different in the improved annotation protocol?
How was the previous dataset annotated?
How big is the dataset?

Do the other multilingual baselines make use of the same amount of training data?
How big is the impact of training data size on the performance of the multilingual encoder?
What data were they used to train the multilingual encoder?

From when are many VQA datasets collected?

What is task success rate achieved?
What simulations are performed by the authors to validate their approach?
Does proposed end-to-end approach learn in reinforcement or supervised learning manner?

Are synonymous relation taken into account in the Japanese-Vietnamese task?
Is the supervised morphological learner tested on Japanese?

What is the dataset that is used in the paper?
What is the performance of the models discussed in the paper?
Does the paper consider the use of perplexity in order to identify text anomalies?
Does the paper report a baseline for the task?

What non-contextual properties do they refer to?
What is the baseline?
What are their proposed features?
What are overall baseline results on new this new task?
What metrics are used in evaluation of this task?
Do authors provide any explanation for intriguing patterns of word being echoed?
What features are proposed?

Which datasets are used to train this model?

How is performance of this system measured?
How many questions per image on average are available in dataset?
Is machine learning system underneath similar to image caption ML systems?
How big dataset is used for training this system?

How do they obtain word lattices from words?
Which metrics do they use to evaluate matching?
Which dataset(s) do they evaluate on?

What languages do they look at?

Do they report results only on English data?
Do they propose any further additions that could be made to improve generalisation to unseen speakers?
What are the characteristics of the dataset?
What type of models are used for classification?
Do they compare to previous work?
How many instances does their dataset have?
What model do they use to classify phonetic segments?
How many speakers do they have in the dataset?

How better is proposed method than baselines perpexity wise?
How does the multi-turn dialog system learns?
How is human evaluation performed?
Is some other metrics other then perplexity measured?
What two baseline models are used?

Do they evaluate on relation extraction?

What is the baseline model for the agreement-based mode?
Do the authors suggest why syntactic parsing is so important for semantic role labelling for interlanguages?
Who manually annotated the semantic roles for the set of learner texts?

By how much do they outperform existing state-of-the-art VQA models?
How do they measure the correlation between manual groundings and model generated ones?
How do they obtain region descriptions and object annotations?

Which training dataset allowed for the best generalization to benchmark sets?
Which model generalized the best?
Which models were compared?
Which datasets were used?

What was the baseline?
Is the data all in Vietnamese?
What classifier do they use?
What is private dashboard?
What is public dashboard?
What dataset do they use?

Do the authors report results only on English data?
What other interesting correlations are observed?

what were the baselines?
what is the state of the art for ranking mc test answers?
what is the size of the introduced dataset?
what datasets did they use?

What evaluation metric is used?
What datasets are used?
What are three main machine translation tasks?
How big is improvement in performance over Transformers?

How do they determine the opinion summary?
Do they explore how useful is the detection history and opinion summary?
Which dataset(s) do they use to train the model?
By how much do they outperform state-of-the-art methods?

What is the average number of turns per dialog?
What baseline models are offered?
Which six domains are covered in the dataset?

What other natural processing tasks authors think could be studied by using word embeddings?
What is the reason that traditional co-occurrence networks fail in establishing links between similar words whenever they appear distant in the text?
Do the use word embeddings alone or they replace some previous features of the model with word embeddings?
On what model architectures are previous co-occurence networks based?

Is model explanation output evaluated, what metric was used?
How many annotators are used to write natural language explanations to SNLI-VE-2.0?
How many natural language explanations are human-written?
How much is performance difference of existing model between original and corrected corpus?
What is the class with highest error rate in SNLI-VE?

What is the dataset used as input to the Word2Vec algorithm?
Are the word embeddings tested on a NLP task?
Are the word embeddings evaluated?
How big is dataset used to train Word2Vec for the Italian Language?
How does different parameter settings impact the performance and semantic capacity of resulting model?
Are the semantic analysis findings for Italian language similar to English language version?
What dataset is used for training Word2Vec in Italian language?

How are the auxiliary signals from the morphology table incorporated in the decoder?
What type of morphological information is contained in the "morphology table"?

Do they report results only on English data?
Do the authors mention any confounds to their study?
Which machine learning models are used?
What methodology is used to compensate for limited labelled data?
Which five natural disasters were examined?

Which social media platform is explored?
What datasets did they use?
What are the baseline state of the art models?

What is the size of the dataset?
What model did they use?
What patterns were discovered from the stories?
Did they use a crowdsourcing platform?

Does the performance increase using their method?
What tasks are they experimenting with in this paper?
What is the size of the open vocabulary?

How do they select answer candidates for their QA task?

How do they extract causality from text?
What is the source of the "control" corpus?
What are the selection criteria for "causal statements"?
Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?
how do they collect the comparable corpus?
How do they collect the control corpus?

What languages do they experiment with?
What are the baselines?
What was the inter-annotator agreement?
Did they use a crowdsourcing platform?

Are resolution mode variables hand crafted?
What are resolution model variables?
Is the model presented in the paper state of the art?

What problems are found with the evaluation scheme?
How is the data annotated?
What collection steps do they mention?
How many intents were classified?
What was the result of the highest performing system?
What metrics are used in the evaluation?

How do they measure the quality of summaries?
Does their model also take the expected answer style as input?
What do they mean by answer styles?
Is there exactly one "answer style" per dataset?
What are the baselines that Masque is compared against?
What is the performance achieved on NarrativeQA?
What is an "answer style"?

What was the previous state of the art model for this task?
What syntactic structure is used to model tones?
What visual information characterizes tones?

Do they report results only on English data?
How do they demonstrate the robustness of their results?
What baseline and classification systems are used in experiments?
How are the EAU text spans annotated?
How are elementary argumentative units defined?

Which Twitter corpus was used to train the word vectors?

How does proposed word embeddings compare to Sindhi fastText word representations?
Are trained word embeddings used for any other NLP task?
How many uniue words are in the dataset?
How is the data collected, which web resources were used?

What trends are found in musical preferences?
Which decades did they look at?
How many genres did they collect from?

Does the paper mention other works proposing methods to detect anglicisms in Spanish?
What is the performance of the CRF model on the task described?
Does the paper motivate the use of CRF as the baseline model?
What are the handcrafted features used?

What is state of the art method?
By how much do proposed architectures autperform state-of-the-art?
What are three new proposed architectures?
How much does the standard metrics for style accuracy vary on different re-runs?

Which baseline methods are used?
How much is the BLEU score?
Which datasets are used in experiments?

What regularizers were used to encourage consistency in back translation cycles?
What are new best results on standard benchmark?
How better is performance compared to competitive baselines?
How big is data used in experiments?
What 6 language pairs is experimented on?
What are current state-of-the-art methods that consider the two tasks independently?

How big is their training set?
What baseline do they compare to?
Which pre-trained transformer do they use?
What is the FEVER task?

How is correctness of automatic derivation proved?
Is this AD implementation used in any deep learning framework?

Do they conduct any human evaluation?
What dataset do they use for experiments?
How do they enrich the positional embedding with length information
How do they condition the output to a given target-source class?
Which languages do they focus on?
What dataset do they use?
Do they experiment with combining both methods?

What state-of-the-art models are compared against?

Does API provide ability to connect to models written in some other deep learning framework?
Is this library implemented into Torch or is framework agnostic?
What baselines are used in experiments?
What general-purpose optimizations are included?

what baseline do they compare to?

How does this compare to traditional calibration methods like Platt Scaling?
What's the input representation of OpenIE tuples into the model?

What statistics on the VIST dataset are reported?

What is the performance difference in performance in unsupervised feature learning between adverserial training and FHVAE-based disentangled speech represenation learning?

What are puns?
What are the categories of code-mixed puns?

How is dialogue guided to avoid interactions that breach procedures and processes only known to experts?
What is meant by semiguided dialogue, what part of dialogue is guided?
Is CRWIZ already used for data collection, what are the results?
How does framework made sure that dialogue will not breach procedures?

How do they combine the models?
What is their baseline?
What context do they use?
What is their definition of hate speech?
What architecture has the neural network?

How is human interaction consumed by the model?
How do they evaluate generated stories?
Do they evaluate in other language appart from English?
What are the baselines?

What is used a baseline?
What contextual features are used?
Where are the cybersecurity articles used in the model sourced from?
What type of hand-crafted features are used in state of the art IOC detection systems?

Do they compare DeepER against other approaches?
How is the data in RAFAEL labelled?
How do they handle polysemous words in their entity library?

How is the fluctuation in the sense of the word and its neighbors measured?

Among various transfer learning techniques, which technique yields to the best performance?

What is the architecture of the model?
What language do they look at?

Where does the vocabulary come from?
What is the worst performing translation granularity?
What dataset did they use?

How do they measure performance?
Do they measure the performance of a combined approach?
Which four QA systems do they use?
How many iterations of visual search are done on average until an answer is found?
Do they test performance of their approaches using human judgements?

What are the sizes of both datasets?

Why are the scores for predicting perceived musical hardness and darkness extracted only for subsample of 503 songs?
How long is the model trained?
What are lyrical topics present in the metal genre?

By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?
What automatic and human evaluation metrics are used to compare SPNet to its counterparts?
Is proposed abstractive dialog summarization dataset open source?
Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?
How does SPNet utilize additional speaker role, semantic slot and dialog domain annotations?
What are previous state-of-the-art document summarization methods used?
How does new evaluation metric considers critical informative entities?
Is new evaluation metric extension of ROGUE?

What measures were used for human evaluation?
What automatic metrics are used for this task?
Are the models required to also generate rationales?
Are the rationales generated after the sentences were written?
Are the sentences in the dataset written by humans who were shown the concept-sets?
Where do the concept sets come from?

How big are improvements of MMM over state of the art?
What out of domain datasets authors used for coarse-tuning stage?
What are state of the art methods MMM is compared to?
What four representative datasets are used for bechmark?

What baselines did they consider?
What are the problems related to ambiguity in PICO sentence prediction tasks?

How is knowledge retrieved in the memory?
How is knowledge stored in the memory?
What are the relative improvements observed over existing methods?
What is the architecture of the neural network?
What methods is RelNet compared to?

How do they measure the diversity of inferences?
By how much do they improve the accuracy of inferences over state-of-the-art methods?
Which models do they use as baselines on the Atomic dataset?
How does the context-aware variational autoencoder learn event background information?
What is the size of the Atomic dataset?

what standard speech transcription pipeline was used?

How much improvement does their method get over the fine tuning baseline?
What kinds of neural networks did they use in this paper?
How did they use the domain tags?

Why mixed initiative multi-turn dialogs are the greatest challenge in building open-domain conversational agents?

How is speed measured?
What is the architecture of their model?
What are the nine types?

How do they represent input features of their model to train embeddings?
Which dimensionality do they use for their embeddings?
Which dataset do they use?
By how much do they outpeform previous results on the word discrimination task?

How does Frege's holistic and functional approach to meaning relates to general distributional hypothesis?
What does Frege's holistic and functional approach to meaning states?

Do they evaluate the quality of the paraphrasing model?
How many paraphrases are generated per question?
What latent variables are modeled in the PCFG?
What are the baselines?

Do they evaluate only on English data?
How strong was the correlation between exercise and diabetes?
How were topics of interest about DDEO identified?

What datasets are used for evaluation?
How do their train their embeddings?
How do they model travel behavior?
How do their interpret the coefficients?

By how much do they outperform previous state-of-the-art models?
Do they use pretrained word embeddings?
How is the lexicon of trafficking flags expanded?

Do they experiment with the dataset?
Do they use a crowdsourcing platform for annotation?
What is an example of a difficult-to-classify case?
What potential solutions are suggested?
What is the size of the dataset?
What Reddit communities do they look at?

How strong is negative correlation between compound divergence and accuracy in performed experiment?
What are results of comparison between novel method to other approaches for creating compositional generalization benchmarks?
How authors justify that question answering dataset presented is realistic?
What three machine architectures are analyzed?
How big is new question answering dataset?
What are other approaches into creating compositional generalization benchmarks?

What problem do they apply transfer learning to?
What are the baselines?
What languages are considered?

Does this model train faster than state of the art models?
What is the performance difference between proposed method and state-of-the-arts on these datasets?
What non autoregressive NMT models are used for comparison?
What are three neural machine translation (NMT) benchmark datasets used for evaluation?

What is result of their attention distribution analysis?
What is result of their Principal Component Analysis?
What are 3 novel fusion techniques that are proposed?

What are two models' architectures in proposed solution?
How do two models cooperate to select the most confident chains?
How many hand-labeled reasoning chains have been created?
What benchmarks are created?

What empricial investigations do they reference?
What languages do they investigate for machine translation?
What recommendations do they offer?
What percentage fewer errors did professional translations make?
What was the weakness in Hassan et al's evaluation design?

By how much they improve over the previous state-of-the-art?
Is there any evidence that encoders with latent structures work well on other tasks?

Do they report results only on English?
What evidence do they present that the model attends to shallow context clues?
In what way is the input restructured?

What are their results on the entity recognition task?
What task-specific features are used?
What kind of corpus-based features are taken into account?
Which machine learning algorithms did the explore?
What language is the Twitter content in?

What is the architecture of the siamese neural network?
How do they explore domain mismatch?
How do they explore dialect variability?
Which are the four Arabic dialects?

What factors contribute to interpretive biases according to this research?
Which interpretative biases are analyzed in this paper?

How many words are coded in the dictionary?
Is the model evaluated on the graphemes-to-phonemes task?

How does the QuaSP+Zero model work?
Which off-the-shelf tools do they use on QuaRel?
How do they obtain the logical forms of their questions in their dataset?
Do all questions in the dataset allow the answers to pick from 2 options?

What is shared in the joint model?
Are the intent labels imbalanced in the dataset?

What kernels are used in the support vector machines?
What dataset is used?
What metrics are considered?

Did the authors evaluate their system output for coherence?
What evaluations did the authors use on their system?

What accuracy does CNN model achieve?
How many documents are in the Indiscapes dataset?
What language(s) are the manuscripts written in?

What metrics are used to evaluation revision detection?
How large is the Wikipedia revision dump dataset?
What are simulated datasets collected?
Which are the state-of-the-art models?

Why is being feature-engineering free an advantage?
Where did this model place in the final evaluation of the shared task?
What in-domain data is used to continue pre-training?
What dialect is used in the Google BERT model and what is used in the task data?
What are the tasks used in the mulit-task learning setup?

What human evaluation metrics were used in the paper?

For the purposes of this paper, how is something determined to be domain specific knowledge?
Does the fact that GCNs can perform well on this tell us that the task is simpler than previously thought?
Are there conceptual benefits to using GCNs over more complex architectures like attention?

Do they evaluate only on English?
What are the 7 different datasets?
What are the three different sources of data?
What type of model are the ELMo representations used in?
Which morphosyntactic features are thought to indicate irony or sarcasm?

Is the model evaluated on other datasets?
Does the model incorporate coreference and entailment?
Is the incorporation of context separately evaluated?

Which frozen acoustic model do they use?
By how much does using phonetic feedback improve state-of-the-art systems?

What features are used?
Do they compare to previous models?
How do they incorporate sentiment analysis?
What is the dataset used?
How do they extract topics?

How does this compare to simple interpolation between a word-level and a character-level language model?

What translationese effects are seen in the analysis?
What languages are seen in the news and TED datasets?
How are the coreference chain translations evaluated?
How are the (possibly incorrect) coreference chains in the MT outputs annotated?
Which three neural machine translation systems are analyzed?
Which coreference phenomena are analyzed?

What new interesting tasks can be solved based on the uncanny semantic structures of the embedding space?
What are the uncanny semantic structures of the embedding space?
What is the general framework for data exploration by semantic queries?
What data exploration is supported by the analysis of these semantic structures?

what are the existing models they compared with?

Do they report results only on English data?
What conclusions do the authors draw from their detailed analyses?
Do the BERT-based embeddings improve results?
What were the traditional linguistic feature-based models?
What type of baseline are established for the two datasets?

What model achieves state of the art performance on this task?
Which multitask annotated corpus is used?
What are the tasks in the multitask learning setup?
What are the subtle changes in voice which have been previously overshadowed?

how are rare words defined?
which public datasets were used?
what are the baselines?

What are the results of the experiment?
How was the dataset collected?
What is the size of the dataset?
How many different subjects does the dataset contain?
How many annotators participated?
How long is the dataset?

Do the authors mention any possible confounds in their study?
What is the relationship between the co-voting and retweeting patterns?
Does the analysis find that coalitions are formed in the same way for different policy areas?
What insights does the analysis give about the cohesion of political groups in the European parliament?
Do they authors account for differences in usage of Twitter amongst MPs into their model?
Did the authors examine if any of the MEPs used the disclaimer that retweeting does not imply endorsement on their twitter profile?

How do they show their model discovers underlying syntactic structure?
Which dataset do they experiment with?
How do they measure performance of language model tasks?

How are content clusters used to improve the prediction of incident severity?
What cluster identification method is used in this paper?

How can a neural model be used for a retrieval if the input is the entire Wikipedia?

Which algorithm is used in the UDS-DFKI system?
Does the use of out-of-domain data improve the performance of the method?

Do they impose any grammatical constraints over the generated output?
Why did they think this was a good idea?

How many languages are included in the tweets?
What languages are explored?
Which countries did they look at?

What QA models were used?
Can this approach model n-ary relations?
Was this benchmark automatically created from an existing dataset?

How does morphological analysis differ from morphological inflection?
What was the criterion used for selecting the lemmata?
What are the architectures used for the three tasks?
Which language family does Chatino belong to?
What system is used as baseline?
How was annotation done?
How was the data collected?

How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?
On which task does do model do worst?
On which task does do model do best?

Is their implementation on CNN-DSA compared to GPU implementation in terms of power consumption, accuracy and speed?
Does this implementation on CNN-DSA lead to diminishing of performance?
How is Super Character method modified to handle tabular data also?

How are the substitution rules built?
Which dataset do they use?

What baseline is used to compare the experimental results against?
How does counterfactual data augmentation aim to tackle bias?
In the targeted data collection approach, what type of data is targetted?

Which language models do they compare against?
Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?
How do they determine which words are informative?

What is their best performance on the largest language direction dataset?
How does soft contextual data augmentation work?
How does muli-agent dual learning work?
Which language directions are machine translation systems of WMT evaluated on?

Approximately how much computational cost is saved by using this model?
What improvement does the MOE model make over the SOTA on machine translation?
What improvement does the MOE model make over the SOTA on language modelling?
How is the correct number of experts to use decided?
What equations are used for the trainable gating network?

What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?
What solutions are proposed for error detection and context awareness?
How is PIEWi annotated?
What methods are tested in PIEWi?
Which specific error correction solutions have been proposed for specialized corpora in the past?

What was the criteria for human evaluation?
What automatic metrics are used to measure performance of the system?
What existing methods is SC-GPT compared to?

Which language-pair had the better performance?
Which datasets were used in the experiment?
What evaluation metrics did they use?

Do they evaluate only on English datasets?
What are the differences in the use of emojis between gang member and the rest of the Twitter population?
What are the differences in the use of YouTube links between gang member and the rest of the Twitter population?
What are the differences in the use of images between gang member and the rest of the Twitter population?
What are the differences in language use between gang member and the rest of the Twitter population?
How is gang membership verified?
Do the authors provide evidence that 'most' street gang members use Twitter to intimidate others?

What is English mixed with in the TRAC dataset?
Which psycholinguistic and basic linguistic features are used?
How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?
What are the key differences in communication styles between Twitter and Facebook?
What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?

What is the baseline?
What datasets did they use?

What scoring function does the model use to score triples?
What datasets are used to evaluate the model?

How long it took for each Doc2Vec model to be trained?
How better are results for pmra algorithm  than Doc2Vec in human evaluation?
What Doc2Vec architectures other than PV-DBOW have been tried?
What four evaluation tasks are defined to determine what influences proximity?
What six parameters were optimized with grid search?

What baseline models do they compare against?

What are the differences with previous applications of neural networks for this task?

How much improvement is gained from the proposed approaches?
Is the problem of determining whether a given model would generate an infinite sequence is a decidable problem?
Is infinite-length sequence generation a result of training with maximum likelihood?

What metrics are used in challenge?
What model was winner of the Visual Dialog challenge 2019?
What model was winner of the Visual Dialog challenge 2018?
Which method for integration peforms better ensemble or consensus dropout fusion with shared parameters?
How big is dataset for this challenge?

What open relation extraction tasks did they experiment on?
How is Logician different from traditional seq2seq models?
What's the size of the previous largest OpenIE dataset?

How is data for RTFM collected?
How better is performance of proposed model compared to baselines?
How does propose model model that capture three-way interactions?

Do transferring hurt the performance is the corpora are not related?
Is accuracy the only metric they used to compare systems?
How do they transfer the model?

Will these findings be robust through different datasets and different question answering algorithms?
What is the underlying question answering algorithm?
What datasets have this method been evaluated on?

Is there a machine learning approach that tries to solve same problem?
What DCGs are used?
What else is tried to be solved other than 12 tenses, model verbs and negative form?
What is used for evaluation of this approach?
Is there information about performance of these conversion methods?
Are there some experiments performed in the paper?

How much is performance improved by disabling attention in certain heads?
In which certain heads was attention disabled in experiments?
What handcrafter features-of-interest are used?
What subset of GLUE tasks is used?

Do they predict the sentiment of the review summary?
What is the performance difference of using a generated summary vs. a user-written one?
Which review dataset do they use?

What evaluation metrics did they look at?
What datasets are used?

What are the datasets used for the task?
What is the accuracy of the model for the six languages tested?
Which models achieve state-of-the-art performances?
Is the LSTM bidirectional?

What are the three languages studied in the paper?

Do they use pretrained models as part of their parser?
Which subtasks do they evaluate on?

Do they test their approach on large-resource tasks?
By how much do they, on average, outperform the baseline multilingual model on 16 low-resource tasks?
How do they compute corpus-level embeddings?

Which dataset do they use?

Which competitive relational classification models do they test?
Which tasks do they apply their method to?
Which knowledge bases do they use?
How do they gather human judgements for similarity between relations?
Which sampling method do they use to approximate similarity between the conditional probability distributions over entity pairs?

What text classification task is considered?
What novel class of recurrent-like networks is proposed?
Is there a formal proof that the RNNs form a representation of the group?

How much bigger is Switchboard-2000 than Switchboard-300 database?
How big is Switchboard-300 database?

What crowdsourcing platform is used for data collection and data validation?
How is validation of the data performed?
Is audio data per language balanced in dataset?

What is the performance of their model?
Which text genres did they experiment with?
What domains are detected in this paper?

Why do they think this task is hard?  What is the baseline performance?
Isn't simple word association enough to predict the next spell?
Do they literally just treat this as "predict the next spell that appears in the text"?
How well does a simple bag-of-words baseline do?

Do they study frequent user responses to help automate modelling of those?
How do they divide text into utterances?
Do they use the same distance metric for both the SimCluster and K-means algorithm?
How do they generate the synthetic dataset?

how are multiple answers from multiple reformulated questions aggregated?

What is the average length of the claims?
What debate websites did they look at?
What crowdsourcing platform did they use?
Which machine baselines are used?
What challenges are highlighted?
What debate topics are included in the dataset?

By how much, the proposed method improves BiDAF and DCN on SQuAD dataset?

Do they report results only on English datasets?
What are the linguistic differences between each class?
What simple features are used?
What lexico-syntactic cues are used to retrieve sarcastic utterances?

what is the source of the song lyrics?
what genre was the most difficult to classify?
what word embedding techniques did they experiment with?
what genres do they songs fall under?

Is the filter based feature selection (FSE) a form of regularization?

To what other competitive baselines is this approach compared?
How is human evaluation performed, what was the criteria?
How much better were results of the proposed models than base LSTM-RNN model?
Which one of the four proposed models performed best?

What metrics are used to measure performance of models?
How much is proposed model better than baselines in performed experiments?
What are state-of-the-art baselines?
What two benchmark datasets are used?

What languages are the model evaluated on?
Do they compare to other models appart from HAN?
What are the datasets used

Do they evaluate only on English datasets?
What evidence does visualizing the attention give to show that it helps to obtain a more robust understanding of semantics and sentiments?
Which SOTA models are outperformed?
What is the baseline for experiments?
What is the motivation for training bi-sense embeddings?

How many parameters does the model have?
How many characters are accepted as input of the language model?

What dataset is used for this task?
What features of the document are integrated into vectors of every sentence?
By how much is precission increased?
Is new approach tested against state of the art?

Is the dataset balanced across categories?
What supervised methods are used?
What labels are in the dataset?
What categories does the dataset come from?

What are all machine learning approaches compared in this work?

Do they evaluate only on English datasets?
Which patterns and rules are derived?
How are customer satisfaction, customer frustration and overall problem resolution data collected?
Which Twitter customer service industries are investigated?
Which dialogue acts are more suited to the twitter domain?

How many improvements on the French-German translation benchmark?
How do they align the synthetic data?
Where do they collect the synthetic data?

Do they analyze what type of content Arabic bots spread in comparison to English?
Do they propose a new model to better detect Arabic bots specifically?

How do they prevent the model complexity increasing with the increased number of slots?
What network architecture do they use for SIM?
How do they measure model size?

Does model uses pretrained Transformer encoders?
What was previous state of the art model?
What was previous state of the art accuracy on LibriSpeech benchmark?
How big is LibriSpeech dataset?

Which language(s) do they work with?
How do they evaluate their sentence representations?
Which model architecture do they for sentence encoding?
How many tokens can sentences in their model at most contain?
Which training objectives do they combine?
Which data sources do they use?

Has there been previous work on SNMT?
Which languages do they experiment on?
What corpora is used?

Do the authors report results only on English datasets?
How were breast cancer related posts compiled from the Twitter streaming API?
What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?

What kind of events do they extract?
Is this the first paper to propose a joint model for event and temporal relation extraction?
What datasets were used for this work?

What languages did they experiment with?

What approach performs better in experiments global latent or sequence of fine-grained latent variables?
What baselines other than standard transformers are used in experiments?
What three conversational datasets are used for evaluation?

What previous approaches did this method outperform?
How big is the Universal Dependencies corpus?
What data is the Prague Dependency Treebank built on?
What data is used to build the embeddings?

How big is dataset used for fine-tuning model for detection of red flag medical symptoms in individual statements?
Is there any explanation why some choice of language pair is better than the other?

Is the new model evaluated on the tasks that BERT and ELMo are evaluated on?
Does the additional training on supervised tasks hurt performance in some tasks?

Which translation system do they use to translate to English?
Which languages do they work with?
Which pre-trained English NER model do they use?

How much training data is required for each low-resource language?
What are the best within-language data augmentation methods?
How much of the ASR grapheme set is shared between languages?

What is the performance of the model for the German sub-task A?
Is the model tested for language identification?
Is the model compared to a baseline model?
What are the languages used to test the model?

Which language has the lowest error rate reduction?
What datasets did they use?

Do they report results only on English data?
What is the Moral Choice Machine?
How is moral bias measured?
What sentence embeddings were used in the previous Jentzsch paper?
How do the authors define deontological ethical reasoning?

How does framework automatically chooses different curricula at the evolving learning process according to the learning status of the neural dialogue generation model?
What human judgement metrics are used?
What automatic evaluation metrics are used?
What state of the art models were used in experiments?
What five dialogue attributes were analyzed?
What three publicly available coropora are used?

Which datasets do they use?

What metrics are used for evaluation?
How much training data is used?
How is the training data collected?
What language(s) is the model trained/tested on?

was bert used?
what datasets did they use?
which existing metrics do they compare with?

Which datasets do they evaluate on?
How does their model differ from BERT?

Which metrics are they evaluating with?

What different properties of the posterior distribution are explored in the paper?
Why does proposed term help to avoid posterior collapse?
How does explicit constraint on the KL divergence term that authors propose looks like?

Did they experiment with the tool?
Can it be used for any language?
Is this software available to the public?

What shared task does this system achieve SOTA in?
How are labels propagated using this approach?
What information is contained in the social graph of tweet authors?

What were the five English subtasks?
How many CNNs and LSTMs were ensembled?

what was the baseline?
how many movie genres do they explore?
what evaluation metrics are discussed?

How big is dataset used?
What is dataset used for news-driven stock movement prediction?

How much better does this baseline neural model do?

What is the SemEval-2016 task 8?

How much faster is training time for MGNC-CNN over the baselines?
What are the baseline models?
By how much of MGNC-CNN out perform the baselines?
What dataset/corpus is this evaluated over?
What are the comparable alternative architectures?

Which state-of-the-art model is surpassed by 9.68% attraction score?
What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?
How is attraction score measured?
How is presence of three target styles detected?
How is fluency automatically evaluated?

What are the measures of "performance" used in this paper?

What are the languages they consider in this paper?
Did they experiment with tasks other than word problems in math?

Do they report results only on English data?
Do the authors offer any hypothesis as to why the transformations sometimes disimproved performance?
What preprocessing techniques are used in the experiments?
What state of the art models are used in the experiments?

What evaluation metrics are used?
What dataset did they use?
What tasks did they experiment with?

What multilingual parallel data is used for training proposed model?
How much better are results of proposed model compared to pivoting method?

What kind of Youtube video transcripts did they use?
Which SBD systems did they compare?
What makes it a more reliable metric?

How much in experiments is performance improved for models trained with generated adversarial examples?
How much dramatically results drop for models on generated adversarial examples?
What is discriminator in this generative adversarial setup?
What are benhmark datasets for paraphrase identification?

What representations are presented by this paper?
What corpus characteristics correlate with more equitable gender balance?
What natural languages are represented in the speech resources studied?

How is the delta-softmax calculated?
Are some models evaluated using this metric, what are the findings?
Where does proposed metric differ from juman judgement?
Where does proposed metric overlap with juman judgement?

Which baseline performs best?
Which baselines are explored?
What is the size of the corpus?
How was the evaluation corpus collected?

Are any machine translation sysems tried with these embeddings, what is the performance?
Are any experiments performed to try this approach to word embeddings?

Which two datasets does the resource come from?

What model was used by the top team?
What was the baseline?
What is the size of the second dataset?
How large is the first dataset?
Who was the top-scoring team?

What supervised learning tasks are attempted with these representations?
What is MRR?
Which techniques for word embeddings and topic models are used?
Why is big data not appropriate for this task?
What is an example of a computational social science NLP task?

Do they report results only on English datasets?
What were the previous state of the art benchmarks?
How/where are the natural question generated?
What is the input to the differential network?
How do the authors define a differential network?
How do the authors define exemplars?

Is this a task other people have worked on?
Where did they get the data for this project?

Which major geographical regions are studied?
How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?
How are the topics embedded in the #MeToo tweets extracted?
How many tweets are explored in this paper?
Which geographical regions correlate to the trend?
How many followers did they analyze?

What two components are included in their proposed framework?
Which framework they propose in this paper?
Why MS-MARCO is different from SQuAD?

Did they experiment with pre-training schemes?
What were their results on the test set?
What is the size of the dataset?
What was the baseline model?

What models are evaluated with QAGS?
Do they use crowdsourcing to collect human judgements?

Which dataset(s) do they evaluate on?
Which modifications do they make to well-established Seq2seq architectures?
How do they measure the size of models?
Do they reduce the number of parameters in their architecture compared to other direct text-to-speech models?

Do they use pretrained models?
What are their baseline models?

How was speed measured?
What were their accuracy results on the task?
What is the state of the art?
How was the dataset annotated?
What is the size of the dataset?
Where did they collect their dataset from?

How much in-domain data is enough for joint models to outperform baselines?
How many parameters does their proposed joint model have?

How does the model work if no treebank is available?
How many languages have this parser been tried on?

Do they use attention?
What non-annotated datasets are considered?

Did they compare to Transformer based large language models?
Which baselines are they using?

What two types the Chinese reading comprehension dataset consists of?
For which languages most of the existing MRC datasets are created?

How did they induce the CFG?
How big is their dataset?

By how much do they outperform basic greedy and cross-entropy beam decoding?
Do they provide a framework for building a sub-differentiable for any final loss metric?
Do they compare partially complete sequences (created during steps of beam search) to gold/target sequences?
Which loss metrics do they try in their new training procedure evaluated on the output of beam search?

How are different domains weighted in WDIRL?
How is DIRL evaluated?
Which sentiment analysis tasks are addressed?

Which NLP area have the highest average citation for woman author?
Which 3 NLP areas are cited the most?
Which journal and conference are cited the most in recent years?
Which 5 languages appear most frequently in AA paper titles?
What aspect of NLP research is examined?
Are the academically younger authors cited less than older?
How many papers are used in experiment?

What ensemble methods are used for best model?
What hyperparameters have been tuned?
How much F1 was improved after adding skip connections?

Where do they retrieve neighbor n-grams from in their approach?
To which systems do they compare their results against?
Does their combination of a non-parametric retrieval and neural network get trained end-to-end?
Which similarity measure do they use in their n-gram retrieval approach?

Where is MVCNN pertained?
How much gain does the model achieve with pretraining MVCNN?
What are the effects of extracting features of multigranular phrases?
What are the effects of diverse versions of pertained word embeddings?
How is MVCNN compared to CNN?

What is the highest accuracy score achieved?
What is the size range of the datasets?

Does the paper report F1-scores for the age and language variety tasks?
Are the models compared to some baseline models?
What are the in-house data employed?
What are the three datasets used in the paper?

What are the potentials risks of this approach?
What elements of natural language processing are proposed to analyze qualitative data?

How does the method measure the impact of the event on market prices?
How is sentiment polarity measured?

Which part of the joke is more important in humor?
What is improvement in accuracy for short Jokes in relation other types of jokes?
What kind of humor they have evaluated?
How they evaluate if joke is humorous or not?

Do they report results only on English data?
Do the authors have a hypothesis as to why morphological agreement is hardly learned by any model?
Which models are best for learning long-distance movement?
Where does the data in CoLA come from?
How is the CoLA grammatically annotated?

What baseline did they compare Entity-GCN to?
How many documents at a time can Entity-GCN handle?
Did they use a relation extraction method to construct the edges in the graph?
How did they get relations between mentions?
How did they detect entity mentions?
What is the metric used with WIKIHOP?
What performance does the Entity-GCN get on WIKIHOP?

Do they evaluate only on English datasets?
What baseline models are used?
What classical machine learning algorithms are used?
What are the different methods used for different corpora?
In which domains is sarcasm conveyed in different ways?

What modalities are being used in different datasets?
What is the difference between Long-short Term Hybrid Memory and LSTMs?

Do they report results only on English data?
What provisional explanation do the authors give for the impact of document context?
What document context was added?
What were the results of the first experiment?

How big is the evaluated dataset?
By how much does their model outperform existing methods?
What is the performance of their model?
What are the existing methods mentioned in the paper?

Does having constrained neural units imply word meanings are fixed across different context?
Do they perform a quantitative analysis of their model displaying knowledge distortions?
How do they damage different neural modules?
Which weights from their model do they analyze?

Do all the instances contain code-switching?
What embeddings do they use?
Do they perform some annotation?
Do they use dropout?
What definition of hate speech do they use?

What are the other models they compare to?
What is the agreement value for each dataset?
How many annotators participated?
How long are the datasets?
What are the sources of the data?
What is the new labeling strategy?

Which future direction in NLG are discussed?
What experimental phenomena are presented?
How strategy-based methods handle obstacles in NLG?
How architecture-based method handle obstacles in NLG?

How are their changes evaluated?

What baseline is used for the verb classification experiments?
What clustering algorithm is used on top of the VerbNet-specialized representations?
How many words are translated between the cross-lingual translation pairs?
What are the six target languages?

what classifiers were used in this paper?
what are their evaluation metrics?
what types of features were used?
what lexical features did they experiment with?
what is the size of the dataset?
what datasets were used?
what are the three reasons everybody hates them?

How are seed dictionaries obtained by fully unsupervised methods?
How does BLI measure alignment quality?
What methods were used for unsupervised CLWE?

What is the size of the released dataset?
Were the OpenIE systems more accurate on some scientific disciplines than others?
What is the most common error type?
Which OpenIE systems were used?
What is the role of crowd-sourcing?

How are meta vertices computed?
How are graphs derived from a given text?
In what sense if the proposed method interpretable?

how are the bidirectional lms obtained?
what metrics are used in evaluation?
what results do they achieve?
what previous systems were compared to?
what are the evaluation datasets?

Are datasets publicly available?
Are this models usually semi/supervised or unsupervised?
Is there any concrete example in the paper that shows that this approach had huge impact on drug discovery?

Do the authors analyze what kinds of cases their new embeddings fail in where the original, less-interpretable embeddings didn't?
When they say "comparable performance", how much of a performance drop do these new embeddings result in?
How do they evaluate interpretability?

What types of word representations are they evaluating?

What type of recurrent layers does the model use?
What is a word confusion network?

What type of simulations of real-time data feeds are used for validaton?
How are FHIR and RDF combined?
What are the differences between FHIR and RDF?
What do FHIR and RDF stand for?

What is the motivation behind the work? Why question generation is an important task?
Why did they choose WER as evaluation metric?

What evaluation metrics were used in the experiment?
What kind of instructional videos are in the dataset?
What baseline algorithms were presented?
What is the source of the triples?

How much better is performance of the proposed model compared to the state of the art in these various experiments?
What was state of the art on SemEval-2014 task4 Restaurant and Laptop dataset?
What was previous state-of-the-art on four Chinese reviews datasets?
In what four Chinese review datasets does LCF-ATEPC achieves state of the art?
Why authors think that researches do not pay attention to the research of the Chinese-oriented ABSA task?
What is specific to Chinese-oriented ABSA task, how is it different from other languages?

what is the size of this dataset?
did they use a crowdsourcing platform for annotations?
where does the data come from?

What is the criteria for a good metric?
What are the three limitations?

What is current state-of-the-art model?

Which language(s) are found in the WSD datasets?
What datasets are used for testing?

How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?
What is the performance proposed model achieved on AlgoList benchmark?
What is the performance proposed model achieved on MathQA?

How do previous methods perform on the Switchboard Dialogue Act and DailyDialog datasets?
What previous methods is the proposed method compared against?
What is dialogue act recognition?
Which natural language(s) are studied?

Does the performance necessarily drop when more control is desired?
How does the model perform in comparison to end-to-end headline generation models?
How is the model trained to do only content selection?

What is the baseline model used?

Is this auto translation tool based on neural networks?
What are results of public code repository study?

Where is the dataset from?
What data augmentation techniques are used?
Do all teams use neural networks for their models?
How are the models evaluated?
What is the baseline model?
What domains are present in the data?

How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?
In which languages did the approach outperform the reported results?
What eight language are reported on?
What are the components of the multilingual framework?

Is the proposed method compared to previous methods?
What metrics are used to evaluate results?

Which is the baseline model?
How big is the Babel database?
What is the main contribution of the paper?

What training settings did they try?
How do they get the formal languages?
Are the unobserved samples from the same distribution as the training data?

By how much does their model outperform the baseline in the cross-domain evaluation?
What are the performance results?

What is a confusion network or lattice?

What dataset is used for training?
How close do clusters match to ground truth tone categories?

what are the evaluation metrics?
which datasets were used in evaluation?
what are the baselines?

What multilingual word representations are used?
Do the authors identify any cultural differences in irony use?
What neural architectures are used?
What text-based features are used?
What monolingual word representations are used?

Does the proposed method outperform a baseline?
What type of RNN is used?

What do they constrain using integer linear programming?
Do they build one model per topic or on all topics?
Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?
Do they evaluate their framework on content of low lexical variety?

Do the authors report on English datasets only?
Which supervised learning algorithms are used in the experiments?
How in YouTube content translated into a vector format?
How is the ground truth of gang membership established in this dataset?

Do they evaluate ablated versions of their CNN+RNN model?

Do they single out a validation set from the fixed SRE training set?
How well does their system perform on the development set of SRE?
Which are the novel languages on which SRE placed emphasis on?

Does this approach perform better than context-based word embeddings?
Have the authors tried this approach on other languages?

What features did they train on?

How big is the test set?
What is coattention?

What off-the-shelf QA model was used to answer sub-questions?
How large is the improvement over the baseline?
What is the strong baseline that this work outperforms?

Which dataset do they use?
Do they trim the search space of possible output sequences?
Which model architecture do they use to build a model?
Do they compare simultaneous translation performance to regular machine translation?
Which metrics do they use to evaluate simultaneous translation?

How big are FigureQA and DVQA datasets?
What models other than SAN-VOES are trained on new PlotQA dataset?

Do the authors report only on English language data?
What dataset of tweets is used?
What external sources of information are used?
What linguistic features are used?

What are the key issues around whether the gold standard data produced in such an annotation is reliable?
How were the machine learning papers from ArXiv sampled?
What are the core best practices of structured content analysis?
In what sense is data annotation similar to structured content analysis?

What additional information is found in the dataset?
Is the dataset focused on a region?
Over what period of time were the tweets collected?
Are the tweets location-specific?
How big is the dataset?

Do the authors suggest any future extensions to this work?
Which of the classifiers showed the best performance?
Were any other word similar metrics, besides Jaccard metric, tested?
How are the keywords associated with events such as protests selected?

How many speeches are in the dataset?
What classification models were used?
Do any speeches not fall in these categories?

What is different in BERT-gen from standard BERT?
How are multimodal representations combined?

What is the problem with existing metrics that they are trying to address?
How do they evaluate their proposed metric?
What is a facet?

How are discourse embeddings analyzed?
What was the previous state-of-the-art?
How are discourse features incorporated into the model?
What discourse features are used?

How are proof scores calculated?
What are proof paths?

What is the size of the model?
What external sources are used?
What submodules does the model consist of?

How they add human prefference annotation to fine-tuning process?
What previous automated evalution approaches authors mention?
How much better peformance is achieved in human evaluation when model is trained considering proposed metric?
Do the authors suggest that proposed metric replace human evaluation on this task?

What is the training objective of their pair-to-sequence model?
How do they ensure the generated questions are unanswerable?
Does their approach require a dataset of unanswerable questions mapped to similar answerable questions?

What conclusions are drawn from these experiments?
What experiments are presented?
What is specific about the specific embeddings?
What embedding algorithm is used to build the embeddings?
How was the KGR10 corpus created?

How big are improvements with multilingual ASR training vs single language training?
How much transcribed data is available for for Ainu language?
What is the difference between speaker-open and speaker-closed setting?

What baseline approaches do they compare against?
How do they train the retrieval modules?
How do they model the neural retrieval modules?
Retrieval at what level performs better, sentence level or paragraph level?

How much better performance of proposed model compared to answer-selection models?
How are some nodes initially connected based on text structure?

how many domains did they experiment with?
what pretrained models were used?

What domains are contained in the polarity classification dataset?
How long is the dataset?
What machine learning algorithms are used?
What is a string kernel?

Which dataset do they use to learn embeddings?
How do they correlate NED with emotional bond levels?

What was their F1 score on the Bengali NER corpus?
Which languages are evaluated?

Which model have the smallest Character Error Rate and which have the smallest Word Error Rate?
What will be in focus for future work?
Which acoustic units are more suited to model the French language?
What are the existing end-to-end ASR approaches for the French language?

How much is decoding speed increased by increasing encoder and decreasing decoder depth?

Did they experiment on this dataset?
What language are the conversations in?
How did they annotate the dataset?
What annotations are in the dataset?

What is the size of the dataset?
What language platform does the data come from?
Which two schemes are used?

How many examples do they have in the target domain?

Does Grail accept Prolog inputs?
What formalism does Grail use?

Which components of QA and QG models are shared during training?
How much improvement does jointly learning QA and QG give, compared to only training QA?

Do they test their word embeddings on downstream tasks?
What are the main disadvantages of their proposed word embeddings?
What dimensions of word embeddings do they produce using factorization?
On which dataset(s) do they compute their word embeddings?
Do they measure computation time of their factorizations compared to other word embeddings?

What datasets are experimented with?
What is the baseline model?

What model do they train?
What are the eight features mentioned?
How many languages are considered in the experiments?

How did they evaluate the system?
Where did they get training data?
What extraction model did they use?
Which datasets did they experiment on?
What types of facts can be extracted from QA pairs that can't be extracted from general text?

How do slot binary classifiers improve performance?
What baselines have been used in this work?

Do sluice networks outperform non-transfer learning approaches?
What is hard parameter sharing?

How successful are they at matching names of authors in Japanese and English?
Is their approach applicable to papers outside computer science?
Do they translate metadata from Japanese papers to English?

what bottlenecks were identified?

What is grounded language understanding?

Does the paper report the performance on the task of a Neural Machine Translation model?
What are the predefined morpho-syntactic patterns used to filter the training data?
Is the RNN model evaluated against any baseline?
Which languages are used in the paper?

What metrics are used for evaluation?
What baselines are used?

Which model is used to capture the implicit structure?
How is the robustness of the model evaluated?
How is the effectiveness of the model evaluated?

Do they assume sentence-level supervision?

By how much do they outperform BiLSTMs in Sentiment Analysis?
Does their model have more parameters than other models?

what state of the accuracy did they obtain?
what models did they compare to?
which benchmark tasks did they experiment on?

Are recurrent neural networks trained on perturbed data?
How does their perturbation algorihm work?

Which language is divided into six dialects in the task mentioned in the paper?
What is one of the first writing systems in the world?

How do they obtain distant supervision rules for predicting relations?
Which structured prediction approach do they adopt for temporal entity extraction?

Which evaluation metric has been measured?
What is the WordNet counterpart for Persian?

Do they consider relations other than binary relations?
Are the grammar clauses manually created?
Do they use an NER system in their pipeline?

What large corpus is used for experiments?

Are any of the utterances ungrammatical?
How is the proficiency score calculated?
What proficiency indicators are used to the score the utterances?
What accuracy is achieved by the speech recognition system?
How is the speech recognition system evaluated?
How many of the utterances are transcribed?
How many utterances are in the corpus?

By how much does their model outperform both the state-of-the-art systems?
What is the state-of-the art?

How do they identify abbreviations?
What kind of model do they build to expand abbreviations?
Do they use any knowledge base to expand abbreviations?
In their used dataset, do they study how many abbreviations are ambiguous?
Which dataset do they use to build their model?

What is the domain of their collected corpus?
What was the performance on the self-collected corpus?
What is the size of their dataset?
What is the source of the CAIS dataset?
What were the baselines models?

Are the document vectors that the authors introduce evaluated in any way other than the new way the authors propose?
According to the authors, why does the CNN model exhibit a higher level of explainability?
Does the LRP method work in settings that contextualize the words with respect to one another?

How do they incorporate lexicon into the neural network?
What is the source of their lexicon?
What was their performance?
How long is the dataset used for training?
What embeddings do they use?

did they use other pretrained language models besides bert?
how was the dataset built?
what is the size of BoolQ dataset?

what processing was done on the speeches before being parsed?

What programming language is the tool written in?

What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?
Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?
What kind is noise is present in typical industrial data?
What is the reason behind the drop in performance using BERT for some popular task?

How they observe that fine-tuning BERT on a specific task does not improve its prunability?
How much is pre-training loss increased in Low/Medium/Hard level of pruning?

How do they gather human reviews?
Do they explain model predictions solely on attention weights?
Can their method of creating more informative visuals be applied to tasks other than turn taking in conversations?

What was the baseline?
What is the average length of the recordings?
How big was the dataset presented?
What were their results?

Does a neural scoring function take both the question and the logical form as inputs?
What is the source of the paraphrases of the questions?
Does the dataset they use differ from the one used by Pasupat and Liang, 2015?

Did they experiment on this corpus?

Is the model compared against a linear regression baseline?
What is the prediction accuracy of the model?
What is the dataset used in the paper?
How does the differential privacy mechanism work?

How does the SCAN dataset evaluate compositional generalization?

Is their model fine-tuned also on all available data, what are results?

How much does this system outperform prior work?
What are the baseline systems that are compared against?

What standard large speaker verification corpora is used for evaluation?
What systems are tested?

How many examples are there in the source domain?
How many examples are there in the target domain?
Did they only experiment with captioning task?

how well this method is compared to other method?

What benchmark datasets they use?

what is the proposed Progressive Dynamic Hurdles method?
What is in the model search space?
How much energy did the NAS consume?
How does Progressive Dynamic Hurdles work?

Do they beat current state-of-the-art on SICK?
How do they combine MonaLog with BERT?
How do they select monotonicity facts?

How does the model recognize entities and their relation to answers at inference time when answers are not accessible?

What other solutions do they compare to?
How does the gatint mechanism combine word and character information?

Which dataset do they use?
How is the PBMT system trained?
Which NMT architecture do they use?
Do they train the NMT model on PBMT outputs?

Is the corpus annotated?
How is the corpus normalized?
What are the 12 categories devised?
Is the corpus annotated with a phonetic transcription?
Is the corpus annotated with Part-of-Speech tags?

what evaluation methods are discussed?
what are the off-the-shelf systems discussed in the paper?

How is "hirability" defined?
Have the candidates given their consent to have their videos used for the research?
Do they analyze if their system has any bias?
Is there any ethical consideration in the research?

What low-resource languages were used in this work?
What classification task was used to evaluate the cross-lingual adaptation method described in this work?

How many parameters does the presented model have?
How do they measure the quality of detection?
What previous approaches are considered?

How is the back-translation model trained?
Are the rules dataset specific?
How many rules had to be defined?
What datasets are used in this paper?

How much labeled data is available for these two languages?
What was performance of classifiers before/after using distant supervision?
What classifiers were used in experiments?
In which countries are Hausa and Yor\`ub\'a spoken?

What is the agreement score of their annotated dataset?
What is the size of the labelled dataset?
Which features do they use to model Twitter messages?
Do they allow for messages with vaccination-related key terms to be of neutral stance?

How big are the datasets used?
Is this a span-based (extractive) QA task?
Are the contexts in a language different from the questions?

What datasets are used for training/testing models?
How better is gCAS approach compared to other approaches?
What is specific to gCAS cell?

What dataset do they evaluate their model on?
What is the source of external knowledge?

Which of their proposed attention methods works better overall?
Which dataset of texts do they use?
Do they measure how well they perform on longer sequences specifically?
Which other embeddings do they compare against?

What were the sizes of the test sets?
What training data did they use?

What domains do they experiment with?

What games are used to test author's methods?
How is the domain knowledge transfer represented as knowledge graph?

What was the baseline?
Which datasets are used?
Which six languages are experimented with?
What shallow local features are extracted?

Do they compare results against state-of-the-art language models?
Do they integrate the second-order term in the mLSTM?
Which dataset do they train their models on?

How much does it minimally cost to fine-tune some model according to benchmarking framework?
What models are included in baseline benchmarking results?

did they compare with other evaluation metrics?
which datasets were used in validation?

It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?

What multimodal representations are used in the experiments?
How much better is inference that has addition of image representation compared to text-only representations?
How they compute similarity between the representations?
How big is vSTS training data?

Which evaluation metrics do they use for language modelling?
Do they do quantitative quality analysis of learned embeddings?
Do they evaluate on downstream tasks?
Which corpus do they use?

How much more accurate is the model than the baseline?

How big is improvement over the old  state-of-the-art performance on CoNLL-2009 dataset?
What is new state-of-the-art performance on CoNLL-2009 dataset?
How big is CoNLL-2009 dataset?
What different approaches of encoding syntactic information authors present?
What are two strong baseline methods authors refer to?

How many category tags are considered?
What domain does the dataset fall into?
What ASR system do they use?
What is the state of the art?

How big are datasets used in experiments?
What previously annotated databases are available?

Do they address abstract meanings and concepts separately?
Do they argue that all words can be derived from other (elementary) words?
Do they break down word meanings into elementary particles as in the standard model of quantum theory?

How big is the dataset used?

How they prove that multi-head self-attention is at least as powerful as convolution layer?
Is there a way of converting existing convolution layers into self-attention to perform very same convolution?
What authors mean by sufficient number of heads?
Is there any nonnumerical experiment that also support author's claim, like analysis of attention layers in publicly available networks?
What numerical experiments they perform?

What dataset is used?
What language do they look at?

What dierse domains and languages are present in new datasets?

Are their corpus and software public?

How are EAC evaluated?
What are the currently available datasets for EAC?
What are the research questions posed in the paper regarding EAC studies?

What evaluation metrics did they use?
What is triangulation?
What languages are explored in this paper?

Did they experiment with the dataset on some tasks?

How better does the hybrid tiled CNN model perform than its counterparts?

Do they use pretrained word embeddings?

Do they use skipgram version of word2vec?
What domains are considered that have such large vocabularies?
Do they perform any morphological tokenization?
How many nodes does the cluster have?

What data do they train the language models on?
Do they report BLEU scores?
What languages do they use?
What architectures are explored to improve the seq2seq model?

Why is this work different from text-only UNMT?

What is baseline used?
Did they evaluate against baseline?
How they evaluate their approach?

How large is the corpus?
Which document classifiers do they experiment with?
How large is the dataset?

What evaluation metrics are used to measure diversity?
How is some information lost in the RNN-based generation models?

What is the model accuracy?
How do the authors define fake news?
What dataset is used?

Did they use other evaluation metrics?
What was their perplexity score?
What languages are explored in this paper?
What parallel corpus did they use?

What datasets are used for experiments on three other tasks?

In which setting they achieve the state of the art?
What accuracy do they approach with their proposed method?
What they formulate the question generation as?
What two main modules their approach consists of?

Are there syntax-agnostic SRL models before?
What is the biaffine scorer?

What languages are were included in the dataset of hateful content?
How was reliability measured?
How did the authors demonstrate that showing a hate speech definition caused annotators to partially align their own opinion with the definition?
What definition was one of the groups was shown?
Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?
How were potentially hateful messages identified?

Which embeddings do they detect biases in?

What does their system consist of?
What are the two PharmaCoNER subtasks?

What neural language models are explored?
How do they perform data augmentation?
What proportion of negative-examples do they use?

Do the authors mention any possible confounds in their study?
What are the characteristics of the city dialect?
What are the characteristics of the rural dialect?

What are their baseline methods?
Which datasets are used for evaluation?

Which of the model yields the best performance?
What is the performance of the models on the tasks?
How is the data automatically generated?

Do they fine-tune the used word embeddings on their medical texts?
Which word embeddings do they use to represent medical visits?
Do they explore similarity of texts across different doctors?
Which clustering technique do they use on partients' visits texts?

What is proof that proposed functional form approximates well generalization error in practice?
How is proposed functional form constructed for some model?

What other non-neural baselines do the authors compare to?

How much better is performance of proposed approach compared to greedy decoding baseline?
What environment is used for self-critical sequence training?
What baseline function is used in REINFORCE algorithm?
What baseline model is used for comparison?

Is Aristo just some modern NLP model (ex. BERT) finetuned od data specific for this task?
On what dataset is Aristo system trained?

Does they focus on any specific product/service domain?
What are the baselines?

How is fluency of generated text evaluated?
How is faithfulness of the resulting text evaluated?
How are typing hints suggested?
What is the effectiveness plan generation?
How is neural planning component trained?

How do they evaluate interpretability in this paper?

How much better performing is the proposed method over the baselines?
What baselines are the proposed method compared against?
What dataset/corpus is this work evaluated over?

How many roles are proposed?

What language technologies have been introduced in the past?

Does the dataset contain non-English reviews?
Does the paper report the performance of the method when is trained for more than 8 epochs?

What do the correlation demonstrate?
On Twitter, do the demographic attributes and answers show more correlations than on Yahoo! Answers?
How many demographic attributes they try to predict?

What evaluation metrics do they use?
How do they define local variance?

How do they quantify alignment between the embeddings of a document and its translation?
Does adversarial learning have stronger performance gains for text classification, or for NER?
Do any of the evaluations show that adversarial learning improves performance in at least two different language families?

what experiments are conducted?
what opportunities are highlighted?
how do they measure discussion quality?
do they use a crowdsourcing platform?

what were the baselines?

Which soft-selection approaches are evaluated?
Is the model evaluated against the baseline also on single-aspect sentences?
Is the accuracy of the opinion snippet detection subtask reported?

What were the baselines?
What metric was used in the evaluation step?
What did they pretrain the model on?
What does the data cleaning and filtering process consist of?
What unlabeled corpus did they use?

How efective is MCDN for ambiguous and implicit causality inference compared to state-of-the-art?
What performance did proposed method achieve, how much better is than previous state-of-the-art?
What was previous state-of-the-art approach?
How is Relation network used to infer causality at segment level?

What is the TREC-CAR dataset?

How does their model utilize contextual information for each work in the given sentence in a multi-task setting? setting?
What metris are used for evaluation?
How better is proposed model compared to baselines?
What are the baselines?
How big is slot filing dataset?

Which machine learning models do they use to correct run-on sentences?
How large is the dataset they generate?

What are least important components identified in the the training of VQA models?
What type of experiments are performed?
What components are identified as core components for training VQA models?

what approaches are compared?

What model do they use a baseline to estimate satisfaction?

what semantically conditioned models did they compare with?

Do they differentiate insights where they are dealing with learned or engineered representations?
Do they show an example of usage for INFODENS?
What kind of representation exploration does INFODENS provide?

What models do they compare to?

What is the optimal trading strategy based on reinforcement learning?
Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?
Which tweets are used to output the daily sentiment signal?
What is the baseline machine learning prediction approach?

What are the weaknesses of their proposed interpretability quantification method?
What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?

How do they generate a graphic representation of a query from a query?
How do they gather data for the query explanation problem?
Which query explanation method was preffered by the users in terms of correctness?
Do they conduct a user study where they show an NL interface with and without their explanation?
How do the users in the user studies evaluate reliability of a NL interface?

What was the task given to workers?
How was lexical diversity measured?
How many responses did they obtain?
What crowdsourcing platform was used?

Are results reported only for English data?
Which existing models does this approach outperform?
What human evaluation method is proposed?

How is human evaluation performed, what were the criteria?
What automatic metrics are used?
What other kinds of generation models are used in experiments?
How does discrete latent variable has an explicit semantic meaning to improve the CVAE on short-text conversation?

What news dataset was used?
How do they determine similarity between predicted word and topics?
What is the language model pre-trained on?

What languages are represented in the dataset?
Which existing language ID systems are tested?
How was the one year worth of data collected?

Which language family does Mboshi belong to?
Does the paper report any alignment-only baseline?
What is the dataset used in the paper?
How is the word segmentation task evaluated?

What are performance compared to former models?
How faster is training and decoding compared to former models?

What datasets was the method evaluated on?

Is the model evaluated against a baseline?
How many people are employed for the subjective evaluation?

What other embedding models are tested?
How is performance measured?
How are rare words defined?

What datasets are used to evaluate the model?

What other datasets are used?
What is the size of the dataset?
What is the source of the dataset?
What were the baselines?

How do they show that acquiring names of places helps self-localization?
How do they evaluate how their model acquired words?
Which method do they use for word segmentation?
Does their model start with any prior knowledge of words?

What were the baselines?
What metadata is included?
How many expert journalists were there?

Do the images have multilingual annotations or monolingual ones?
Could you learn such embedding simply from the image annotations and without using visual information?
How much important is the visual grounding in the learning of the multilingual representations?

How is the generative model evaluated?

How do they evaluate their method?
What is an example of a health-related tweet?

Was the introduced LSTM+CNN model trained on annotated data in a supervised fashion?

What is the challenge for other language except English
How many categories of offensive language were there?
How large was the dataset of Danish comments?
Who were the annotators?

Is is known whether Sina Weibo posts are censored by humans or some automatic classifier?

Which matching features do they employ?

How often are the newspaper websites crawled daily?

How much better in terms of JSD measure did their model perform?
What does the Jensen-Shannon distance measure?

Which countries and languages do the political speeches and manifestos come from?
Do changes in policies of the political actors account for all of the mistakes the model made?
What model are the text features used in to provide predictions?

By how much does their method outperform the multi-head attention model?
How large is the corpus they use?
Does each attention head in the decoder calculate the same output?

Which distributional methods did they consider?
Which benchmark datasets are used?
What hypernymy tasks do they study?

Do they repot results only on English data?
What were the variables in the ablation study?
How many shared layers are in the system?
How many additional task-specific layers are introduced?

What is barycentric Newton diagram?

Do they propose any solution to debias the embeddings?
How are these biases found?

How many layers of self-attention does the model have?
Is human evaluation performed?
What are the three datasets used?

Did they experiment with the corpus?

How were the feature representations evaluated?
What linguistic features were probed for?

Does the paper describe experiments with real humans?

What are bottleneck features?
What languages are considered?

Do they compare speed performance of their model compared to the ones using the LID model?
How do they obtain language identities?

What other multimodal knowledge base embedding methods are there?

What is the data selection paper in machine translation

Do they compare computational time of AM-softmax versus Softmax?
Do they visualize the difference between AM-Softmax and regular softmax?

what metrics were used for evaluation?
what are the state of the art methods?

What datasets do they use for the tasks?

What evaluation metrics do they use?
What performance is achieved?
Do they use BERT?
What is their baseline?
Which two datasets is the system tested on?

Which four languages do they experiment with?

Does DCA or GMM-based attention perform better in experiments?
How they compare varioius mechanisms in terms of naturalness?

What evaluation metric is used?
What datasets are used?

Is the origin of the dialogues in corpus some video game and what game is that?
Is any data-to-text generation model trained on this new corpus, what are the results?
How the authors made sure that corpus is clean despite being crowdsourced?

Do they build a generative probabilistic language model for sign language?

Does CLSTM have any benefits over BERT?

How do they obtain human generated policies?
How many agents do they ensemble over?

What is the task of slot filling?

Do they report results only on English data?
Does this system improve on the SOTA?
How are the potentially relevant text fragments identified?
What algorithm and embedding dimensions are used to build the task-specific embeddings?
What data is used to build the task-specific embeddings?

Do they evaluate the syntactic parses?
What knowledge bases do they use?

Which dataset do they use?

What pre-trained models did they compare to?
How does the fusion method work?
What dataset did they use?
What benchmarks did they experiment on?

What were the evaluation metrics used?
What is the size of the dataset?
What multi-domain dataset is used?
Which domains did they explored?

Do they report results only on English data?
Which is the best performing method?
What size are the corpora?
What is a self-compiled corpus?
What are the 12 AV approaches which are examined?

how was annotation done?
what is the source of the new dataset?

Do the authors give examples of positive and negative sentiment with regard to the virus?
Which word frequencies reflect on the psychology of the twitter users, according to the authors?
Do they specify which countries they collected twitter data from?
Do they collect only English data?

How do they measure correlation between the prediction and explanation quality?
Does the Agent ask for a value of a variable using natural language generated text?

What models does this overview cover?

What datasets are used to evaluate the introduced method?
What are the results achieved from the introduced method?

How do they incorporate human advice?
What do they learn jointly?

Is this an English-language dataset?
What affective-based features are used?
What conversation-based features are used?

What are the evaluation metrics used?

Do they report results only on English datasets?

What datasets or tasks do they conduct experiments on?

How big is performance improvement proposed methods are used?
How authors create adversarial test set to measure model robustness?

Do they compare with the MAML algorithm?

By how much does transfer learning improve performance on this task?
What baseline is used?
What topic clusters are identified by LDA?
What are the near-offensive language categories?

How much do they outperform previous state-of-the-art?
How do they generate the auxiliary sentence?

Have any baseline model been trained on this abusive language dataset?
How big are this dataset and catalogue?
What is open website for cataloguing abusive language data?

how many speeches are in the dataset?

How big is the provided treebank?
What is LAS metric?

is the dataset balanced across the four languages?
what evaluation metrics were used?
what dataset was used?

What are the citation intent labels in the datasets?
What is the size of ACL-ARC datasets?

Is the affect of a word affected by context?

asdfasdaf
asdfasdf
asdfasd
asdf

How is quality of annotation measured?

On what data is the model evaluated?

What accuracy score do they obtain?
What is their baseline model?
What is the size of the dataset?
What is the 12 class bilingual text?

Which languages do they focus on?

Which are the sequence model architectures this method can be transferred across?
 What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?

What is the metric that is measures in this paper?

Do they only test on one dataset?
What baseline decoder do they use?

Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?
What dataset was used in this work?

How do they evaluate the sentence representations?
What are the two decoding functions?

How is language modelling evaluated?

Why there is only user study to evaluate the model?

What datasets are used to evaluate the model?

How did they gather the data?
What are the domains covered in the dataset?

What is their baseline?

Do they use the cased or uncased BERT model?
How are the two different models trained?
How long is the dataset?

How big are negative effects of proposed techniques on high-resource tasks?
What datasets are used for experiments?
Are this techniques used in training multilingual models, on what languages?
What baselines non-adaptive baselines are used?

What text sequences are associated with each vertex?
How long does it take for the model to run?

Do they report results only on English data?
Which other unsupervised models are used for comparison?
What metric is used to measure performance?
How do the n-gram features incorporate compositionality?

Which dataset do they use?

How do Zipf and Herdan-Heap's laws differ?

What was the best performing baseline?
Which approaches did they use?
What is the size of the dataset?
Did they use a crowdsourcing platform for the summaries?

How are the synthetic examples generated?

Do they measure the number of created No-Arc long sequences?
By how much does the new parser outperform the current state-of-the-art?

Do they evaluate only on English datasets?
What experimental evaluation is used?
How is the architecture fault-tolerant?
Which elements of the platform are modular?

What is the source of memes?
Is the dataset multimodal?
How is each instance of the dataset annotated?

Which dataset do they use for text modelling?
Do they compare against state of the art text generation?
How do they evaluate generated text quality?

Was the system only evaluated over the second shared task?

Could you tell me more about the metrics used for performance evaluation?
which tasks are used in BLUE benchmark?

What are the tasks that this method has shown improvements?
Why does the model improve in monolingual spaces as well?

What are the categories being extracted?

Do the authors test their annotation projection techniques on tasks other than AMR?
How is annotation projection done when languages have different word order?

What is the reasoning method that is used?
What KB is used in this work?
What's the precision of the system?

How did they measure effectiveness?

Which of the two ensembles yields the best performance?
What are the two ways of ensembling BERT and E-BERT?
How is it determined that a fact is easy-to-guess?

How is dependency parsing empirically verified?
How are different network components evaluated?
What are the performances obtained for PTB and CTB?
What are the models used to perform constituency and dependency parsing?

Is the proposed layer smaller in parameters than a Transformer?

What is the new initialization method proposed in this paper?
How was a quality control performed so that the text is noisy but the annotations are accurate?

Is it a neural model? How is it trained?

How do people engage in Twitter threads on different types of news?
How are the clusters related to security, violence and crime identified?

What are the features of used to customize target user interaction?

How they evaluate quality of generated output?
What automated metrics authors investigate?

What supervised models are experimented with?
Who annotated the data?
What are the four forums the data comes from?

How do they obtain parsed source sentences?
What kind of encoders are used for the parsed source sentence?
Whas is the performance drop of their model when there is no parsed input?

How were their results compared to state-of-the-art?

What supports the claim that injected CNN into recurent units will enhance ability of the model to catch local context and reduce ambiguities?
How is CNN injected into recurent units?
Are there some results better than state of the art on these tasks?
Do experiment results show consistent significant improvement of new approach over traditional CNN and RNN models?
What datasets are used for testing sentiment classification and reading comprehension?

So we do not use pre-trained embedding in this case?

How are sentence embeddings incorporated into the speech recognition system?

How different is the dataset size of source and target?

How do you find the entity descriptions?

How is OpenBookQA different from other natural language QA?

At what text unit/level were documents processed?
What evaluation metric were used for presenting results?
Was the structure of regulatory filings exploited when training the model?
What type of documents are supported by the annotation platform?

What are the state-of-the-art models for the task?

Which datasets are used for evaluation?

What are the strong baselines you have?

What are causal attribution networks?

How accurate is their predictive model?

How large language sets are able to be explored using this approach?

how did they ask if a tweet was racist?

What other cross-lingual approaches is the model compared to?
What languages are explored?

How many human subjects were used in the study?

How does the model compute the likelihood of executing to the correction semantic denotation?

Which conventional alignment models do they use as guidance?
Which dataset do they use?

On average, by how much do they reduce the diarization error?
Do they compare their algorithm to voting without weights?
How do they assign weights between votes in their DOVER algorithm?

What are state of the art methods authors compare their work with?

What are the baselines model?

What is the architecture of the model?
What languages are explored in the work?

What is the state-of-the-art neural coreference resolution model?

How much improvement do they get?
How large is the dataset?
What features do they extract?

What they use as a metric of finding hot spots in meeting?
Is this approach compared to some baseline?
How big is ICSI meeting corpus?
What annotations are available in ICSI meeting corpus?

Is such bias caused by bad annotation?

How do they determine similar environments for fragments in their data augmentation scheme?
Do they experiment with language modeling on large datasets?
Which languages do they test on?

What limitations are mentioned?
What examples of applications are mentioned?
Did they crowdsource the annotations?

Why they conclude that the usage of Gated-Attention provides no competitive advantage against concatenation in this setting?

What was the best team's system?
What are the baselines?

What semantic features help in detecting whether a piece of text is genuine or generated? of
Which language models generate text that can be easier to classify as genuine or generated?
Is the assumption that natural language is stationary and ergodic valid?

Which models do they try out?

Do they compare executionttime of their model against other models?
What is the memory footprint decrease of their model in comparison to other models?

What architectural factors were investigated?

Any other bias may be detected?

What is the introduced meta-embedding method introduced in this paper?

How long are dialogue recordings used for evaluation?

What do the models that they compare predict?

What SMT models did they look at?
Which NMT models did they experiment with?

How big PIE datasets are obtained from dictionaries?
What compleentary PIE extraction methods are used to increase reliability further?
Are PIEs extracted automatically subjected to human evaluation?
What dictionaries are used for automatic extraction of PIEs?

Are experiments performed with any other pair of languages, how did proposed method perform compared to other models?
Is pivot language used in experiments English or some other language?
What are multilingual models that were outperformed in performed experiment?

What are the common captioning metrics?

Which English domains do they evaluate on?

What is the road exam metric?
What are the competing models?

How is the input triple translated to a slot-filling task?

Is model compared against state of the art models on these datasets?
How is octave convolution concept extended to multiple resolutions and octaves?

Does this paper address the variation among English dialects regarding these hedges?

On which dataset is model trained?
How is module that analyzes behavioral state trained?

Can the model add new relations to the knowledge graph, or just new entities?

How large is the dataset?

Why is a Gaussian process an especially appropriate method for this classification problem?

Do the authors do manual evaluation?

What datasets did they use?

Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?
What is the training and test data used?
Was performance of the weakly-supervised model compared to the performance of a supervised model?

Do the tweets come from a specific region?

Did they experiment with the corpus?
What writing styles are present in the corpus?
How did they determine the distinct classes?

Do they jointly tackle multiple tagging problems?
How many parameters does their CNN have?
How do they confirm their model working well on out-of-vocabulary problems?

What approach does this work propose for the new task?
What is the new task proposed in this work?

Which news organisations are the headlines sourced from?

What meta-information is being transferred?
What datasets are used to evaluate the approach?

Does their solution involve connecting images and text?
Which model do they use to generate key messages?

What experiments they perform to demonstrate that their approach leads more accurate region based representations?
How they indentify conceptual neighbours?

What experiment result led to conclussion that reducing the number of layers of the decoder does not matter much?
How much is performance hurt when using too small amount of layers in encoder?
What was previous state of the art model for automatic post editing?

What neural machine translation models can learn in terms of transfer learning?

Did they experiment on the proposed task?
Is annotation done manually?
How large is the proposed dataset?

How large is the dataset?
How is the dataset created?

What is binary variational dropout?

Which strategies show the most promise in deterring these attacks?

What are baseline models on WSJ eval92 and LibriSpeech test-clean?

Do they use the same architecture as LSTM-s and GRUs with just replacing with the LAU unit?

So this paper turns unstructured text inputs to parameters that GNNs can read?

What other models are compared to the Blending Game?
What empirical data are the Blending Game predictions compared to?

How does the semi-automatic construction process work?
Does the paper report translation accuracy for an automatic translation model for Tunisian to Arabish words?

Did participants behave unexpectedly?
Was this experiment done in a lab?

How long is new model trained on 3400 hours of data?

How much does HAS-QA improve over baselines?

What does "explicitly leverages their probabilistic correlation to guide the training process of both models" mean?

How does this compare to contextual embedding methods?

Does the new system utilize pre-extracted bounding boxes and/or features?
To which previous papers does this work compare its results?

Do they consider other tasks?

What were the model's results on flood detection?
What dataset did they use?

What exactly is new about this stochastic gradient descent algorithm?

What codemixed language pairs are evaluated?
How do they compress the model?
What is the multilingual baseline?

Which features do they use?
By how much do they outperform state-of-the-art solutions on SWDA and MRDA?

What type and size of word embeddings were used?
What data was used to build the word embeddings?

How are templates discovered from training data?

What is WNGT 2019 shared task?

Do they use pretrained word representations in their neural network models?
How do they combine the two proposed neural network models?
Which dataset do they evaluate grammatical error correction on?

How many users/clicks does their search engine have?

what was their baseline comparison?

Was any variation in results observed based on language typology?
Does the work explicitly study the relationship between model complexity and linguistic structure encoding?

Which datasets are used in this work?

Does the training dataset provide logical form supervision?
What is the difference between the full test set and the hard test set?

How is the discriminative training formulation different from the standard ones?
How are the two datasets artificially overlapped?

What baseline system is used?
What type of lexical, syntactic, semantic and polarity features are used?

How does nextsum work?

Can the approach be generalized to other technical domains as well?

How many tweets were manually labelled?

What dataset they use for evaluation?

What is the source of the tables?

Which regions of the United States do they consider?
Why did they only consider six years of published books?

What state-of-the-art general-purpose pretrained models are made available under the unified API?

How is performance measured?

What models are included in the toolkit?

Is there any human evaluation involved in evaluating this famework?

How big is multilingual dataset?

How big is dataset used for fine-tuning BERT?

How big are datasets for 2019 Amazon Alexa competition?
What is novel in author's approach?

How large is the Dialog State Tracking Dataset?

What dataset is used for train/test of this method?

How much is the gap between using the proposed objective and using only cross-entropy objective?

What is the multi-instance learning?

How many domains of ontologies do they gather data from?

How is the semi-structured knowledge base created?

what is the practical application for this paper?

Do they use a neural model for their task?

What's the method used here?

By how much does their method outperform state-of-the-art OOD detection?

What are dilated convolutions?

what was the evaluation metrics studied in this work?

Do they analyze ELMo?

what are the three methods presented in the paper?

what datasets did the authors use?

What are three possible phases for language formation?

How many parameters does the model have?

Do the experiments explore how various architectures and layers contribute towards certain decisions?

What social media platform does the data come from?

How much performance improvements they achieve on SQuAD?

Do the authors perform experiments using their proposed method?

What NLP tasks do the authors evaluate feed-forward networks on?

What are three challenging tasks authors evaluated their sequentially aligned representations?

What is the difference in findings of Buck et al? It looks like the same conclusion was mentioned in Buck et al..

What is the baseline?
What is the unsupervised task in the final layer?
How many supervised tasks are used?
What is the network architecture?

Is the proposed model more sensitive than previous context-aware models too?
In what ways the larger context is ignored for the models that do consider larger context?

What does recurrent deep stacking network do?

Does the latent dialogue state heklp their model?
Do the authors test on datasets other than bAbl?
What is the reward model for the reinforcement learning appraoch?

Does this paper propose a new task that others can try to improve performance on?

What knowledge base do they use?
How big is their dataset?
What task do they evaluate on?

Do some pretraining objectives perform better than others for sentence level understanding tasks?

Did the authors try stacking multiple convolutional layers?
How many feature maps are generated for a given triple?
How does the number of parameters compare to other knowledge base completion models?

which multilingual approaches do they compare with?
what are the pivot-based baselines?
which datasets did they experiment with?
what language pairs are explored?

what ner models were evaluated?
what is the source of the news sentences?
did they use a crowdsourcing platform for manual annotations?

what are the topics pulled from Reddit?
What predictive model do they build?

What accuracy does the proposed system achieve?
What crowdsourcing platform is used?

How do they match words before reordering them?
On how many language pairs do they show that preordering assisting language sentences helps translation quality?
Which dataset(s) do they experiment with?

Which information about text structure is included in the corpus?
Which information about typography is included in the corpus?

On which benchmarks they achieve the state of the art?
What they use in their propsoed framework?
What does KBQA abbreviate for
What is te core component for KBQA?

What experiments are proposed to test that upper layers produce context-specific embeddings?
How do they calculate a static embedding for each word?

What is the performance of BERT on the task?
What are the other algorithms tested?
Does BERT reach the best performance among all the algorithms compared?
What are the clinical datasets used in the paper?

how is model compactness measured?
what was the baseline?
what evaluation metrics were used?
what datasets did they use?

What is the interannotator agreement for the human evaluation?
Who were the human evaluators used?
Is the template-based model realistic?
Is the student reflection data very different from the newspaper data?
What is the recent abstractive summarization method in this paper?

Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models?
What state-of-the-art compression techniques were used in the comparison?

What evaluations methods do they take?
What is the size of the dataset?
Which methods are considered to find examples of biases and unwarranted inferences??
What biases are found in the dataset?

What discourse relations does it work best/worst for?
How much does this model improve state-of-the-art?

Where is a question generation model used?

Were any of these tasks evaluated in any previous work?

Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?
Which demographic dimensions of people do they obtain?
How do they obtain psychological dimensions of people?

What is the baseline?
Is the data de-identified?
What embeddings are used?

What datasets did they use for evaluation?
On top of BERT does the RNN layer work better or the transformer layer?

How was this data collected?
What is the average length of dialog?

How are models evaluated in this human-machine communication game?
How many participants were trying this communication game?
What user variations have been tested?
What are the baselines used?

Do they use off-the-shelf NLP systems to build their assitant?
How does the IPA label data after interacting with users?
What kind of repetitive and time-consuming activities does their assistant handle?

How was the audio data gathered?
What is the GhostVLAD approach?
Which 7 Indian languages do they experiment with?

What datasets do they evaluate on?
Do they evaluate only on English datasets?
What is the invertibility condition?

Do they show on which examples how conflict works better than attention?
Which neural architecture do they use as a base for their attention conflict mechanisms?
On which tasks do they test their conflict method?

Do they use graphical models?
What are the sources of the datasets?
What metric is used for evaluation?

Which eight NER tasks did they evaluate on?
What in-domain text did they use?

Does their framework automatically optimize for hyperparameters?
Does their framework always generate purely attention-based models?
Do they test their framework performance on commonly used language pairs, such as English-to-German?
Which languages do they test on for the under-resourced scenario?

Are the automatically constructed datasets subject to quality control?
Do they focus on Reading Comprehension or multiple choice question answering?
After how many hops does accuracy decrease?
How do they control for annotation artificats?
Is WordNet useful for taxonomic reasoning for this task?

How do they perform multilingual training?
What languages are evaluated?
Does the model have attention?
What architecture does the decoder have?
What architecture does the encoder have?
What is MSD prediction?
What type of inflections are considered?

Do they use attention?
What other models do they compare to?
What is the architecture of the span detector?

What evaluation metric do they use?

What are the results from these proposed strategies?
What are the baselines?
What are the two new strategies?

Do they report results only on English data?
How much better than the baseline is LiLi?
What baseline is used in the experiments?
In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?
What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation?
What are the components of the general knowledge learning engine?

How many labels do the datasets have?
What is the architecture of the model?
What are the baseline methods?
What are the source and target domains?

Did they use a crowdsourcing platform for annotations?
How do they deal with unknown distribution senses?

Do they report results only on English data?
What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?
How id Depechemood trained?
How are similarities and differences between the texts from violent and non-violent religious groups analyzed?
How are prominent topics idenified in Dabiq and Rumiyah?

Are the images from a specific domain?
Which datasets are used?
Which existing models are evaluated?
How is diversity measured?

What state-of-the-art deep neural network is used?
What boundary assembling method is used?
What are previous state of the art results?

What is the model performance on target language reading comprehension?
What source-target language pairs were used in this work?
What model is used as a baseline?
what does the model learn in zero-shot setting?

Do they inspect their model to see if their model learned to associate image parts with words related to entities?
Does their NER model learn NER from both text and images?
Which types of named entities do they recognize?
Can named entities in SnapCaptions be discontigious?
How large is their MNER SnapCaptions dataset?

What is masked document generation?
Which of the three pretraining tasks is the most helpful?

What useful information does attention capture?
What datasets are used?
In what cases is attention different from alignment?

How do they calculate variance from the model outputs?
How much data samples do they start with before obtaining the initial model labels?
Which model do they use for end-to-end speech recognition?
Which dataset do they use?

Which baselines did they compare against?

What baselines did they consider?
What types of social media did they consider?

How was the dataset annotated?
Which classifiers are evaluated?
What is the size of this dataset?
Where does the data come from?

What are method improvements of F1 for paraphrase identification?
What are method's improvements of F1 for NER task for English and Chinese datasets?
What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?
How are weights dynamically adjusted?

Ngrams of which length are aligned using PARENT?
How many people participated in their evaluation study of table-to-text models?
By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?

Which stock market sector achieved the best performance?

What languages pairs are used in machine translation?
What sentiment classification dataset is used?
What pooling function is used?

Do they report results only on English?
What neural network modules are included in NeuronBlocks?
How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?

what datasets did they use?
what ml based approaches were compared?

Is pre-training effective in their evaluation?
What parallel corpus did they use?

How much does their model outperform existing models?
What do they mean by global and local context?

What are the 18 propaganda techniques?
What dataset was used?
What was the baseline for this task?

What is a second order co-ocurrence matrix?
How many humans participated?
What embedding techniques are explored in the paper?

Do the authors also try the model on other datasets?
What word level and character level model baselines are used?

By how much do they improve the efficacy of the attention mechanism?
How were the human judgements assembled?

Did they only experiment with one language pair?

Which other approaches do they compare their model with?
What results do they achieve using their proposed approach?
How do they combine a deep learning model with a knowledge base?

What are the models used for the baseline of the three NLP tasks?
How is non-standard pronunciation identified?

Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon?

What novel PMI variants are introduced?
What semantic and syntactic tasks are used as probes?
What are the disadvantages to clipping negative PMI?
Why are statistics from finite corpora unreliable?

what is the domain of the corpus?
what challenges are identified?
what is the size of the speech corpus?

Which two pairs of ERPs from the literature benefit from joint training?
What datasets are used?

which datasets did they experiment with?
which languages are explored?

Do they use number of votes as an indicator of preference?
What does a node in the network approach repesent?
Which dataset do they use?

What kind of celebrities do they obtain tweets from?

How did they extend LAMA evaluation framework to focus on negation?

What summarization algorithms did the authors experiment with?
What evaluation metrics were used for the summarization task?
What clustering algorithms were used?
What evaluation metrics are looked at for classification tasks?
What methods were used for sentence classification?
What is the average length of the sentences?
What is the size of the real-life dataset?

What are the language pairs explored in this paper?

Do they experiment with the toolkits?

Have they made any attempt to correct MRC gold standards according to their findings?
What features are absent from MRC gold standards that can result in potential lexical ambiguity?
What modern MRC gold standards are analyzed?
How does proposed qualitative annotation schema looks like?

How many tweets were collected?
What language is explored in this paper?

What are the baselines?
What is the attention module pretrained on?

How long of dialog history is captured?

What evaluation metrics were used?
What was the score of the proposed model?
What was the previous best model?
Which datasets did they use for evaluation?

What hyperparameters are explored?
What Named Entity Recognition dataset is used?
What sentiment analysis dataset is used?
Do they test both skipgram and c-bow?

What is the state-of-the-art model for the task?
What is the strong baseline?

what aspects of conversation flow do they look at?
what debates dataset was used?

what is the state of the art?
what standard dataset were used?

Do they perform error analysis?
How do their results compare to state-of-the-art?
What is the Random Kitchen Sink approach?

what are the baseline systems?

What word embeddings do they test?
How do they define similar equations?

What evaluation criteria and metrics were used to evaluate the generated text?

Do they evaluate only on English datasets?
What are the three steps to feature elimination?
How is the dataset annotated?
What dataset is used for this study?

what were their performance results?
where did they obtain the annotated clinical notes from?

Which architecture do they use for the encoder and decoder?
How does their decoder generate text?
Which dataset do they use?

What model is used to encode the images?
How is the sequential nature of the story captured?
Is the position in the sequence part of the input?
Do the decoder LSTMs all have the same weights?

Is fine-tuning required to incorporate these embeddings into existing models?
How are meaningful chains in the graph selected?

Do the authors also analyze transformer-based architectures?

Do they remove seasonality from the time series?
What is the dimension of the embeddings?
What dataset is used to train the model?

What is the previous state of the art?

Which text embedding methodologies are used?

Which race and gender are given higher sentiment intensity predictions?
What criteria are used to select the 8,640 English sentences?

what were the baselines?
what competitive results did they obtain?

By how much is performance improved with multimodality?
Is collected multimodal in cabin dataset public?

What is the performance reported for the best models in the VLSP 2018 and VLSP 2019 challenges?
Is the model tested against any baseline?
What is the language model combination technique used in the paper?
What are the deep learning architectures used in the task?

How much is performance improved on NLI?
Do they train their model starting from a checkpoint?
What BERT model do they test?

What downstream tasks are evaluated?
What is active learning?

what was the baseline?

How is segmentation quality evaluated?

How do they compare lexicons?

Is it possible to convert a cloze-style questions to a naturally-looking questions?

How larger are the training sets of these versions of ELMo compared to the previous ones?
What is the improvement in performance for Estonian in the NER task?

what is the state of the art on WSJ?

How did they obtain the OSG dataset?
How large is the Twitter dataset?

what is the size of the augmented dataset?

How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?

What dataset do they use to evaluate their method?

Why are current ELS's not sufficiently effective?

What is the best model?
How many sentences does the dataset contain?
Do the authors train a Naive Bayes classifier on their dataset?
What is the baseline?
Which machine learning models do they explore?
What is the size of the dataset?
What is the source of their dataset?
Do they try to use byte-pair encoding representations?
How many different types of entities exist in the dataset?
How big is the new Nepali NER dataset?
What is the performance improvement of the grapheme-level representation model over the character-level model?
Which models are used to solve NER for Nepali?

What language(s) is/are represented in the dataset?
What baseline model is used?
Which variation provides the best results on this dataset?
What are the different variations of the attention-based approach which are examined?
What dataset is used for this work?
What types of online harassment are studied?
What was the baseline?
What were the datasets used in this paper?

Is car-speak language collection of abstract features that classifier is later trained on?
Is order of "words" important in car speak language?
What are labels in car speak language dataset?
How big is dataset of car-speak language?
What is the performance of classifiers?
What classifiers have been trained?
How does car speak pertains to a car's physical attributes?

What topic is covered in the Chinese Facebook data?
How many layers does the UTCNN model have?
What topics are included in the debate data?
What is the size of the Chinese data?
Did they collected the two datasets?
What are the baselines?

What transfer learning tasks are evaluated?
What metrics are used for the STS tasks?
How much time takes its training?
How many GPUs are used for the training of SBERT?
How are the siamese networks trained?
What other sentence embeddings methods are evaluated?

What is the average length of the title text?
Which pretrained word vectors did they use?
What evaluation metrics are used?
Which shallow approaches did they experiment with?
Where do they obtain the news videos from?
What is the source of the news articles?

which non-english language had the best performance?
which non-english language was the had the worst results?
what datasets were used in evaluation?
what are the baselines?
how did the authors translate the reviews to other languages?
what dataset was used for training?

How do they demonstrate that this type of EEG has discriminative information about the intended articulatory movements responsible for speech?
What are the five different binary classification tasks?
How was the spatial aspect of the EEG signal computed?
What data was presented to the subjects to elicit event-related responses?
How many electrodes were used on the subject in EEG sessions?
How many subjects does the EEG data come from?

Do they report results only on English data?
What type of classifiers are used?
Which real-world datasets are used?
How are the interpretability merits of the approach demonstrated?
How are the accuracy merits of the approach demonstrated?
How is the keyword specific expectation elicited from the crowd?

Does the paper provide any case studies to illustrate how one can use Macaw for CIS research?
What functionality does Macaw provide?
What is a wizard of oz setup?
What interface does Macaw currently have?
What modalities are supported by Macaw?
What are the different modules in Macaw?

Do they report results only on English data?
What baseline model is used?
What news article sources are used?
How do they determine the exact section to use the input article?
What features are used to represent the novelty of news articles to entity pages?
What features are used to represent the salience and relative authority of entities?

Do they experiment with other tasks?
What baselines do they introduce?
How large is the corpus?
How was annotation performed?
How many documents are in the new corpus?
What baseline systems are proposed?

How did they obtain the dataset?
What activation function do they use in their model?
What baselines do they compare to?
How are chunks defined?
What features are extracted?
How many layers does their model have?
Was the approach used in this work to detect fake news fully supervised?
Based on this paper, what is the more predictive set of features to detect fake news?
How big is the dataset used in this work?
How is a "chunk of posts" defined in this work?
What baselines were used in this work?

What is the performance of their method?
Which evaluation methods are used?
What dataset is used in this paper?
Which other methods do they compare with?
How are sentences selected from the summary graph?

What models are used in the experiment?
What are the differences between this dataset and pre-existing ones?
In what language are the tweets?
What is the size of the new dataset?
What kinds of offensive content are explored?
What is the best performing model?
How many annotators participated?
What is the definition of offensive language?
What are the three layers of the annotation scheme?
How long is the dataset for each step of hierarchy?

Do the authors report results only on English data?
In the proposed metric, how is content relevance measured?
What different correlations result when using different variants of ROUGE scores?
What manual Pyramid scores are used?
What is the common belief that this paper refutes? (c.f. 'contrary to the common belief, ROUGE is not much [sic] reliable'

which existing strategies are compared?
what dataset was used?
what kinds of male and female words are looked at?
how is mitigation of gender bias evaluated?
what bias evaluation metrics are used?

What kind of questions are present in the dataset?
What baselines are presented?
What tasks were evaluated?
What language are the reviews in?
Where are the hotel reviews from?

What was the baseline used?
What are their results on both datasets?
What textual patterns are extracted?
Which annotated corpus did they use?
Which languages are explored in this paper?

what language does this paper focus on?
what evaluation metrics did they use?
by how much did their model improve?
what state of the art methods did they compare with?
what are the sizes of both datasets?

What are the distinctive characteristics of how Arabic speakers use offensive language?
How did they analyze which topics, dialects and gender are most associated with tweets?
How many annotators tagged each tweet?
How many tweets are in the dataset?
In what way is the offensive dataset not biased by topic, dialect or target?

What experiments are conducted?
What is the combination of rewards for reinforcement learning?
What are the difficulties in modelling the ironic pattern?
How did the authors find ironic data on twitter?
Who judged the irony accuracy, sentiment preservation and content preservation?

How were the tweets annotated?
Which SVM approach resulted in the best performance?
What are hashtag features?
How many tweets did they collect?
Which sports clubs are the targets?

Does this method help in sentiment classification task improvement?
For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMos embedding?
What are the black-box probes used?
What are improvements for these two approaches relative to ELMo-only baselines?
Which syntactic features are obtained automatically on downstream task data?

Do they report results only on English data?
What baseline approaches does this approach out-perform?
What datasets are used?
What alternative to Gibbs sampling is used?
How does this model overcome the assumption that all words in a document are generated from a single event?

How many users do they look at?
What do they mean by a person's industry?
What model did they use for their system?
What social media platform did they look at?
What are the industry classes defined in this paper?

Do they report results only on English data?
What are the hyperparameters of the bi-GRU?
What baseline is used?
What data is used in experiments?
What meaningful information does the GRU model capture, which traditional ML models do not?

What is the approach of previous work?
Is the lexicon the same for all languages?
How do they obtain the lexicon?
What evaluation metric is used?
Which languages are similar to each other?
Which datasets are employed for South African languages LID?
Does the paper report the performance of a baseline model on South African languages LID?
What are the languages represented in the DSL datasets?
Does the algorithm improve on the state-of-the-art methods?

What background knowledge do they leverage?
What are the three regularization terms?
What NLP tasks do they consider?
How do they define robustness of a model?

Are the annotations automatic or manually created?
Do the errors of the model reflect linguistic similarity between different L1s?
Is the dataset balanced between speakers of different L1s?
How long are the essays on average?

How large are the textual descriptions of entities?
What neural models are used to encode the text?
What baselines are used for comparison?
What datasets are used to evaluate this paper?

Which approach out of two proposed in the paper performed better in experiments?
What classification baselines are used for comparison?
What TIMIT datasets are used for testing?
How does this approach compares to the state-of-the-art results on these tasks?

What state-of-the-art results are achieved?
What baselines do they compare with?
What datasets are used in evaluation?
What is the tagging scheme employed?

How they extract "structured answer-relevant relation"?
How big are significant improvements?
What metrics do they use?
On what datasets are experiments performed?

What was the baseline model?
What dataset did they use?
What was their highest recall score?
What was their highest MRR score?

Does their model suffer exhibit performance drops when incorporating word importance?
How do they measure which words are under-translated by NMT models?
How do their models decide how much improtance to give to the output words?
Which model architectures do they test their word importance approach on?

Do they compare human-level performance to model performance for their dataset?
What are the weaknesses found by non-expert annotators of current state-of-the-art NLI models?
What data sources do they use for creating their dataset?
Do they use active learning to create their dataset?

Do the hashtag and SemEval datasets contain only English data?
What current state of the art method was used for comparison?
What set of approaches to hashtag segmentation are proposed?
How is the dataset of hashtags sourced?

How big is their created dataset?
Which data do they use as a starting point for the dialogue dataset?
What labels do they create on their dataset?
How do they select instances to their hold-out test set?

Which models/frameworks do they compare to?
Which classification algorithm do they use for s2sL?
Up to how many samples do they experiment with?
Do they use pretrained models?

Do they report results only on English datasets?
How do the authors examine whether a model is robust to noise or not?
What type of model is KAR?
Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?

What type of system does the baseline classification use?
What experiments were carried out on the corpus?
How many annotators tagged each text?
Where did the texts in the corpus come from?

What is the previous state-of-the-art in summarization?
What dataset do they use?
What other models do they compare to?
What language model architectures are used?

What are the user-defined keywords?
Does the method achieve sota performance on this dataset?
What are the baselines used in the paper?
What is the size of the Airbnb?

How better is performance compared to previous state-of-the-art models?
How does Gaussian-masked directional multi-head attention works?
What is meant by closed test setting?
What are strong baselines model is compared to?

Does the dataset feature only English language data?
What additional features and context are proposed?
What learning models are used on the dataset?
What examples of the difficulties presented by the context-dependent nature of online aggression do they authors give?

Do they report results only on English data?
What evidence do the authors present that the model can capture some biases in data annotation and collection?
Which publicly available datasets are used?
What baseline is used?
What new fine-tuning methods are presented?
What are the existing biases?
What biases does their model capture?
What existing approaches do they compare to?

What is the benchmark dataset?
What are the two neural embedding models?
which neural embedding model works better?
What is the degree of dimension reduction of the efficient aggregation method?

For which languages do they build word embeddings for?
How do they evaluate their resulting word embeddings?
What types of subwords do they incorporate in their model?
Which matrix factorization methods do they use?

Do they report results only on English data?
What experiments do they use to quantify the extent of interpretability?
Along which dimension do the semantically related words take larger values?
What is the additive modification to the objective function?

Which dataset do they use?
Do they evaluate their learned representations on downstream tasks?
Which representation learning architecture do they adopt?
How do they encourage understanding of literature as part of their objective function?

What are the limitations of existing Vietnamese word segmentation systems?
Why challenges does word segmentation in Vietnamese pose?
How successful are the approaches used to solve word segmentation in Vietnamese?
Which approaches have been applied to solve word segmentation in Vietnamese?

Which two news domains are country-independent?
How is the political bias of different sources included in the model?
What are the two large-scale datasets used?
What are the global network features which quantify different aspects of the sharing process?

Which datasets are used for evaluation?
What previous methods is their model compared to?
Did they use a crowdsourcing platform?
How was the dataset collected?

Which datasets do they use?
How effective is their NCEL approach overall?
How do they verify generalization ability?
Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?

Do the authors mention any downside of lemmatizing input before training ELMo?
What other examples of morphologically-rich languages do the authors give?
Why is lemmatization not necessary in English?
How big was the corpora they trained ELMo on?

What metrics are used for evaluation?
Do they use pretrained embeddings?
What dataset is used?
What is a bifocal attention mechanism?

What does the "sensitivity" quantity denote?
What end tasks do they evaluate on?
What is a semicharacter architecture?
Do they experiment with offering multiple candidate corrections and voting on the model output, since this seems highly likely to outperform a one-best correction?
Why is the adversarial setting appropriate for misspelling recognition?
Why do they experiment with RNNs instead of transformers for this task?
How do the backoff strategies work?

What baseline model is used?
Which additional latent variables are used in the model?
Which parallel corpora are used?
Overall, does having parallel data improve semantic role induction across multiple languages?
Do they add one latent variable for each language pair in their Bayesian model?
What does an individual model consist of?
Do they improve on state-of-the-art semantic role induction?

how many tags do they look at?
which algorithm was the highest performer?
how is diversity measured?
how large is the vocabulary?
what dataset was used?
what algorithms did they use?

How does their ensemble method work?
How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?
How do they show there is space for further improvement?

Do they report results only on English data?
What argument components do the ML methods aim to identify?
Which machine learning methods are used in experiments?
How is the data in the new corpus come sourced?
What argumentation phenomena encounter in actual data are now accounted for by this work?
What challenges do different registers and domains pose to this task?

who transcribed the corpus?
how was the speech collected?
what accents are present in the corpus?
what evaluation protocols are provided?
what age range is in the data?
what is the source of the data?

what topics did they label?
did they compare with other extractive summarization methods?
what datasets were used?

what levels of document preprocessing are looked at?
what keyphrase extraction models were reassessed?
how many articles are in the dataset?

Is this dataset publicly available for commercial use?
How many different phenotypes are present in the dataset?
What are 10 other phenotypes that are annotated?

What are the state of the art models?
Which benchmark datasets are used?
What are the network's baseline features?

What tasks are used for evaluation?
HOw does the method perform compared with baselines?
How does their model improve interpretability compared to softmax transformers?

What baseline method is used?
What details are given about the Twitter dataset?
What details are given about the movie domain dataset?
Which hand-crafted features are combined with word2vec?
What word-based and dictionary-based feature are used?
How are the supervised scores of the words calculated?

what dataset was used?
how many total combined features were there?
what pretrained word embeddings were used?

What evaluation metrics did look at?
What datasets are used?
What is the state of the art described in the paper?

What GAN models were used as baselines to compare against?
How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)?
Is the discriminator's reward made available at each step to the generator?

What is the algorithm used to create word embeddings?
What is the corpus used for the task?
How is evaluation performed?

What is a normal reading paradigm?
Did they experiment with this new dataset?
What kind of sentences were read?

why are their techniques cheaper to implement?
what data simulation techniques were introduced?
what is their explanation for the effectiveness of back-translation?
what dataset is used?
what language pairs are explored?
what language is the data in?

Does the experiments focus on a specific domain?
how many training samples do you have for training?
Do the answered questions measure for the usefulness of the answer?

What profile metadata is used for this analysis?
What are the organic and inorganic ways to show political affiliation through profile changes?
How do profile changes vary for influential leads and their followers over the social movement?

What evaluation metrics do they use?
What is the size of this dataset?
How do they determine if tweets have been used by journalists?

how small of a dataset did they train on?
what was their character error rate?
which lstm models did they compare with?

Do they use datasets with transcribed text or do they determine text from the audio?
By how much does their model outperform the state of the art results?
How do they combine audio and text sequences in their RNN?

What was the baseline?
By how much did they improve?
What dataset did they use?

What is the reported agreement for the annotation?
How many annotators participated?
What features are used?

What future possible improvements are listed?
Which qualitative metric are used for evaluation?
What is quantitative improvement of proposed method (the best variant) w.r.t. baseline (the best variant)?

How is "propaganda" defined for the purposes of this study?
What metrics are used in evaluation?
Which natural language(s) are studied in this paper?

Do they report results only on English data?
What objective function is used in the GAN?
Which datasets are used?

What metrics are used for evaluation?
What natural language(s) are the recipes written in?
What were their results on the new dataset?
What are the baseline models?
How did they obtain the interactions?
Where do they get the recipes from?

What were the baselines?
Does RoBERTa outperform BERT?
Which multiple datasets did they train on during joint training?
What were the previously reported results?
What is the size of SFU Review corpus?
What is the size of bioScope corpus?

Do they study numerical properties of their obtained vectors (such as orthogonality)?
How do they score phrasal compositionality?
Which translation systems do they compare against?

what are their results on the constructed dataset?
what evaluation metrics are reported?
what civil field is the dataset about?
what are the state-of-the-art models?
what is the size of the real-world civil case dataset?
what datasets are used in the experiment?

Do they model semantics
How do they identify discussions of LGBTQ people in the New York Times?
Do they analyze specific derogatory words?

What is novel about their document-level encoder?
What rouge score do they achieve?
What are the datasets used for evaluation?

What was their performance on emotion detection?
Which existing benchmarks did they compare to?
Which Facebook pages did they look at?

LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?
What is the benchmark dataset and is its quality high?
How do they detect spammers?

Do they use other evaluation metrics besides ROUGE?
What is their ROUGE score?
What are the baselines?

What datasets do they use?
What other factors affect the performance?
What are the benchmark attacking methods?

What domains are covered in the corpus?
What is the architecture of their model?
How was the dataset collected?
Which languages are part of the corpus?
How is the quality of the data empirically evaluated?
Is the data in CoVoST annotated for dialect?
Is Arabic one of the 11 languages in CoVost?
How big is Augmented LibriSpeech dataset?

By how much does their best model outperform the state-of-the-art?
Which dataset do they train their models on?
How does their simple voting scheme work?
Which variant of the recurrent neural network do they use?
How do they obtain the new context represetation?

Does the paper report the performance of the model for each individual language?
What is the performance of the baseline?
Did they pefrorm any cross-lingual vs single language evaluation?
What was the performance of multilingual BERT?
What annotations are present in dataset?

What is an unordered text document, do these arise in real-world corpora?
What kind of model do they use?
Do they release a data set?
Do they release code?
Which languages do they evaluate on?

Are the experts comparable to real-world users?
Are the answers double (and not triple) annotated?
Who were the experts used for annotation?
What type of neural model was used?
Were other baselines tested to compare with the neural baseline?

Does the paper clearly establish that the challenges listed here exist in this dataset and task?
Is this hashtag prediction task an established task, or something new?
What is the word-level baseline?
What other tasks do they test their method on?
what is the word level baseline they compare to?

What is the state of the art system mentioned?
Do they incoprorate WordNet into the model?
Is SemCor3.0 reflective of English language data in general?
Do they use large or small BERT?
How does the neural network architecture accomodate an unknown amount of senses per word?

Which fonts are the best indicators of high quality?
What kind of model do they use?
Did they release their data set of academic papers?
Do the methods that work best on academic papers also work best on Wikipedia?
What is their system's absolute accuracy?
Which is more useful, visual or textual features?
Which languages do they use?
How large is their data set?
Where do they get their ground truth quality judgments?

Which models did they experiment with?
What were their best results on the benchmark datasets?
What were the baselines?
Which datasets were used?

what datasets were used?
what are the previous state of the art?
what surface-level features are used?
what linguistics features are used?

what dataset statistics are provided?
what is the size of their dataset?
what crowdsourcing platform was used?
how was the data collected?

What is best performing model among author's submissions, what performance it had?
What extracted features were most influencial on performance?
Did ensemble schemes help in boosting peformance, by how much?
Which basic neural architecture perform best by itself?
What participating systems had better results than ones authors submitted?
What is specific to multi-granularity and multi-tasking neural arhiteture design?

Do they report results only on English data?
What aspects of discussion are relevant to instructor intervention, according to the attention mechanism?
What was the previous state of the art for this task?
What type of latent context is used to predict instructor intervention?

Do they report results only on English dataset?
What dataset does this approach achieve state of the art results on?

How much training data from the non-English language is used by the system?
Is the system tested on low-resource languages?
What languages are the model transferred to?
How is the model transferred to other languages?
What metrics are used for evaluation?
What datasets are used for evaluation?

what are the existing approaches?
what dataset is used in this paper?

How is keyphrase diversity measured?
How was the StackExchange dataset collected?
What does the TextWorld ACG dataset contain?
What is the size of the StackExchange dataset?
What were the baselines?
What two metrics are proposed?

Can the findings of this paper be generalized to a general-purpose task?
Why does the proposed task a good proxy for the general-purpose sequence to sequence tasks?

What was the baseline?
What was their system's performance?
What other political events are included in the database?
What classifier did they use?

What labels for antisocial events are available in datasets?
What are two datasets model is applied to?

What is the CORD-19 dataset?
How large is the collection of COVID-19 literature?

Which deep learning architecture do they use for sentence segmentation?
How do they utilize unlabeled data to improve model representations?

What is the McGurk effect?
Are humans and machine learning systems fooled by the same kinds of illusions?

how many humans evaluated the results?
what was the baseline?
what phenomena do they mention is hard to capture?
by how much did the BLEU score improve?

What is NER?
Does the paper explore extraction from electronic health records?

Does jiant involve datasets for the 50 NLU tasks?
Is jiant compatible with models in any programming language?

What models are used for painting embedding and what for language style transfer?
What applicability of their approach is demonstrated by the authors?
What limitations do the authors demnostrate of their model?
How does final model rate on Likert scale?
How big is English poem description of the painting dataset?
What is best BLEU score of language style transfer authors got?

How better does new approach behave than existing solutions?
How is trajectory with how rewards extracted?
On what Text-Based Games are experiments performed?
How do the authors show that their learned policy generalize better than existing solutions to unseen games?

How much is classification performance improved in experiments for low data regime and class-imbalance problems?
What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?

What subtasks did they participate in?
What were the scores of their system?
How was the training data translated?
What dataset did they use?
What other languages did they translate the data from?
What semi-supervised learning is applied?

How were the datasets annotated?
What are the 12 languages covered?

Does the corpus contain only English documents?
What type of evaluation is proposed for this task?
What baseline system is proposed?
How were crowd workers instructed to identify important elements in large document collections?
Which collections of web documents are included in the corpus?
How do the authors define a concept map?

Is the LSTM baseline a sub-word model?
How is pseudo-perplexity defined?

What is the model architecture used?
How is the data used for training annotated?

what quantitative analysis is done?
what are the baselines?

Do they report results only on English data?
What machine learning and deep learning methods are used for RQE?

by how much did nus outperform abus?
what corpus is used to learn behavior?

Which dataset has been used in this work?
What can word subspace represent?

How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information?
To what baseline models is proposed model compared?
How big is dataset for testing?
What existing dataset is re-examined and corrected for training?

What are the qualitative experiments performed on benchmark datasets?
How does this approach compare to other WSD approaches employing word embeddings?

What tasks did they use to evaluate performance for male and female speakers?
What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?
Which corpora does this paper analyse?
How many categories do authors define for speaker role?
How big is imbalance in analyzed corpora?
What are four major corpora of French broadcast?

What did the best systems use for their model?
What were their results on the classification and regression tasks

Do the authors conduct experiments on the tasks mentioned?
Did they collect their own datasets?
What data do they look at?
What language do they explore?

Do they report results only on English datasets?
Which hyperparameters were varied in the experiments on the four tasks?
Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?
How were the cluster extracted?

what were the evaluation metrics?
what are the state of the art methods?
what english datasets were used?
which chinese datasets were used?

What were their distribution results?
How did they determine fake news tweets?
What is their definition of tweets going viral?
What are the characteristics of the accounts that spread fake news?
What is the threshold for determining that a tweet has gone viral?
How is the ground truth for fake news established?

What was the baseline?
Which three discriminative models did they use?

what NMT models did they compare with?
Where does the ancient Chinese dataset come from?

How many different characters were in dataset?
How does dataset model character's profiles?
How big is the difference in performance between proposed model and baselines?
What baseline models are used?

Was PolyReponse evaluated against some baseline?
What metric is used to evaluate PolyReponse system?
How does PolyResponse architecture look like?
In what 8 languages is PolyResponse engine used for restourant search and booking system?

Why masking words in the decoder is helpful?
What is the ROUGE score of the highest performing model?
How are the different components of the model trained? Is it trained end-to-end?
When is this paper published?

Can their indexing-based method be applied to create other QA datasets in other domains, and not just Wikipedia?
Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?
How many question types do they find in the datasets analyzed?
How do they analyze contextual similaries across datasets?

What were their performance results?
What cyberbulling topics did they address?

Were any of the pipeline components based on deep learning models?
How is the effectiveness of this pipeline approach evaluated?

What is the size of the parallel corpus used to train the model constraints?
How does enforcing agreement between parse trees work across different languages?

What datasets are used to assess the performance of the system?
How is the vocabulary of word-like or phoneme-like units automatically discovered?

IS the graph representation supervised?
Is the G-BERT model useful beyond the task considered?

How well did the baseline perform?
What is the baseline?

what methods were used to reduce data sparsity effects?
what was the baseline?
did they collect their own data?
what japanese-vietnamese dataset do they use?

How do they measure style transfer success?
Do they introduce errors in the data or does the data already contain them?
What error types is their model more reliable for?
How does their parallel data differ in terms of style?

How do they split text to obtain sentence levels?
Do they experiment with their proposed model on any other dataset other than MovieQA?

What is the difference of the proposed model with a standard RNN encoder-decoder?
Does the model evaluated on NLG datasets or dialog datasets?

What tasks do they experiment with?
What is the meta knowledge specifically?

Are there elements, other than pitch, that can potentially result in out of key converted singing?
How is the quality of singing voice measured?

what data did they use?
what previous RNN models do they compare with?

What are examples of these artificats?
What are the languages they use in their experiment?
Does the professional translation or the machine translation introduce the artifacts?
Do they recommend translating the premise and hypothesis together?
Is the improvement over state-of-the-art statistically significant?
What are examples of these artifacts?
What languages do they use in their experiments?

How much higher quality is the resulting annotated data?
How do they match annotators to instances?
How much data is needed to train the task-specific encoder?
What kind of out-of-domain data?
Is an instance a sentence or an IE tuple?

Who are the crowdworkers?
Which toolkits do they use?
Which sentiment class is the most accurately predicted by ELS systems?
Is datasets for sentiment analysis balanced?
What measures are used for evaluation?

what were the baselines?
what datasets were used?
What BERT models are used?
What are the sources of the datasets?
What labels does the dataset have?

Do they evaluate on English only datasets?
What experiments are used to demonstrate the benefits of this approach?
What hierarchical modelling approach is used?
How do co-purchase patterns vary across seasons?
Which words are used differently across ArXiv?

What is future work planed?
What is this method improvement over the best performing state-of-the-art?
Which baselines are used for evaluation?
Did they used dataset from another domain for evaluation?
How is sensationalism scorer trained?

Which component is the least impactful?
Which component has the greatest impact on performance?
What is the state-of-the-art system?
Which datasets are used?
What is the message passing framework?

What other evaluation metrics are looked at?
What is the best reported system?
What kind of stylistic features are obtained?
What traditional linguistics features did they use?
What cognitive features are used?

What approaches do they use towards text analysis?
What dataset do they use for analysis?
Do they demonstrate why interdisciplinary insights are important?
What background do they have?
What kind of issues (that are not on the forefront of computational text analysis) do they tackle?

How big is the ANTISCAM dataset?
How is intent annotated?
What are the baselines outperformed by this work?
What are the evaluation metrics and criteria used to evaluate the model performance?

What is the accuracy of this model compared to sota?

What previous methods do they compare against?
What is their evaluation metric?
Are their methods fully supervised?
Do they build a dataset of rumors?
What languages do they evaluate their methods on?
How do they define rumors?

What baselines did they compare with?
Which tasks are explored in this paper?

Which NER dataset do they use?
How do they incorporate direction and relative distance in attention?
Do they outperform current NER state-of-the-art models?

What was their accuracy score?
What are the state-of-the-art systems?
What dataset did they evaluate on?

What are the contributions of this paper?
What are the baselines this paper uses?
Can the model be extended to other languages?

How do they decide what is the semantic concept label of particular cluster?
How do they discover coherent word clusters?
How big are two introduced datasets?
What are strong baselines authors used?

How do data-driven models usually respond to abuse?
How much data did they gather from crowdsourcing?
How many different strategies were evaluated?

Was the automatic annotation evaluated?

What morphological typologies are considered?
Does the model consider both derivational and inflectional morphology?
What type of morphological features are used?

What datasets are used in this paper?
What language are the captions in?
What ad-hoc approaches are explored?
What supervised baselines did they compare with?
Is the data specific to a domain?
Where do their figure and captions come from?

did the top teams experiment with lexicons?
did they experiment with lexicons?
what was the baseline?
what was their result?
what dataset was used?

What is their definition of hate speech?
What languages does the new dataset contain?
What aspects are considered?
How big is their dataset?

What are the opportunities presented by the use of Semantic Web technologies in Machine Translation?
What are the challenges associated with the use of Semantic Web technologies in Machine Translation?
What are the other obstacles to automatic translations which are not mentioned in the abstract?

what eeg features were used?
what were the baselines?
what dataset was used?

Does LadaBERT ever outperform its knowledge destilation teacher in terms of accuracy on some problems?
Do they evaluate which compression method yields the most gains?
On which datasets does LadaBERT achieve state-of-the-art?

What domain of text are they working with?
What dataset do they use?
Do they compare to abstractive summarization methods?

What types of commonsense knowledge are they talking about?
What do they mean by intrinsic geometry of spaces of learned representations?

Did they pre-train on existing sentiment corpora?
What were the most salient features extracted by the models?
How many languages are in the dataset?
Did the system perform well on low-resource languages?

What are the parts of the "multimodal" resources?
Are annotators familiar with the science topics annotated?
How are the expert and crowd-sourced annotations compared to one another?
What platform do the crowd-sourced workers come from?
Who are considered trained experts?

Which model architecture do they opt for?
Which dataset do they use?
Which setup shows proves to be the hardest: cross-topic, cross-domain, cross-temporal, or across annotators?
Which weak signal data do they use?
Do they compare their semantic feature approach to lexical approaches?

what dataset was used for training?
what is the size of the training data?
what features were derived from the videos?

Do any of the models use attention?
What translation models are explored?
What is symbolic rewriting?

How do they incorporate expert knowledge into their topic model?
On which corpora do they evaluate on?
Do they compare against popular topic models, such as LDA?

What is F-score obtained?
What is the state-of-the-art?
Which Chinese social media platform does the data come from?
What dataset did they use?

What are the five downstream tasks?
Is this more effective for low-resource than high-resource languages?
Is mBERT fine-tuned for each language?
How did they select the 50 languages they test?

What kind of evaluations do use to evaluate dialogue?
By how much do their cross-lingual models lag behind other models?
Which translation pipelines do they use to compare against?
Which languages does their newly created dataset contain?

did they collect their own contrastive test set?
what are the baselines?
what context aware models were experimented?
what languages did they experiment on?

How do they obtain the entity linking results in their model?
Which model architecture do they use?
Which datasets do they evaluate on?

How many domain experts were involved into creation of dataset?
What metrics are used for evaluation?
What is the performance of fine tuned model on this dataset?
Are constructed datasets open sourced?
How does labeling scheme look like?
What pretrained language model is used?
How big is constructed dataset?

What metric is considered?
What hand-crafted features are used?
What word embeddings are used?
Do they annotate their own dataset?
How are the sentence embeddings generated?
What is argumentative zoning?

How did they obtain the tweets?
What baseline do they compare to?
What language is explored in this paper?
What blackmarket services do they look at?

What languages do they use during pretraining?
What is the architecture of the decoder?
What is the architecture of the encoder?
What is their baseline?

What human evaluation metrics do they look at?
Which automated evaluation metrics are used?
What baselines do they compare against?
Do they use pre-trained embeddings like BERT?
What model is used to generate the premise?
Are the stories in the dataset fictional stories?
Where are the stories collected from?

which pretrained embeddings were experimented with?
what datasets where used?
what are the state of the art methods they compare with?

What agreement measure is used?
Do they report the annotation agreement?
How many annotators participated?
What social-network features are used?
What are the five factors considered?
How is cyberbullying defined?

What evaluation was performed on the output?
Where did the joke data come from?
What type of quotes is this system trying to generate?

What size filters do they use in the convolution layer?
By how much do they outperform state-of-the-art models on knowledge graph completion?

did they test with other pretrained models besides bert?
what models did they compare with?
what datasets were used for testing?

What inter-annotator agreement did they obtain?
How did they annotate the corpus?
What is the size of the corpus?

Which datasets do they use?
What models are explored in this paper?

what features of the essays are extracted?
what were the evaluation metrics?
what model is used?
what future work is described?
what was the baseline?

How is the sentence alignment quality evaluated?
How is the speech alignment quality evaluated?

Is their gating mechanism specially designed to handle one sentence bags?
Do they show examples where only one sentence appears in a bag and their method works, as opposed to using selective attention?
By how much do they outperform previous state-of-the-art in terms of top-n precision?

By how much do they outperform existing methods?
Which datasets do they evaluate on?
Do they separately evaluate performance of their learned representations (before forwarding them to the CNN layer)?

What was the baseline?
What dataset was used in this challenge?
Which subsystem outperformed the others?

Do they reduce language variation of text by enhancing frequencies?
Which domains do they explore?
Which thesauri did they use?

What is their definition of hate speech?
What type of model do they train?
How many users does their dataset have?
How long is their dataset?

In what tasks does fine-tuning all layers hurt performance?
Do they test against the large version of RoBERTa?

What is the performance improvement of their method over state-of-the-art models on the used datasets?
Could the proposed training framework be applied to other NLP problems?
How does the proposed training framework mitigate the bias pattern?
Which datasets do they use in the cross-dataset evaluation?

Which was the most helpful strategy?
How large is their tweets dataset?

what is the size of the idn tagged corpus?
what neural network models were explored?
what rule based models were evaluated?
what datasets have been used for this task?

How much data do they use to train the embeddings?
Do they evaluate their embeddings in any downstream task appart from word similarity and word analogy?
What dialects of Chinese are explored?

What are the issues identified for out-of-vocabulary words?
Is the morphology detection task evaluated?
How does the model proposed extend ENAMEX?
Which morphological features are extracted?

Do the authors report results on only English datasets?
What are the characteristics of the dataset of Twitter users?
How can an existing bot detection system by customized for health-related research?
What type of health-related research takes place in social media?

Do the QA tuples fall under a specific domain?
What is the baseline model?
How large is the corpus of QA tuples?
What corpus did they use?

what boosting techniques were used?
did they experiment with other text embeddings?
what is the size of this improved dataset?
how was the new dataset collected?
who annotated the new dataset?
what shortcomings of previous datasets are mentioned?

Do single-language BERT outperforms multilingual BERT?
What types of agreement relations do they explore?

what text classification datasets do they evaluate on?
which models is their approach compared to?

by how much did their approach outperform previous work?
what was the previous best results model?
what are the baseline models?
what domains are explored?
what training data was used?

What is the performance of the best model?
What are the models tested on the dataset?
Which method best performs on the offensive language identification task?
Did they use crowdsourcing for the annotations?
How many annotators did they have?
Is the dataset balanced?
What models do they experiment on?

Do any of their reviews contain translations for both Catalan and Basque?
What is the size of their published dataset?
How many annotators do they have for their dataset?

How does sentence construction component works?
What are two use cases that demonstrate capability of created system?

Do they explore how their word representations vary across languages?
Which neural language model architecture do they use?
How do they show genetic relationships between languages?

Did they test the idea that the system reduces the time needed to encode ADR reports on real pharmacologists?
Do the authors offer a hypothesis as to why the system performs better on short descriptions than longer ones?
What are the steps in the MagiCoder algorithm?
How is the system constructed to be linear in the size of the narrative input and the terminology?

What conclusions do the authors draw about the aspects and mechanisms of personal recovery in bipolar disorder?
What languages were included in this multilingual population?
What computational linguistic methods were used for the analysis?
Was permission sought from the bipolar patients to use this data?
How are the individuals with bipolar disorder identified?

What is the source of the training/testing data?
What are the types of chinese poetry that are generated?

what is the previous work they are comparing to?

Do they use skip-gram word2vec?
How is quality of the word vectors measured?

Do they report results only on English data?
Where do the news texts come from?
What baseline is used for this task?
What type of nerual keyphrase generation models are trained?
How do the editors' annotations differ from those in existing datasets?

How long is their dataset?
Do they use pretrained word embeddings?
How many layers does their model have?
What metrics do they use?

what dataset did they use?
what was their model's f1 score?
what are the state of the art models?

How do you know the word alignments are correct?
How slow is the unparallelizable ART model in the first place?
What metric is used to measure translation accuracy?
Were any datasets other than WMT used to test the model?
Are the results applicable to other language pairs than German-English?

What dicrimating features are discovered?
What results are obtained on the alternate datasets?

Are answers in this dataset guaranteed to be substrings of the text? If not, what is the coverage of answers being substrings?
How much is the gap between pretraining on SQuAD and not pretraining on SQuAD?

What is the machine learning method used to make the predictions?
How is the event prediction task evaluated?
What are the datasets used in the paper?

Do they compare to other models that include subword information such as fastText?
Is there a difference between the model's performance for morphologically impoverished and morphologically complex languages?
What languages do they apply the model to?
How are the embeddings evaluated in the human judgement comparison?

what was the margin their system outperformed previous ones?
what prior approaches did they compare to?

what are the baselines?
what results do they achieve?
what chinese dialects are explored?

Which neural machine translation model was used?
What position did this entry finish in, in the overall shared task?
What are the restrictions of the restricted track?
What does BEA stand for?

Which works better according to human evaluation, the concurrent or the modular system?
Were the Wikipedia edits that removed framings, presuppositions and attitudes from biased sentences a Wiki community effort, or were annotators trained to do it?
How is subjective text automatically neutralized?

What is the sign language recognition task investigated?
What is the performance of the best model in the sign language recognition task?
What are the deep learning architectures used?

Who made the stated claim (that "this is because character-level models learn morphology")?
Which languages do they use?
Do the character-level models perform better than models with access to morphological analyses only?
What is case syncretism?

Do humans assess the quality of the generated responses?
What models are used to generate responses?
What types of hate speech are considered?

Which baselines to they compare to?
Which sentence compression technique works best?
Do they compare performance against state of the art systems?

What is the performance of large state-of-the-art models on these datasets?
What is used as a baseline model?
How do they build gazetter resources from Wikipedia knowlege base?

What is the dataset that is used to train the embeddings?
What speaker characteristics are used?
What language is used for the experiments?
Is the embedding model test in any downstream task?

what is the baseline model
What contribute to improve the accuracy on legal question answering task?

What sizes were their datasets?
How many layers does their model have?
What is their model's architecture?
What languages did they use?

Are there experiments with real data?

What supervised machine learning models do they use?
Does the supervised machine learning approach outperform previous work?
How large is the released data set?
What is an example of a condition-action pair?

Which metrics were considered?
What NLG tasks were considered?

what state of the art methods are compared to?
what are the performance metrics?
what is the original model they refer to?
how are sentences selected prior to making the summary?

Do they evaluate only on English datasets?
What type of frequency analysis was used?
What type of classifiers were used?
Who annotated the Twitter and Reddit data for irony?

what resources are combined to build the labeler?
what datasets were used?
what is the monolingual baseline?
what languages are explored in this paper?

Does their model use MFCC?
What is the problem of session segmentation?
What dataset do they use?

Was the filtering based on fluency and domain relevance done automatically?
How was domain relevance estimated?
How many hand-crafted templates did they have to make?
How was the fluency measured?

What data is used in this work?

What dataset is used?

How was the dataset collected?

what evaluation metrics were used?
What datasets are used?

how did they measure grammatical correctness?
how was quality of sentence transition measured?
what is the size of the dataset?
what manual evaluation is presented?

What downstream tasks are analyzed?
How much time takes the training of DistilBERT?

Which datasets do they use?
How do they compare representations performance obtained from a naive encoder versus ones learned from large amount of source language data?
Which pairs of languages do they consider similar enough to capture phonetic structure?

Did they try Roberta?

What are their results on this task?
How is the text segmented?

what are the state of the art models?

How many parameters does their noisy channel model have?
Which language pairs do they evaluate on?

How large the improvement margin is?

Which languages do they explore?

What are two baseline methods?
How does model compare to the baselines?

Are the two paragraphs encoded independently?

What is their baseline?
Is human evaluation of the malicious content performed?
Do they compare to previous work?

by how much did their model outperform the other models?

What is reordering in the context of the paper?
How does the paper use language model for context aware search?

What datasets are used?

what evaluation metrics were used?
what is the source of their dataset?
by how much did the performance improve?
how many experts were there?
what is the size of the data collected?
did they use a crowdsourcing platform?
how was annotation conducted?
what does their dataset contain?

Do they report results only on English data?
How do the authors measure the extent to which LGI has learned the task?
Which 8 tasks has LGI learned?
In what was does an LSTM mimic the prefrontal cortex?
In what way does an LSTM mimic the intra parietal sulcus?
How do the authors define imagination, or imagined scenarios?

Which classifiers did they experiment with?
Is the distribution of the edits uniform across all languages?
How did they identify what language the text was?
Which repositories did they collect from?
Which three features do they use?
Which languages are covered in the corpus?

Do they report results only on English data?
What is the BM25 baseline?
Which BERT layers were combined to boost performance?
Which NLI data was used to improve the quality of the embeddings?
Which four QA datasets are examined?
Which two tasks from SentEval are the sentence embeddings evaluated against?

what classifiers did they train?
what dataset did they use?
what combination of features helped improve the classification?
what linguistics features did they apply?
what is the state of the art in English?

Are results reported only on English data?
What type of model were the features used in?
What unsupervised approach was used to deduce the thematic information?
What profile features are used?
What textual features are used?

what other representations do they compare with?
how many layers are in the neural network?
what empirical evaluations performed?
which document understanding tasks did they evaluate on?
what dataset was used?

What private companies are members of consortium?
Does programme plans gathering and open sourcing some large dataset for Icelandic language?
What concrete software is planned to be developed by the end of the programme?
What other national language technology programs are described in the paper?
When did language technology start in Iceland?

what was their accuracy result?
what domain do the opinions fall under?
what was the baseline?
what dataset was used?

is this the first dataset with a grading scaling rather than binary?
what are the existing datasets for this task?
what is the size of the introduced dataset?
did they crowdsource annotations?
how was labeling done?
where does their dataset come from?
what are the baselines?
what tools did they use?

What is the performance of NJM?
How are the results evaluated?
How big is the self-collected corpus?
How is the funny score calculated?

Which dataset do they use?
Do they compare their proposed domain adaptation methods to some existing methods?
Which of their proposed domain adaptation methods proves best overall?
Do they use evolutionary-based optimization algorithms as one of their domain adaptation approaches?

With how many languages do they experiment in the multilingual setup?
How do they extract target language bottleneck features?
Which dataset do they use?
Which intrisic measures do they use do evaluate obtained representations?

Do they use pretrained embeddings in their model?
What results are obtained by their model?
What sources do the news come from?
What is the size of Multi-news dataset?

Which vocabulary size was the better performer?
Which languages are explored?
What datasets are used in the paper?
What vocabulary sizes are explored?
What vocabulary size was the best performer?
What datasets do they look at?
Which vocab sizes did they analyze?

Why is improvement on OntoNotes significantly smaller compared to improvement on WNUT 2017?
How is "complexity" and "confusability" of entity mentions defined in this work?
What are the baseline models?

What text classification tasks are considered?
Do they compare against other models?
What is episodic memory?

Are LSA-reduced n-gram features considered hand-crafted features?
What is the performance of the model on English, Spanish and Arabic?
How is this model different from a LSTM?

What does the cache consist of?
What languages is the model tested on?
What is a personalized language model?

Is the dataset used in other work?
What is the drawback to methods that rely on textual cues?
What community-based profiling features are used?

what user traits are taken into account?
does incorporating user traits help the task?
how many activities are in the dataset?
who annotated the datset?
how were the data instances chosen?
what social media platform was the data collected from?

Do they report results only for English data?
What conclusions do the authors draw from their experiments?
In what way does each classifier evaluate one of the syntactic or social properties which are salient for a tweet?

How is a per-word reward tuned with the perceptron algorithm?
What methods are used to correct the brevity problem?
Why does wider beam search hurt NMT?

What linguistic model does the conventional method use?
What is novel about the newly emerging CNN method, in comparison to well-established conventional method?
What lexical cues are used for humor recogition?
Do they evaluate only on English data?
How many speakers are included in the dataset?
How are the positive instances annotated? e.g. by annotators, or by laughter from the audience?

How you incorporate commonsense into an LSTM?
Which domain are the conversations in?
Which commonsense knowledge base are they using?

How did they obtain the dataset?
Are the recommendations specific to a region?
Did they experiment on this dataset?

What sized character n-grams do they use?
Do they experiment with fine-tuning their embeddings?
Which word embeddings do they compare against?
Which dataset do they evaluate on for headline generation?
What results do their embeddings obtain on machine translation?
How do they combine ordinary word embeddings and ones constructed from character n-grams?

Which dataset do they use?
By how much do they outperform previous state-of-the-art approaches?
Do they analyze attention outputs to determine which terms in general contribute to clickbait titles?

What other scenarios can the bias mitigation methods be applied to?
Are the three bias mitigation methods combined in any model?
Which of the three bias mitigation methods is most effective?
What model architectures are used?
What pre-trained word embeddings are used?
What metrics are used to measure gender biases?

Do they ensure the that the architecture is differentiable everywhere after adding the Hungarian layer?
Which dataset(s) do they train on?
By how much does their model outperform state-of-the-art baselines?

Do they compare to previous work?
What is the model trained?
How large is the dataset used?

How exactly do they weigh between different statistical models?
Do they compare against state-of-the-art summarization approaches?
What showed to be the best performing combination of semantic and statistical model on the summarization task in terms of ROUGE score?

What QA system was used in this work?
Is the re-ranking approach described in this paper a transductive learning technique?
How big is the test set used for evaluating the proposed re-ranking approach?

What is the new metric?
How long do other state-of-the-art models take to process the same amount of data?
What context is used when computing the embedding for an entity?

What are the limitations of the currently used quantitative metrics? e.g. why are they not 'good'?
What metrics are typically used to compare models?
Is there a benchmark to compare the different approaches?
What GAN and RL approaches are used?
What type of neural models are used?
What type of statistical models were used initially?
What was the proposed use of conversational agents in pioneering work?
What work pioneered the field of conversational agents?

How does this research compare to research going on in the US and USSR at this time?
What is the reason this research was not adopted in the 1960s?
What is included in the cybernetic methods mentioned?
What were the usual logical approaches of the time period?
What language was this research published in?

what language was the data in?
what was the baseline?
which automatic metrics were used in evaluation?
how do humans judge the simplified sentences?
what datasets were used?

What previous approaches are presented for comparison?
What kind of data is used to train the model?
Does proposed approach use neural networks?
What machine learning techniques are used in the model architecture?
What language(s) is the model tested on?

By how much did their model outperform baselines?
Which baselines did they compare against?
What was their performance on this task?
What dataset did they use to evaluate?
How did they obtain part-of-speech tags?

what was their system's f1 score?
what were the baselines?
what emotion cause dataset was used?
what lexical features are extracted?
what word level sequences features are extracted?

what are the recent models they compare with?
what were their results on the hutter prize dataset?
what was their newly established state of the art results?
what regularisation methods did they look at?
what architectures were reevaluated?

what baseline models are trained?
what dataset was used?
what are the human evaluation metrics?
what automatic evaluation is performed?
what are the existing online systems?

What are their baselines?
Do they report the annotation agreement?
How long is the test dataset for Dutch?
How long is the training dataset for English?
What features are used?
What is the source of the data?

What languages feature in the dataset?
What textual, psychological and behavioural patterns are observed in radical users?
Where is the propaganda material sourced from?
Which behavioural features are used?
Which psychological features are used?
Which textual features are used?

what is the cold-start problem?
how was the experiment evaluated?
what other applications did they experiment in?
what dataset was used for training?

Was the entire annotation process done manually?
What were the results of their experiment?
How big is the dataset?
What are all the domains the corpus came from?

How big is benefit in experiments of this editing approach compared to generating entire SQL from scratch?
What are state-of-the-art baselines?

Which dialogue data do they use to evaluate on?
How much faster are pairwise annotations than other annotations?

How much improvement is there in the BLEU score?
What is the established approach used for comparison?
What are the five domains?
Which pre-trained language models are used?

Do they report results only on English data?
What are the hyperparameter setting of the MTL model?
What architecture does the rest of the multi-task learning setup use?
How is the selected sharing layer trained?

what were the length constraints they set?
what is the test set size?

what are the evaluation metrics used?
what other training procedures were explored?

What baseline did they use?
What is the threshold?
How was the masking done?
How large is the FEVER dataset?

How do they obtain structured data?
Which prior approaches for style transfer do they test with?
Which competing objectives for their unsupevised method do they use?
Which content coverage constraints do they design?

what were the evaluation metrics?
how many sentiment labels do they explore?

Which dataset do they use for text altering attributes matching to image parts?
Is it possible for the DCM module to correct text-relevant content?

Is an ablation test performed?
What statistical test is performed?

Which downstream tasks are used for evaluation in this paper?
Which datasets are used for evaluation?

What does the human-in-the-loop do to help their system?
Which dataset do they use to train their model?
Can their approach be extended to eliminate racial or ethnic biases?
How do they evaluate their de-biasing approach?

Is there a metric that also rewards good stylistic response?
What are existing baseline models on these benchmark datasets?
On what two languages is experimented on?
What three benchmark datasets are used?

What IS versification?
How confident is the conclusion about Shakespeare vs Flectcher?
Is Henry VIII reflective of Shakespeare in general?
Is vocabulary or versification more important for the analysis?
What are the modifications by Thomas Merriam?
What are stop words in Shakespeare?

What sources of less sensitive data are available?
Other than privacy, what are the other major ethical challenges in clinical data?

what evaluation metrics were used?
what state of the art models did they compare with?

Is the performance improvement (with and without affect attributes) statistically significant?
How to extract affect attributes from the sentence?

How many layers does the neural network have?
Which BERT-based baselines do they compare to?
What are the propaganda types?
Do they look at various languages?
What datasets did they use in their experiment?

What size ngram models performed best? e.g. bigram, trigram, etc.
How were the ngram models used to generate predictions on the data?
What package was used to build the ngram language models?
What rank did the language model system achieve in the task evaluation?
What were subtasks A and B?

Do the authors report only on English
How does counterfactual data augmentation affect gender bias in predictions and performance?
How does hard debiasing affect gender bias in prediction and performance?
How does name anonymization affect gender bias in predictions and performance?
How are the sentences in WikiGenderBias curated?

what crowdsourcing platform did they use?
did they crowdsource annotations?
where does their data come from?
which existing corpora do they compare with?
what is the size of their corpus?
which architectures did they experiment with?
what domains are present in the corpus?
what was the inter-annotator agreement?

Which metrics are used for quantitative analysis?
Is their data open sourced?
What dataset did they use?
What metric did they use for qualitative evaluation?
What metric did they use for quantitative evaluation?
Which similarity metrics are used for quantitative analysis?

How is the data labeled?
What is the best performing model?
How long is the dataset?

what languages did they evaluate on?
were these categories human evaluated?
do language share categories?

What languages are evaluated?
Does the training of ESuLMo take longer compared to ELMo?
How long is the vocabulary of subwords?

what rnn classifiers were used?
what results did their system obtain?
what are the existing approaches?

Which dataset do they use?
How do they use extracted intent to rescore?
Do they evaluate by how much does ASR improve compared to state-of-the-art just by using their FST?

How is the model evaluated against the original recursive training algorithm?
What is the improvement in performance compared to the linguistic gold standard?
What is the improvement in performance brought by lexicon pruning on a simple EM algorithm?

Which metrics do they use to evaluate results?
Does the performance increase with the number of used languages?
By how much do they outperform translating without contextual information?

Which baselines did they compare to?
What dialog tasks was it experimented on?
How was annotation done?
Which news outlets did they focus on?
Do the interviews fall under a specific news category?
Which baselines did they compare to?
Which dialog tasks did they experiment on?
Did they use crowdsourcing for annotations?
Were annotations done manually?
Which news sources do the transcripts come from?

Which real world datasets do they experiment on?
Which other models that incorporate meta information do they compare against?
How do they measure topic quality?
Which data augmentation techniques do they use?

Is this an English language corpus?
The authors point out a relevant constraint on the previous corpora of workplace, do they authors mention any relevant constrains on this corpus?
What type of annotation is performed?
How are the tweets selected?

what dataset was used?
by how much did their model improve over current alternatives?
did they experiment with other languages besides portuguese?
how many rules did they use?

What is the state-of-the-art?
How large is the dataset?
How are labels for trolls obtained?
Do they only look at tweets?

Which datasets did they use to train the model?
What is the performance of their model?
What baseline do they compare against?
What datasets is the model evaluated on?

What is the percentage of human judgment agreement on the set?
Are the orders of case assignment biases motivated by frequency considerations?
Does the paper list other heuristic biases in the LSTMs?
What are the performances of LSTMs and humans on the task?

Do they authors offer a hypothesis for why Twitter data makes better predictions about the inventory of languages used in each country?
What social media platforms are represented?
Which websites were used in the web crawl?
What countries and languages are represented in the datasets?

What other evaluation metrics did they use other than ROUGE-L??
Do they encode sentences separately or together?
How do they use BERT to encode the whole text?
What is the ROUGE-L score of baseline method?
Which is the baseline method?

What loss function is used?
Do they use the unstructured text on the webpage that was the source of the table?
Does their method rely on the column headings of the table?
Are all the tables in the dataset from the same website?
How are the tables extracted from the HTML?

Does the query-bag matching model use a neural network?
What datasets are used for experiments?
Which natural language(s) is/are studied?
Is model compared to some baseline?
What datasets are used in experiments?

How many lexical features are considered?
What is the performance for the three languages tested?
How many Universal Dependency features are considered?
Do they evaluate any non-zero-shot parsers on the three languages?
How big is the Parallel Meaning Bank?
What is the source of the crosslingual word embeddings?

Do they compare against manually-created lexicons?
Do they compare to non-lexicon methods?
What language pairs are considered?

How many abstractive summarizations exist for each dialogue?
How is human evaluators' judgement measured, what was the criteria?
What models have been evaluated?
Do authors propose some better metric than ROUGE for measurement of abstractive dialogue summarization?
How big is SAMSum Corpus?

Do they manually check all adversarial examples that fooled some model for potential valid examples?
Are all generated examples semantics-preserving perturbations to the original text?
What is success rate of fooling tested models in experiments?
What models are able to be fooled for AG's news corpus news categorization task by this approach?
What models are able to be fooled for IMDB sentiment classification task by this approach?
Do they use already trained model on some task in their reinforcement learning approach?
How does proposed reinforcement learning based approach generate adversarial examples in black-box settings?

Which languages with different script do they look at?
What languages do they experiment with?
What language pairs are affected?
What evaluation metrics are used?
What datasets did they use?

what are the other methods they compare to?
what preprocessing method is introduced?

How well does their model perform on the recommendation task?
Which knowledge base do they use to retrieve facts?
Which neural network architecture do they use?

Are reddit and twitter datasets, which are fairly prevalent, not effective in addressing these problems?

did they experiment with other languages?
by how much did their system outperform previous tasks?
what are the previous state of the art for sentiment categorization?
what are the previous state of the art for tweet semantic similarity?

By how much do they outperform baselines?
Which baselines do they use?
Which datasets do they evaluate on?

What deep learning methods do they look at?
What is their baseline?
Which architectures do they experiment with?
Are pretrained embeddings used?

Does the paper discuss limitations of considering only data from Twitter?
Did they represent tie strength only as number of social ties in a networks?
What sociolinguistic variables (phonetic spellings) did they analyze?
What older dialect markers did they explore?

How many domains do they create ontologies for?
Do they separately extract topic relations and topic hierarchies in their model?
How do they measure the usefulness of obtained ontologies compared to domain expert ones?
How do they obtain syntax from raw documents in hrLDA?

What datasets are available for CDSA task?
What two novel metrics proposed?
What similarity metrics have been tried?
What 20 domains are available for selection of source domain?

why do they think sentiment features do not result in improvement?
what was the size of the datasets?
what were the evaluation metrics?
what were their results on both tasks?
what domain-specific features did they train on?
what are the sentiment features used?
what surface-form features were used?

How does their BERT-based model work?
How do they use Wikipedia to automatically collect a query-focused summarization dataset?

How is GPU-based self-critical Reinforcement Learing model designed?
What are previoius similar models authors are referring to?
What was previous state of the art on factored dataset?

How much did the model outperform
What language is in the dataset?
How big is the HotPotQA dataset?

Which labeling scheme do they use?
What parts of their multitask model are shared?
Which dataset do they use?

Do they compare against Reinforment-Learning approaches?
How long is the training dataset?
What dataset do they use?

What high-resource language pair is the parent model trained on?
Did they use any regularization method to constrain the training?
How did they constrain training using the parameters?

What are their evaluation metrics?
Are their formal queries tree-structured?
What knowledge base do they rely on?
How do they recover from noisy entity linking?
What datasets do they evaluate on?

Did they use the same dataset as Skip-gram to train?
How much were the gains they obtained?

What is the extractive technique used for summarization?
How big is the dataset?

By how much they outperform the baseline?
How long are the datasets?
What bayesian model is trained?
What low resource languages are considered?

How is cluster purity measured?
What was the previous state of the art for bias mitigation?
How are names paired in the Names Intervention?
Which tasks quantify embedding quality?
What empirical comparison methods are used?

How do they define their tokens (words, word-piece)?
By how much do they outperform existing state-of-the-art model on end-to-end Speech recognition?s

Did the authors collect new data for evaluation?

what were the evaluation metrics?
what language pairs are explored?
what datasets did they use?
which attention based nmt method did they compare with?
by how much did their system improve?

What were the baseline methods?
What dataset is used for training?

Do they compare to previous work?
What is the source of their data?
What is their binary classifier?
How long is their dataset?
What is a study descriptor?

How are experiments designed to measure impact on performance by different choices?
What impact on performance is shown for different choices of optimizers and learning rate policies?

What domain do the audio samples fall under?
How did they evaluate the quality of annotations?
How many annotators did they have?
What is their baseline method?

In what language are the captions written in?
What is the average length of the captions?
Does each image have one caption?
What is the size of the dataset?
What is the source of the images and textual captions?

what evaluation metrics did they use?
what was the baseline?
what were roberta's results?
which was the worst performing model?

How long is their sentiment analysis dataset?
What NLI dataset was used?
What aspects are considered?
What layer gave the better results?

How many annotators were used for sentiment labeling?
How is data collected?
How much better is performance of Nigerian Pitdgin English sentiment classification of models that use additional Nigerian English data compared to orginal English-only models?
What full English language based sentiment analysis models are tried?

Do they treat differerent turns of conversation differently when modeling features?
How do they bootstrap with contextual information?
Which word embeddings do they utilize for the EmoContext task?

What were the performance results of their network?
What were the baselines?
What dataset is used?
Do they explore other language pairs?

How do they preprocess Tweets?
What kind of inference model do they build to estimate socioeconomic status?
How much data do they gather in total?
Do they analyze features which help indicate socioeconomic status?
What inference models are used?
What baseline model is used?
How is the remotely sensed data annotated?
Where are the professional profiles crawled from?

How much additional data do they manage to generate from translations?
Do they train discourse relation models with augmented data?
How many languages do they at most attempt to use to generate discourse relation labelled data?

by how much did the system improve?
what existing databases were used?
what existing parser is used?

How do they combine the socioeconomic maps with Twitter data?
Does the fact that people are active during the day time define their SEC?
How did they define standard language?
How do they operationalize socioeconomic status from twitter user data?

Do the authors provide any benchmark tasks in this new environment?

What dimensions do the considered embeddings have?
How are global structures considered?

Which translation model do they employ?
Which datasets do they experiment on?
Which other units of text do they experiment with (apart from BPE and ortographic syllables)?
How many steps of BPE do they experiment with?

What nuances between fake news and satire were discovered?
What empirical evaluation was used?
What is the baseline?
Which linguistic features are used?
What contextual language model is used?

what state of the art models do they compare to?

What is the weak supervision signal used in Baidu Baike corpus?
How is BERT optimized for this task?
What is a soft label?

Do the authors examine the real-world distribution of female workers in the country/countries where the gender neutral languages are spoken?
Which of the 12 languages showed the strongest tendency towards male defaults?
How many different sentence constructions are translated in gender neutral languages?

What are the evaluation metrics used?
What are the baselines?
Which language learning datasets are used?

What does it mean for sentences to be "lexically overlapping"?
How many tables are in the tablestore?

what dataset is used?
what neural network models are used?
Do they report results only on English data?
What baseline model is used?
What type of neural network models are used?
How is validity identified and what metric is used to quantify it?
How is severity identified and what metric is used to quantify it?
How is urgency identified and what metric is used to quantify it?

How many of the attribute-value pairs are found in video?
How many of the attribute-value pairs are found in audio?
How many of the attribute-value pairs are found in images?
How many of the attribute-value pairs are found in semi-structured text?
How many of the attribute-value pairs are found in unstructured text?
How many different semi-structured templates are represented in the data?
Are all datapoints from the same website?
Do they consider semi-structured webpages?

What are the baseline models?
What image caption datasets were used in this work?
How long does it take to train the model on the mentioned dataset?
How big is the human ratings dataset?

What existing techniques do the authors compare against?

Is the dataset completely automatically generated?
Does the SESAME dataset include discontiguous entities?
How big is the resulting SESAME dataset?

Can their method be transferred to other Q&A platforms (in other languages)?
What measures of quality do they use for a Q&A platform?

Do they evaluate whether local or global context proves more important?
How many layers of recurrent neural networks do they use for encoding the global context?
How did their model rank in three CMU WMT2018 tracks it didn't rank first?

Do they evaluate only on English datasets?
What is the Ritter dataset?
Does this model perform better than the state of the art?
What features are extracted from text?
What features are extracted from images?

What are the baseline models?
How are the three different forms defined in this work?
What datasets are used for training and testing?
Does approach handle overlapping forms (e.g., metaphor and irony)?
Does this work differentiate metaphor(technique) from irony and sarcasm (purpose)?

What classification tasks do they experiment on?
What categories of fake news are in the dataset?

How much gain in performance was obtained with user embeddings?

By how much does their similarity measure outperform BM25?
How do they represent documents when using their proposed similarity measure?
How do they propose to combine BM25 and word embedding similarity?
Do they use pretrained word embeddings to calculate Word Mover's distance?

Which Twitter sentiment treebank is used?
Where did the system place in the other sub-tasks?
What were the five labels to be predicted in sub-task C?

What is the previous state-of-the-art?
What is the architecture of the decoder?
What is the architecture of the encoder?
What are the languages of the datasets?
What is the architecture of the saliency model?

What are special architectures this review focuses on that are related to multimodal fusion?

What other model inference optimization schemes authors explore?
On what dataset is model trained/tested?

By how much do they improve on domain classification?
Which dataset do they evaluate on?
How does their approach work for domains with few overlapping utterances?
How do they decide by how much to decrease confidences of incorrectly predicted domains?

Is some baseline method trained on new dataset?
What potential applications are demonstrated?
What method is proposed to mitigate class imbalance in final dataset?

What are remaining challenges in VQA?
How quickly is this hybrid model trained?
What are the new deep learning models discussed in the paper?
What was the architecture of the 2017 Challenge Winner model?
What is an example of a common sense question?

What pretrained language representations are used?
How many instances are explored in the few-shot experiments?
What tasks are explored?

How is the training time compared to the original position encoding?
Does the new relative position encoder require more parameters?
Can the new position representation be generalized to other tasks?

which social media platforms was the data collected from?
how many data pairs were there for each dataset?
how many systems were there?
what was the baseline?
what metrics did they use for evaluation?
what datasets did they use?

What is the Semantic Web?

How many tags are included in the ENE tag set?
Does the paper evaluate the dataset for smaller NE tag tests?

Do they report results only on English data?
What are the most discriminating patterns which are analyzed?
What bootstrapping methodology was used to find new patterns?
What patterns were extracted which were correlated with emotional arguments?
What patterns were extracted which were correlated with factual arguments?
How were the factual and feeling forum posts annotated?

What evaluation metrics did they use?
What NMT techniques did they explore?
What was their best performing model?
What datasets did they use?

Which ontologies did they use?

how is user satisfaction estimated?
by how much did performance improve?

What datasets do they use in the experiment?
What new tasks do they use to show the transferring ability of the shared meta-knowledge?
What kind of meta learning algorithm do they use?

what dataset were used?
what was the baseline?
what text embedding methods were used in their approach?

Do they compare against state-of-the-art?
What are the benchmark datasets?
What tasks are the models trained on?
What recurrent neural networks are explored?

What extractive models were trained on this dataset?
What abstractive models were trained?
Do the reviews focus on a specific video game domain?
What is the size of this dataset?

What language(s) does the system answer questions in?
What metrics are used for evaluation?
Is the proposed system compared to existing systems?

How do they determine that a decoder handles an easier task than the encoder?
How do they measure conditional information strength?
How do they generate input noise for the encoder and decoder?

How do they perform the joint training?
How many parameters does their model have?
What is the previous model that achieved state-of-the-art?

Did the survey provide insight into features commonly found to be predictive of abusive content on online platforms?
Is deep learning the state-of-the-art method in automated abuse detection
What datasets were used in this work?
How is abuse defined for the purposes of this research?

Do they use external financial knowledge in their approach?
Which evaluation metrics do they use?
Which finance specific word embedding model do they use?

How does lattice rescoring improve inference?
What three languages are used in the translation experiments?
What metrics are used to measure bias reduction?
How is the set of trusted, gender-balanced examples selected?

Which data sources do they use?
Which tasks do they evaluate supervised systems on?
How do they evaluate domain portability?
Which unsupervised representation-learning objectives do they introduce?

Do they manage to consistenly outperform the best performing methods?
Do they try to use other models aside from Maximum Entropy?
What methods to they compare to?
Which dataset to they train and evaluate on?
Do they attempt to jointly learn connectives, arguments, senses and non-explicit identiifers end-to-end?

What settings did they experiment with?
what domains are explored in this paper?
what multi-domain dataset is repurposed?
what four learning strategies are investigated?

By how much did the new model outperform multilingual BERT?
What previous proposed methods did they explore?
What was the new Finnish model trained on?

How many TV series are considered?
How long is the dataset?
Is manual annotation performed?
What are the eight predefined categories?

Do they report results only on English data?
When the authors say their method largely outperforms the baseline, does this mean that the baseline performed better in some cases? If so, which ones?
What baseline method was used?
What was the motivation for using a dependency tree based recursive architecture?
How was a causal diagram used to carefully remove this bias?
How does publicity bias the dataset?
How do the speakers' reputations bias the dataset?

What is the state-of-the-art approach?

what do they mean by description length?
do they focus on english verbs?
what evaluation metrics are used?

Do the authors mention any possible confounds in this study?
Do they report results only on English data?
Are there any other standard linguistic features used, other than ngrams?
What is the relationship between author and emotional valence?
What is the relationship between time and emotional valence?
What is the relationship between location and emotional valence?

What is the computational complexity of old method
Could you tell me more about the old method?

How this system recommend features for the new application?
What is the similarity of manually selected features across related applications in different domains?
What type of features are extracted with this language?
What are meta elements of language for specifying NLP features?

what previous work do they also look at?
what languages did they experiment with?

What are state of the art results on OSA and PD corpora used for testing?
How better does x-vectors perform than knowlege-based features in same-language corpora?
What is meant by domain missmatch occuring?
How big are OSA and PD corporas used for testing?

How do they think this treebank will support research on second language acquisition?
What are their baseline models?
How long is the dataset?
Did they use crowdsourcing to annotate the dataset?

How significant are the improvements over previous approaches?
Which other tasks are evaluated?
What are the performances associated to different attribute placing?

What architecture is used in the encoder?

Do they evaluate their parallel sentence generation?
How much data do they manage to gather online?
Which models do they use for phrase-based SMT?
Which models do they use for NMT?
What are the BLEU performance improvements they achieve?

What is the architecture of the model?
How many translation pairs are used for training?

Do they use multitask learning?
Is Chinese a pro-drop language?
Is English a pro-drop language?

Which movie subtitles dataset did they use?

What are the other two Vietnamese datasets?
Which English dataset do they evaluate on?
What neural network models do they use in their evaluation?
Do they use crowdsourcing for the captions?
What methods are used to build two other Viatnamese datsets?
What deep neural network models are used in evaluation?
How authors evaluate datasets using models trained on different datasets?

Do they evaluate their model on datasets other than RACE?
What is their model's performance on RACE?

What deep learning models do they plan to use?
What baseline, if any, is used?
How are the language models used to make predictions on humorous statements?
What type of language models are used? e.g. trigrams, bigrams?

How do attention, recurrent and convolutional networks differ on the language classes they accept?
What type of languages do they test LSTMs on?

What is possible future improvement for proposed method/s?
What is percentage change in performance for better model when compared to baseline?
Which of two design architectures have better performance?

What evaluation metrics did they use?
By how much does their model outperform the baseline?
Which models did they compare with?
What is the source of their datasets?

What new advances are included in this dataset?
What language is this dataset in?

How do they prove that RNNs with arbitrary precision are as powerful as a pushdown automata?
What are edge weights?

Does the paper report F1-scores with and without post-processing for the second task?
What does post-processing do to the output?
Do they test any neural architecture?
Is the performance of a Naive Bayes approach evaluated?

What supports the claim that enhancement in training is advisable as long as enhancement in test is at least as strong as in training?
How does this single-system compares to system combination ones?
What was previous single-system state of the art result on the CHiME-5 data?
How much is error rate reduced by cleaning up training data?

Which baselines were they used for evaluation?
What is the difference in size compare to the previous model?

What languages are used as input?
What are the components of the classifier?
Which uncertain outcomes are forecast using the wisdom of crowds?

What set topics are looked at?
What were the baselines?
Which widely used dataset did the authors use?

How do they perform semi-supervised learning?
What are the five evaluated tasks?

What downstream tasks are explored?
What factors contribute to the stability of the word embeddings?
How is unstability defined?
What embedding algorithms are explored?

Which data-selection algorithms do they use?
How are the artificial sentences generated?
What domain is their test set?

What morphological features are considered?
What type of attention do they use in the decoder?

What set of semantic tags did they use?
How much improvement did they see on the NLI task?

How better are results of new model compared to competitive methods?
What is the metrics used for benchmarking methods?
What are other competitive methods?
What is the size of built dataset?

which had better results, the svm or the random forest model?
which network community detection dataset was used?
did they collect the human labeled data?
how many classes are they classifying?

Do the authors evaluate only on English datasets?
What metrics of gender bias amplification are used to demonstrate the effectiveness of this approach?
How is representation learning decoupled from memory management in this architecture?

What method did the highest scoring team use?
What descriptive statistics are provided about the data?
What was the level of inter-annotator agreement?
What questions were asked in the annotation process?
Why is NER for tweets more challenging as the number of entities increases?
What data preparation steps were used to construct the dataset?

What is the training objective in the method introduced in this paper?
Does regularization of the fine-tuning process hurt performance in the target domain?

What kind of baseline model do they compare against?
Do they analyze which types of sentences/reviews are useful or not?
Which set of datasets do they use?

How long is the dataset?
How are adversarial examples generated?
Is BAT smaller (in number of parameters) than post-trained BERT?
What are the modifications made to post-trained BERT?
What aspects are considered?

Were human evaluations conducted?
What datasets are used?
How does inference time compare to other methods?

Where can I access the dataset?
Did they release their dataset?
Did they use Amazon Mechanical Turk to collect data?
Did they use The Onion as their dataset?

What sources did they get the data from?

What language is the model tested on?
How much lower is the computational cost of the proposed model?
What is the state-of-the-art model?
What is a pseudo language model?

How significant is the performance compared to LSTM model?
How does the introduced model combine the both factors?
How much improvement do the introduced model achieve compared to the previous models?

do they compare their system with other systems?
what is the architecture of their model?
what dataset did they use for this tool?

Did they build a dataset?
Do they compare to other methods?
How large is the dataset?

what is the average number of speakers in the dataset?
by how much is accuracy improved?
what are the previous state of the art systems?

What are the three SOTA models evaluated?
What is the morphological constraint added?

How do they interpret the model?
Do they compare their approach to data-driven only methods?
What are the two applications of neuro-symbolism?

what elements of each profile did they use?
Does this paper discuss the potential these techniques have for invading user privacy?
How is the gold standard defined?

What is the timeframe of the current events?
What model was used for sentiment analysis?
How many tweets did they look at?
What language are the tweets in?

Is this analysis performed only on English data?
Do they authors offer any hypothesis for why the parameters of Zipf's law and Heaps' law differ on Twitter?
What explanation do the authors offer for the super or sublinear urban scaling?
Do the authors give examples of the core vocabulary which follows the scaling relationship of the bulk text?

What syntactic and semantic features are proposed?
Which six speech acts are included in the taxonomy?
what classifier had better performance?
how many tweets were labeled?
how many annotators were there?
who labelled the tweets?
what are the proposed semantic features?
what syntactic features are proposed?
what datasets were used?

What was the baseline?
How many songs were collected?

how does end of utterance and token tags affect the performance
what are the baselines?
what kind of conversations are in the douban conversation corpus?
what pretrained word embeddings are used?

What other evaluation metrics are reported?
What out of domain scenarios did they evaluate on?
What was their state of the art accuracy score?
Which datasets did they use?
What are the neural baselines mentioned?

What regularization methods are used?
What metrics are used?
How long is the dataset?
What dataset do they use?

What simplification of the architecture is performed that resulted in same performance?
How much better is performance of SEPT compared to previous state-of-the-art?

How many actions are present in the dataset?
How many videos did they use?
What unimodal algorithms do they compare with?
What platform was used for crowdsourcing?
What language are the videos in?
How long are the videos?

What was the inter-annotator agreement between the expert annotators?
How were missing hypotheses discovered?

Which aspects of response generation do they evaluate on?
Which dataset do they evaluate on?
What model architecture do they use for the decoder?
Do they ensure the edited response is grammatical?
What do they use as the pre-defined index of prototype responses?

what are all the datasets they experiment with?
what was the baseline model?

What do they mean by explicit selection of most relevant segments?
What datasets they used for evaluation?

Which part of their architecture provides the most speedup in comparison to existing approaches?
Do they consistently outperform existing systems in terms of accuracy?

How big is this dataset?
How are biases identified in the dataset?

How do they deal with imbalanced datasets?
What models do they compare to?
What text preprocessing tasks do they focus on?
What news sources did they get the dataset from?
Did they collect their own corpus?

Do the tweets fall under a specific domain?
How many tweets are in the dataset?
What categories do they look at?

Which knowledge destilation methods do they introduce?
What type of weight pruning do they use?
Which dataset do they train on?
Do they reason why greedy decoding works better then beam search?

Are experiments conducted on multiple datasets?
What baselines is the neural relation extractor compared to?
What additional evidence they use?
How much improvement they get from the previous state-of-the-art?
What is the previous state-of-the-art?

What is the architecture of the model?
What fine-grained semantic types are considered?
What hand-crafted features do other approaches use?

What is the strong baseline model used?
What crowdsourcing platform did they obtain the data from?

How large is the test set?
What does SARI measure?
What are the baseline models?

Is dataset balanced in terms of available data per language?
What datasets are used?

How do they split the dataset when training and evaluating their models?
Do they demonstrate the relationship between veracity and stance over time in the Twitter dataset?
How much improvement does their model yield over previous methods?

How many GPUs do they train their models on?
What of the two strategies works best?
What downstream tasks are tested?

Is this model trained in unsuperized manner?
How much is BELU score difference between proposed approach and insertion-only method?

are the protocols manually annotated?
what ML approaches did they experiment with?

What type of attention is used in the recognition system?
What are the solutions proposed for the seq2seq shortcomings?

How much is training speeded up?
What experiments do they perform?
What is mean field theory?

Which datasets do they evaluate on?
Do they compare against a system that does not use streaming text, but has the entire text at disposal?
Does larger granularity lead to better translation quality?

Do they report results only on English data?
What is the baseline method?
What aspects are used to judge question quality?
What did the human annotations consist of?
What characterizes the 303 domains? e.g. is this different subject tags?

How long is their dataset?
What metrics are used?
What is the best performing system?
What tokenization methods are used?
What baselines do they propose?

What is the size of the dataset?
What models are trained?
Does the baseline use any contextual information?
What is the strong rivaling system?
Where are the debates from?

What is the state-of-the-art model in this task?
How does this result compare to other methods KB QA in CCKS2019?

Do they have an elementary unit of text?
By how much do they outpeform existing text denoising models?
In their nonsymbolic representation can they represent two same string differently depending on the context?
On which datasets do they evaluate their models?

How do they determine demographics on an image?
Do they assume binary gender?
What is the most underrepresented person group in ILSVRC?

How long did the training take?
Is the proposed model smaller or bigger than the conventional NMT system?
Do they compare to state-of-the-art models?

how many sentences did they annotate?
what dataset was used in their experiment?
what are the existing annotation tools?

what ontologies did they use?

How much improvement is given on RACE by their introduced approach?

what pruning did they perform?

Do they evaluate binary paragraph vectors on a downstream task?
How do they show that binary paragraph vectors capture semantics?
Which training dataset do they use?
Do they analyze the produced binary codes?

How long is the dataset?
Do they use machine learning?
What are the ICD-10 codes?

Do they release their code?
What media sources do they use?

What evidence is presented that humans perceive the sentiment classes as ordered?
What size of dataset is sufficiently large for the model performance to approach the inter-annotator agreement?
Which measures of inter-annotator agreement are used?
What statistical test(s) is used to compare the top classification models?

What is the baseline?
How is their NER model trained?
Do they use pretrained word embeddings such as BERT?
How well does the system perform?
Where does their information come from?
What intents do they have?

Is proposed approach compared to some baselines?
What datasets are used for this tasks?
How big are improvements on these tasks?

Which downstream tasks are considered?
How long are the two unlabelled corpora?

Do the authors report only on English data?
How is the impact of ParityBOT analyzed?
What public online harassment datasets was the system validated on?
Where do the supportive tweets about women come from? Are they automatically or manually generated?
How are the hateful tweets aimed at women detected/classified?

How many GPUs do they use for this task?
Do they use all the hidden layer representations?

What languages are used for the experiments?
What is the caching mechanism?
What language model architectures are examined?
What directions are suggested to improve language models?

What logic rules can be learned using ELMo?
Does Elmo learn all possible logic rules?

Which metrics are used for evaluating the quality?

Are face tracking, identification, localization etc multimodal inputs in some ML model or system is programmed by hand?
What are baselines used?

What are the baselines for this paper?
What VQA datasets are used for evaluating this task?
How do they model external knowledge?
What type of external knowledge has been used for this paper?

What is the proposed algorithm or model architecture?
Do they attain state-of-the-art performance?
What fusion methods are applied?
What graph-based features are considered?

How does Overton handles contradictory or incomplete supervision data?
What are high level declarative abstractions Overton provides?
How are applications presented in Overton?
Does Overton support customizing deep learning models without writing any code?

what metrics are used to evaluate the models?
what are the baselines?
what is the size of the dataset?
what dataset did they use?

How is the quality of the translation evaluated?
What are the post-processing approaches applied to the output?
Is the MUSE alignment independently evaluated?
How does byte-pair encoding work?

How many general qualitative statements are in dataset?
What are state-of-the-art models on this dataset?
How are properties being compared annotated?

What state-of-the-art tagging model did they use?

By how much do they improve upon supervised traning methods?
Do they jointly optimize both agents?
Which neural network architecture do they use for the dialog agent and user simulator?
Do they create the basic dialog agent and basic user simulator separately?

Is this done in form of unsupervised (clustering) or suppervised learning?
Does this study perform experiments to prove their claim that indeed personalized profiles will have inclination towards particular cuisines?

What baselines do they compare to?
What training set sizes do they use?
What languages do they experiment with?

What language model is trained?
What machine learning models are considered?
What is the agreement of the dataset?

Do the authors offer any potential reasons why cross-validation variants tend to overestimate the performance, while the sequential methods tend to underestimate it?
Which three variants of sequential validation are examined?
Which three variants of cross-validation are examined?
Which European languages are targeted?
In what way are sentiment classes ordered?
