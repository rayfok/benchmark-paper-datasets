{"id": "1911.10742.397a1e851aab41c455c2b284f5e4947500d797f0", "question": "How big is the ANTISCAM dataset? ", "answer": [" 3,044 sentences in 100 dialogs", "220 human-human dialogs", "220 human-human dialogs.  ... 3,044 sentences in 100 dialogs", "220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. ", "3,044 sentences in 100 dialogs"], "num_words": 5348, "gpt4_answer": "The ANTISCAM dataset consists of 220 human-human dialogs."}
{"id": "1911.10742.cc8b4ed3985f9bfbe1b5d7761b31d9bd6a965444", "question": "How is intent annotated?", "answer": ["On-task dialog are annotated as on-task intents , the other dialog are annotated as pre-defined off-task intents.", "Separate on-task and off task intents and annotate on task for data set specific intents, while annotating  off task intents with a fixed set of general intents.", "separate on-task and off-task intents ... on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task ... off-task content is too general to design task-specific intents, we choose common dialog acts as the categories", "using a hierarchical scheme where on-task intents uses task-related intents for representation and off-task intents chooses dialog acts that convey the syntax information", "using a role-playing task on the Amazon Mechanical Turk platform and collecting typed conversations", "we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories.  ... In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme ... For specific tasks, we also design a semantic slot annotation scheme for annotating sentences based on their semantic content. We identify 13 main semantic slots in the anti-scam task, for example, credit card numbers. We present a detailed semantic slot annotation in Table TABREF3. Following BIBREF1, we segment each conversation turn into single sentences and then annotate each sentence rather than turns."], "num_words": 5348, "gpt4_answer": "In the hierarchical intent annotation scheme, intents are separated into on-task and off-task intents. On-task intents are key actions specific to each task, while off-task intents are common dialog acts that convey syntax information, which are general to all tasks. On-task intents are defined for each task, and off-task intents include six general intents and six additional social intents."}
{"id": "1911.10742.f7662b11e87c1e051e13799413f3db459ac3e19c", "question": "What are the baselines outperformed by this work?", "answer": ["TransferTransfo ...  hybrid model", "TransferTransfo ... Hybrid", "TransferTransfo The vanilla TransferTransfo framework ... Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA", "TransferTransfo and Hybrid "], "num_words": 5348, "gpt4_answer": "This work outperforms two baseline models: TransferTransfo and a hybrid model that combines vanilla TransferTransfo and MISSA."}
{"id": "1911.10742.b584739622d0c53830e60430b13fd3ae6ff43669", "question": "What are the evaluation metrics and criteria used to evaluate the model performance?", "answer": ["Automatic evaluation metrics (Perplexity (PPl), Response-Intent Prediction (RIP), Response-Slot Prediction(RSP), Extended Response-Intent Prediction(ERIP),  Extended Response-Slot Prediction (ERSP)) and Human Evaluation Metrics (Fluency, Coherence, Engagement, Lenhth, TaskSuc)", "Automatic metrics used: Perplexity, RIP, RSP, ERIP ERSP.\nHuman evaluation metrics used: Fluency, Coherence, Engagement, Dialog length and Task Success Score.", "Fluency Fluency is used to explore different models' language generation quality.\n\nCoherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.\n\nEngagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.\n\nDialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.\n\nTask Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score.", "Perplexity  ... Response-Intent Prediction (RIP) ... Response-Slot Prediction (RSP) ... Extended Response-Intent Prediction (ERIP) ... Extended Response-Slot Prediction (ERSP) ... Fluency  ... Coherence  ... Engagement  ... Dialog length (Length)  ... Task Success Score (TaskSuc)", "Perplexity ... Response-Intent Prediction (RIP) ... Response-Slot Prediction (RSP) ... Extended Response-Intent Prediction (ERIP)  ... Extended Response-Slot Prediction (ERSP)  ... Fluency ... Coherence  ... Engagement ... Dialog length  ... Task Success Score (TaskSuc)"], "num_words": 5348, "gpt4_answer": "The evaluation metrics used to evaluate the model performance include Perplexity, Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP), Extended Response-Slot Prediction (ERSP), Fluency, Coherence, Engagement, Dialog length (Length), and Task Success Score (TaskSuc)."}
{"id": "1904.09131.2849c2944c47cf1de62b539c5d3c396a3e8d283a", "question": "What is the accuracy of this model compared to sota?", "answer": ["The accuracy ", "The micro and macro f1-scores of this model are 0.482 and 0.399 on the AIDA-CoNLL dataset, 0.087 and 0.515 on the Microposts 2016 dataset, 0.870 and 0.858 on the ISTEX-1000 dataset, 0.335 and 0.310 on the RSS-500 dataset", "The model improves the state of the art performance for the ISTEX dataset (F1 micro: 0.870, F1 macro: 0.858) and for the Microposts 2016 dataset (F1 micro:  0.087).", "Unanswerable"], "num_words": 2884, "gpt4_answer": "The paper does not provide a direct comparison to state-of-the-art models in terms of accuracy."}
{"id": "1611.06322.1a6156189297b2fe17f174ef55cbd20341bb7dbf", "question": "What previous methods do they compare against?", "answer": ["Liu et al. (2015) and Yang et al. (2012)", "Liu et. al (2015)  ... Yang et. al (2012)", "Liu et. al (2015) ... Yang et. al (2012)", "They compare against two other methods that apply message-,user-, topic- and propagation-based features and rely on an SVM classifier. One perform early rumor detection and operates with a delay of 24 hrs, while the other requires a cluster of 5 repeated messages to judge them for rumors.", "two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. ... Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter."], "num_words": 4314, "gpt4_answer": "The authors compare their method against two state-of-the-art early rumour detection baselines: Liu et al. (2015) and Yang et al. (2012)."}
{"id": "1611.06322.3319d56556ae1597a86384057db0831e32774b90", "question": "What is their evaluation metric?", "answer": ["Accuracy compared to two state-of-the-art baselines", "The metrics are accuracy, detection error trade-off curves and computing efficiency", "accuracy  ... Detection Error Trade-off (DET) curves ... efficiency of computing the proposed features, measured by the throughput per second", "accuracy to evaluate effectiveness ... Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability ... throughput per second"], "num_words": 4314, "gpt4_answer": "The evaluation metrics used in the paper are accuracy and Detection Error Trade-off (DET) curves."}
{"id": "1611.06322.8cbe3fa4ec0f66071e3d6b829b09b6395b631c44", "question": "Are their methods fully supervised?", "answer": ["No. They additionally use similarity to previously detected rumors to make the decision of whether a document is likely to be a rumor"], "num_words": 4314, "gpt4_answer": "Yes, their methods rely on supervised learning using an SVM classifier for rumour detection."}
{"id": "1611.06322.85e417231a4bbb6691f7a89bd81710525f8fec4c", "question": "Do they build a dataset of rumors?", "answer": ["Yes", "Yes, consisting of trusted resources, rumours and non-rumours"], "num_words": 4314, "gpt4_answer": "Yes, they created their own dataset of rumors using Sina Weibo's official rumour debunking service and collected 202 confirmed rumours."}
{"id": "1611.06322.57ee20f494d8ce3fae46028c3f3551d180dba3e0", "question": "What languages do they evaluate their methods on?", "answer": ["Chinese", "Mandarin Chinese", "Mandarin Chinese (see table 3)"], "num_words": 4314, "gpt4_answer": "The paper evaluates their methods on Chinese social media, specifically Sina Weibo."}
{"id": "1611.06322.2974237446d04da33b78ce6d22a477cdf80877b7", "question": "How do they define rumors?", "answer": ["Information of doubtful or unconfirmed truth", "information of doubtful or unconfirmed truth", "information that is not fact- and background-checked and thoroughly investigated for authenticity", "the presence of information unconfirmed by the official media is construed as an indication of being a rumour. "], "num_words": 4314, "gpt4_answer": "The Cambridge dictionary defines a rumour as information of doubtful or unconfirmed truth."}
{"id": "1604.02038.bc8526d4805e2554adb2e9c01736d3f3a3b19895", "question": "What baselines did they compare with?", "answer": ["LDA ... Doc-NADE ... HTMM ... GMNTM", "LDA BIBREF2  ... Doc-NADE BIBREF24 ... HTMM BIBREF9 ... GMNTM BIBREF12 ... LDA BIBREF2  ... Doc-NADE BIBREF24 ... HTMM BIBREF9  ... GMNTM BIBREF12", "LDA BIBREF2 ... Doc-NADE BIBREF24 ... HTMM BIBREF9 ... GMNTM BIBREF12"], "num_words": 3436, "gpt4_answer": "They compared SLRTM with LDA, Doc-NADE, HTMM, and GMNTM as baselines."}
{"id": "1604.02038.a0fd0c0fe042ad045b8d5095c81643ef3a352b81", "question": "Which tasks are explored in this paper?", "answer": ["generative document evaluation task ... document classification task ... topic2sentence task", "generative model evaluation (i.e. test set perplexity) ... document classification", "generative model evaluation (i.e. test set perplexity) and document classification", "generative model evaluation ... document classification"], "num_words": 3436, "gpt4_answer": "This paper explores generative document evaluation, document classification, and topic2sentence tasks."}
{"id": "1911.04474.6e040e80f2da69d50386a90a38ed6d2fa4f77bbd", "question": "Which NER dataset do they use?", "answer": ["CoNLL2003  ... OntoNotes 5.0 ... OntoNotes 4.0 ... MSRA  ... Weibo ... Resume ", "CoNLL2003 ... OntoNotes 5.0 ... BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part ... Chinese NER dataset MSRA ... Weibo NER ... Resume NER", "CoNLL2003 ... OntoNotes 5.0 ... OntoNotes 4.0 ... MSRA ... Weibo NER ... Resume NER", "CoNLL2003 ... OntoNotes 5.0 ... OntoNotes 4.0. ... Chinese NER dataset MSRA ... Weibo NER ... Resume NER"], "num_words": 3719, "gpt4_answer": "They use two English NER datasets (CoNLL2003 and OntoNotes 5.0) and four Chinese NER datasets (OntoNotes 4.0, MSRA, Weibo NER, and Resume NER)."}
{"id": "1911.04474.aebd1f0d728d0de5f76238844da044a44109f76f", "question": "How do they incorporate direction and relative distance in attention?", "answer": ["Self-attention mechanism is changed to allow for direction-aware calculations", "Yes", "by using an relative sinusodial positional embedding and unscaled attention", "calculate the attention scores  which can  distinguish different directions and distances"], "num_words": 3719, "gpt4_answer": "To incorporate direction and relative distance in attention, the authors adopt the relative positional encoding instead of the absolute position encoding and propose a revised relative positional encoding that uses fewer parameters and performs better."}
{"id": "1911.04474.cb4086ad022197da79f28dc609d0de90108c4543", "question": "Do they outperform current NER state-of-the-art models?", "answer": ["No", "Yes", "we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features"], "num_words": 3719, "gpt4_answer": "Yes, the proposed TENER model outperforms current state-of-the-art models in six NER datasets when only using Glove 100d embedding and CNN character embedding."}
{"id": "1905.00840.756a8a9125e6984e0ca768b653c6c760efa3db66", "question": "What was their accuracy score?", "answer": ["95.6% on knowledge authoring, 95% on the manually constructed QA dataset and 100% accuracy on the MetaQA dataset", "KALM achieves an accuracy of 95.6% ... KALM-QA achieves 100% accuracy", "KALM achieves an accuracy of 95.6%, KALM-QA achieves 95% accuracy on the manually constructured general questions dataset based on the 50 logical frames and achieves 100% accuracy on MetaQA dataset", "KALM-QA achieves an accuracy of 95% for parsing the queries ... The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy"], "num_words": 1914, "gpt4_answer": "KALM achieved an accuracy of 95.6% for knowledge authoring, and KALM-QA achieved an accuracy of 95% for parsing the queries and 100% accuracy on the MetaQA dataset for question answering."}
{"id": "1905.00840.fe52b093735bb456d7e699aa9a2b806d2b498ba0", "question": "What are the state-of-the-art systems?", "answer": ["SEMAFOR ... SLING ... Stanford KBP ", "SEMAFOR ... SLING ... Stanford KBP system", "SEMAFOR, SLING, and Stanford KBP system ... BIBREF14"], "num_words": 1914, "gpt4_answer": "The state-of-the-art systems include SEMAFOR, SLING, and Stanford KBP system."}
{"id": "1905.00840.7748c072e07d6c6db5a34be38b4a5e97ac6d7999", "question": "What dataset did they evaluate on?", "answer": [" manually constructed general questions based on the 50 logical frames ... MetaQA dataset", "a manually created dataset of 50 logical frames mostly derived from FrameNet, a manually constructed general questions dataset based on the 50 logical frames and MetaQA dataset", "dataset consisting 250 sentences adapted from FrameNet exemplar sentences, dataset consisting general questions based on 50 logical framesderived from FrameNet, MetaQA dataset", "first dataset is manually constructed general questions based on the 50 logical frames ... second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions"], "num_words": 1914, "gpt4_answer": "They evaluated KALM-QA on two datasets: a manually constructed dataset based on the 50 logical frames and the MetaQA dataset."}
{"id": "1810.02229.c97306c1be5d59cf27b1054adfa8f1da47d292ce", "question": "What are the contributions of this paper?", "answer": ["(1) Using seq2seq for event detection and classification in Italian (2) Investigating quality of Italian word embeddings for this task (3) Comparison to state-of-the-art discrete classifier", "Adapting a seq2seq neural system to event detection and classification for Italian, investigating the quality of existing embeddings for the task, and comparing against a state-of-the-art discrete classifier.", "adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach ... investigation on the quality of existing Italian word embeddings for this task ... a comparison against a state-of-the-art discrete classifier", "the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach ... an investigation on the quality of existing Italian word embeddings for this task ... a comparison against a state-of-the-art discrete classifier ... pre-trained models and scripts running the system"], "num_words": 2330, "gpt4_answer": "The contributions of this paper are: i) adaptation of a state-of-the-art seq2seq neural system for event detection and classification in Italian, ii) investigation of the quality of existing Italian word embeddings for this task, and iii) comparison against a state-of-the-art discrete classifier."}
{"id": "1810.02229.e42916924b69cab1df25d3b4e6072feaa0ba8084", "question": "What are the baselines this paper uses?", "answer": [" cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features", "FBK-HLT - a cascade of two SVM classifiers (one for detection and one for classification)", "FBK-HLT BIBREF23", "Unanswerable"], "num_words": 2330, "gpt4_answer": "The baseline used in this paper is the FBK-HLT system, a cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features."}
{"id": "1810.02229.079ca5810060e1cdc12b5935d8c248492f0478b9", "question": "Can the model be extended to other languages?", "answer": ["Unanswerable", "Yes"], "num_words": 2330, "gpt4_answer": "The paper focuses on Italian, but the approach can be adapted to other languages by using appropriate word embeddings and annotated corpora."}
{"id": "1909.00091.a3e7d7389228a197c8c44e0c504a791b60f2c80d", "question": "How do they decide what is the semantic concept label of particular cluster?", "answer": ["Candidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.", "Given a cluster, our algorithm proceeds with the following three steps:\n\nSense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.\n\nCandidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\n\nCandidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.\n\nIn steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.", "Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering", "They automatically  label the cluster using WordNet and context-sensitive strengths of domain-specific word embeddings"], "num_words": 2916, "gpt4_answer": "The cluster label is determined through a three-step process: 1) Sense disambiguation, which assigns each cluster word to one of its WordNet synsets; 2) Candidate label generation, which takes the union of all hypernyms of the synsets from step 1; and 3) Candidate label ranking, which scores candidate labels by the sum of their distances to each synset from step 1 and ranks them from least to most distance."}
{"id": "1909.00091.8b4bd0a962241ea548752212ebac145e2ced7452", "question": "How do they discover coherent word clusters?", "answer": ["First, they  trained domain-specific word embeddings using the Word2Vec  model, then used k-means clustering to cluster the embeddings of the gender-associated words.", "First, we trained domain-specific word embeddings ... Then, we used k-means clustering to cluster the embeddings of the gender-associated words", "First, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors.", "The authors first generated a set of words which are associated with each gender, then built domain-specific word embeddings and used k-means clustering to cluster the gendered word associations together. "], "num_words": 2916, "gpt4_answer": "They discover coherent word clusters by training domain-specific word embeddings using the Word2Vec CBOW model and then using k-means clustering to cluster the embeddings of the gender-associated words."}
{"id": "1909.00091.d39059340a79bdc0ebab80ad3308e3037d7d5773", "question": "How big are two introduced datasets?", "answer": ["300K sentences in each dataset", "Celeb dataset: 15917 texts and 342645 sentences\nProfessor dataset: 283973 texts and 976677 sentences", "Celebrity Dataset has 15,917 texts, 342,645 sentences, and the Female Male Proportions are  0.67/ 0.33. \nProfessor Dataset has 283,973 texts, 976, 667 sentences, and the Femal Male Proportions are 0.28./ 0,72", "each consisting of over 300K sentences"], "num_words": 2916, "gpt4_answer": "Each of the two datasets introduced consists of over 300K sentences."}
{"id": "1909.00091.31d4b0204702907dc0cd0f394cf9c984649e1fbf", "question": "What are strong baselines authors used?", "answer": ["The authors contrasted human evaluations against a random baseline, and used the centroid of the cluster as a strong baseline.", "Unanswerable", "the top 4 predicted labels and the centroid of the cluster", "the top 4 predicted labels and the centroid of the cluster as a strong baseline label"], "num_words": 2916, "gpt4_answer": "The strong baselines used for cluster labels were the centroid of the cluster."}
{"id": "1909.04387.371433bd3fb5042bacec4dfad3cfff66147c14f0", "question": "How do data-driven models usually respond to abuse?", "answer": ["Data-driven systems rank low in general", "either by refusing politely, or, with flirtatious responses, or, by retaliating", "flirt; retaliation", "politely refuse ... politely refuses ... flirtatious responses"], "num_words": 1728, "gpt4_answer": "Data-driven models generally rank low in terms of perceived appropriateness, often producing responses that can be interpreted as flirtatious or retaliatory. They lack effective control mechanisms and do not adapt their responses sufficiently to the context of the abuse."}
{"id": "1909.04387.f64449a21c452bc5395a0f0a49fb49825e6385f4", "question": "How much data did they gather from crowdsourcing?", "answer": ["600K", "9960", "9960 HITs", "9960 HITs from 472 crowd workers"], "num_words": 1728, "gpt4_answer": "They gathered 9960 HITs from 472 crowd workers."}
{"id": "1909.04387.3aeb25e334c8129b376f11c7077bcb2dd54f7e0e", "question": "How many different strategies were evaluated?", "answer": ["12", "14", "Unanswerable"], "num_words": 1728, "gpt4_answer": "14 response types were evaluated."}
{"id": "2003.13016.c19e9fd2f1c969e023fb99b74e78eb1f3db8e162", "question": "Was the automatic annotation evaluated?", "answer": ["No", "Yes"], "num_words": 4406, "gpt4_answer": "Yes, the automatic annotation of time expressions using a customized version of HeidelTime was evaluated."}
{"id": "1805.11937.230ff86b7b90b87c33c53014bb1e9c582dfc107f", "question": "What morphological typologies are considered?", "answer": ["Turkish, Finnish, Czech, German, Spanish, Catalan and English", "agglutinative and fusional", "agglutinative and fusional languages"], "num_words": 3103, "gpt4_answer": "The paper considers agglutinative and fusional languages for morphological typologies."}
{"id": "1805.11937.dc23006d67f20f430f1483398de4a89c0be4efe2", "question": "Does the model consider both derivational and inflectional morphology?", "answer": ["Yes"], "num_words": 3103, "gpt4_answer": "The paper does not explicitly mention considering both derivational and inflectional morphology."}
{"id": "1805.11937.887d7f3edf37ccc6bf2e755dae418b04d2309686", "question": "What type of morphological features are used?", "answer": ["For all languages, morph outputs the lemma of the token followed by language specific morphological tags ... additional information for some languages, such as parts-of-speech tags for Turkish", "char3 slides a character window of width $n=3$ over the token ... lemma of the token ... additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. ... characters ... character sequences", "language specific morphological tags", "morph outputs the lemma of the token followed by language specific morphological tags ... semantic roles of verbal predicates"], "num_words": 3103, "gpt4_answer": "Gold morphological features are used as outputs of the morph-language in the study."}
{"id": "1909.09070.b8a3ab219be6c1e6893fe80e1fbf14f0c0c3c97c", "question": "What datasets are used in this paper?", "answer": ["Semantic Scholar corpus BIBREF21 (SemScholar) ... Springer Nature's SciGraph ... Textbook Question Answering corpus BIBREF23 ... Wikipedia ... Flickr30K ... COCO", "The Semantic Scholar corpus  ... Springer Nature's SciGraph ... The Textbook Question Answering corpus ... Wikipedia ... Flickr30K and COCO", "The Semantic Scholar corpus ... Springer Nature's SciGraph ... The Textbook Question Answering corpus ... January 2018 English Wikipedia dataset ... Flickr30K ... COCO", "The Semantic Scholar corpus ... Springer Nature's SciGraph ... The Textbook Question Answering corpus ... Wikipedia ... Flickr30K ... COCO"], "num_words": 5321, "gpt4_answer": "The datasets used in this paper are Semantic Scholar corpus (SemScholar), Springer Nature's SciGraph, Textbook Question Answering corpus (TQA), Wikipedia, Flickr30K, and COCO."}
{"id": "1909.09070.780c7993d446cd63907bb38992a60bbac9cb42b1", "question": "What language are the captions in?", "answer": ["English", "Unanswerable"], "num_words": 5321, "gpt4_answer": "The captions are in English."}
{"id": "1909.09070.3da4606a884593f7702d098277b9a6ce207c080b", "question": "What ad-hoc approaches are explored?", "answer": ["Embedding network ... 2WayNet ... VSE++ ... DSVE-loc)", "HolE ... Vecsigrafo", "Unanswerable"], "num_words": 5321, "gpt4_answer": "The paper demonstrates the positive impact of the features learned in the FCC task in other tasks involving scientific text and figures, like multi-modal classification and machine comprehension for question answering, outperforming supervised baselines and ad-hoc approaches."}
{"id": "1909.09070.91336f12ab94a844b66b607f8621eb8bbd209f32", "question": "What supervised baselines did they compare with?", "answer": ["The direct combination baseline  ... The supervised pre-training baseline", "direct combination ... supervised pre-training", "direct combination baseline ... supervised pre-training baseline"], "num_words": 5321, "gpt4_answer": "They compared the FCC task with two supervised baselines: the direct combination baseline and the supervised pre-training baseline. Both baselines train the vision and language networks independently and then combine them."}
{"id": "1909.09070.c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "question": "Is the data specific to a domain?", "answer": ["No", "Yes"], "num_words": 5321, "gpt4_answer": "Yes, the data is specific to the scientific domain."}
{"id": "1909.09070.42a4ab4607a9eec42c427a817b7e898230d26444", "question": "Where do their figure and captions come from?", "answer": ["SN SciGraph and AI2 Semantic Scholar", "The Semantic Scholar corpus ... Springer Nature's SciGraph", "scientific literature", "scientific publications ... middle school science curricula"], "num_words": 5321, "gpt4_answer": "The figures and captions come from the Semantic Scholar corpus (SemScholar) and Springer Nature's SciGraph."}
{"id": "1708.05521.622efbecd9350a0f4487bdff2b8b362ef2541f3c", "question": "did the top teams experiment with lexicons?", "answer": ["Unanswerable"], "num_words": 2063, "gpt4_answer": "Unanswerable"}
{"id": "1708.05521.f54e19f7ecece1bb0ef3171403ae322ad572ff00", "question": "did they experiment with lexicons?", "answer": ["No", "Unanswerable"], "num_words": 2063, "gpt4_answer": "No, they did not experiment with lexicons."}
{"id": "1708.05521.4137a82d7752be7a6c142ceb48ce784fd475fb06", "question": "what was the baseline?", "answer": [" Weka baseline BIBREF5", "Weka", "Weka baseline BIBREF5"], "num_words": 2063, "gpt4_answer": "The baseline was the Weka baseline."}
{"id": "1708.05521.6c50871294562e4886ede804574e6acfa8d1a5f9", "question": "what was their result?", "answer": ["0.689 on development and 0.522 on test set", "For the anger dataset, our experiments showed that GloVe embeddings of dimension 50 outperformed others, obtaining an average gain of 0.066 correlation over embeddings of size 25 and of 0.021 for embeddings of size 100. ... In the joy dataset, our experiments showed us that GloVe vectors of dimension 50 again outperformed others, in this case obtaining an average correlation gain of 0.052 ( INLINEFORM0 ) over embeddings of size 100, and of 0.062 ( INLINEFORM1 ) for size 25. ... On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively. ... on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points INLINEFORM0 over size 25", "Pearson correlation on sadness test data is 0.52, on joy test data is .537, on anger test data is 0.47, on fear data is 0.561.", "Unanswerable"], "num_words": 2063, "gpt4_answer": "Their model obtained the 13th place among 22 competitors in the WASSA-2017 Shared Task on Emotion Intensity."}
{"id": "1708.05521.0ac6fbd81e2dd95b800283dc7e59ce969d45fc02", "question": "what dataset was used?", "answer": [" training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger", "Dataset from shared task BIBREF5", "Dataset of tweets provided for the shared task.", "datasets provided for the shared task BIBREF5"], "num_words": 2063, "gpt4_answer": "The dataset used was the training, validation, and test datasets provided for the WASSA-2017 Shared Task on Emotion Intensity, which include tweets for four emotions: joy, sadness, fear, and anger."}
{"id": "1908.11049.ed44f7e698d6124cb86791841d02fc6f8b4d862a", "question": "What is their definition of hate speech?", "answer": [" in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis.", "Hate speech is a text that contains one or more of the following aspects: directness, offensiveness, targeting a group or individual based on specific attributes, overall negativity.", "rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech"], "num_words": 4210, "gpt4_answer": "The paper does not provide a specific definition for hate speech."}
{"id": "1908.11049.d9e7633004ed1bc1ee45be58409bcc1fa6db59b2", "question": "What languages does the new dataset contain?", "answer": ["English ... French ... Arabic", "English, French, and Arabic "], "num_words": 4210, "gpt4_answer": "The new dataset contains English, French, and Arabic tweets."}
{"id": "1908.11049.c58ef13abe5fa91a761362ca962d7290312c74e4", "question": "What aspects are considered?", "answer": [" (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments", "(a) whether the text is direct or indirect ... (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal ... (c) the attribute based on which it discriminates against an individual or a group of people ... (d) the name of this group ... (e) how the annotators feel about its content within a range of negative to neutral sentiments", "Directness ... Hostility ... Target group ... Target ... Sentiment of the annotator", "whether the text is direct or indirect ... if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal ... the attribute based on which it discriminates against an individual or a group of people ... the name of this group ...  how the annotators feel about its content within a range of negative to neutral sentiments"], "num_words": 4210, "gpt4_answer": "The paper considers five aspects in hate speech analysis: (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments."}
{"id": "1908.11049.9ef0d2365bde0d18054511fbb53cec5fa2cda5ee", "question": "How big is their dataset?", "answer": ["13 000 tweets", "13014", "5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets"], "num_words": 4210, "gpt4_answer": "The final dataset consists of 5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets."}
{"id": "1907.10676.cbb3c1c1e6e1818b6480f929f1c299eaa5ffd07a", "question": "What are the opportunities presented by the use of Semantic Web technologies in Machine Translation?", "answer": ["Disambiguation ... Named Entities ... Non-standard speech ... Translating KBs", "SWT can be applied to support the semantic disambiguation in MT: to  recognize ambiguous words before translation and  as a post-editing technique applied to  the output language. SWT may be used for translating KBs.", "disambiguation ... NERD ...  non-standard language ... translating KBs", "disambiguation ... Named Entities ... Non-standard speech ... Translating KBs"], "num_words": 3339, "gpt4_answer": "The opportunities presented by the use of Semantic Web technologies in Machine Translation include addressing lexical and syntactic ambiguity, improving translation quality, handling named entities, translating knowledge bases, and dealing with non-standard speech."}
{"id": "1907.10676.9f74f3991b8681619d95ab93a7c8733a843ddffe", "question": "What are the challenges associated with the use of Semantic Web technologies in Machine Translation?", "answer": ["SWT are hard to implement", "reordering errors ...  lexical and syntactic ambiguity", "syntactic disambiguation problem which as yet lacks good solutions ... directly related to the ambiguity problem and therefore has to be resolved in that wider context ... In rare cases SMT can solve this problem, but considering that new idiomatic expressions appear every day and most of them are isolated sentences, this challenge still remains open"], "num_words": 3339, "gpt4_answer": "The challenges associated with the use of Semantic Web technologies in Machine Translation include disambiguation, named entities, non-standard speech, and translating knowledge bases."}
{"id": "1907.10676.7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "question": "What are the other obstacles to automatic translations which are not mentioned in the abstract?", "answer": ["Excessive focus on English and European languages ... limitations of SMT approaches for translating across domains ... no-standard speech texts from users ... morphologically rich languages ... parallel data for training differs widely from real user speech", "Unanswerable", "reordering errors"], "num_words": 3339, "gpt4_answer": "Reordering errors are another obstacle to automatic translations not mentioned in the abstract."}
{"id": "1906.08871.a77d38427639d54461ae308f3045434f81e497d0", "question": "what eeg features were used?", "answer": ["We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0 ...  So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel ... We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. ", "root mean square ... zero crossing rate ... moving window average ... kurtosis ... power spectral entropy", "root mean square ... zero crossing rate ... moving window average ... kurtosis ... power spectral entropy ... extracted 31(channels) X 5 or 155 features", "root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy"], "num_words": 2828, "gpt4_answer": "The EEG features used were root mean square, zero crossing rate, moving window average, kurtosis, and power spectral entropy."}
{"id": "1906.08871.010fd15696580d9924ac0275a4ff269005e5808d", "question": "what were the baselines?", "answer": ["Unanswerable"], "num_words": 2828, "gpt4_answer": "Unanswerable"}
{"id": "1906.08871.d36a6447bfe58204e0d29f9213d84be04d875624", "question": "what dataset was used?", "answer": [" two types of simultaneous speech EEG recording databases ", "For database A five female and five male subjects took part in the experiment. ... For database B five male and three female subjects took part in the experiment.", "Speech EEG recording collected from male and female subjects under different background noises", "The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment."], "num_words": 2828, "gpt4_answer": "The authors built two types of simultaneous speech EEG recording databases, referred to as dataset A and dataset B, using recordings from different subjects in different noise conditions."}
{"id": "2004.04124.5ed02ae6c534cd49d405489990f0e4ba0330ff1b", "question": "Does LadaBERT ever outperform its knowledge destilation teacher in terms of accuracy on some problems?", "answer": ["No"], "num_words": 4174, "gpt4_answer": "No, the paper does not claim that LadaBERT outperforms its knowledge distillation teacher in terms of accuracy."}
{"id": "2004.04124.f6346828c2f44529dc307abf04dd246bfeb4a9b2", "question": "Do they evaluate which compression method yields the most gains?", "answer": ["No", "Yes"], "num_words": 4174, "gpt4_answer": "The paper does not specifically evaluate which compression method yields the most gains but demonstrates that the combination of matrix factorization, weight pruning, and knowledge distillation in LadaBERT consistently outperforms the strongest baselines under similar model sizes."}
{"id": "2004.04124.935873b97872820b7b6100d6a785fba286b94900", "question": "On which datasets does LadaBERT achieve state-of-the-art?", "answer": ["LadaBERT -1, -2 achieves state of art on all datasets namely, MNLI-m MNLI-mm, SST-2, QQP, and QNLI. \nLadaBERT-3 achieves SOTA on the first four dataset. \nLadaBERT-4 achieves SOTA on MNLI-m, MNLI-mm, and QNLI ", "LadaBERT-1 and LadaBERT-2  on MNLI-m, MNLI-mm, SST-2, QQP and QNLI .\nLadaBERT-3  on MNLI-m, MNLI-mm, SST-2, and QQP . LadaBERT-4  on MNLI-m, MNLI-mm and QNLI .", "MNLI-m, MNLI-mm, SST-2, QQP, QNLI", "SST-2 ... MNLI-m ... MNLI-mm ... QNLI ... QQP"], "num_words": 4174, "gpt4_answer": "LadaBERT achieves state-of-the-art accuracy on various public datasets, including sentiment classification (SST-2), natural language inference (MNLI-m, MNLI-mm, QNLI), and pairwise semantic equivalence (QQP)."}
{"id": "1603.07252.f2bcfdbebb418e7da165c19b8c7167719432ee48", "question": "What domain of text are they working with?", "answer": ["news", "news articles"], "num_words": 3981, "gpt4_answer": "They are working with news articles and their corresponding highlights from the DailyMail website."}
{"id": "1603.07252.0fe49431db5ffaa24372919daf24d8f84117bfda", "question": "What dataset do they use?", "answer": ["DUC 2002 ... our own Dailymail news highlights corpus", "DUC 2002 document summarization corpus ... our own DailyMail news highlights corpus", "DailyMail news articles", "the benchmark DUC 2002 document summarization corpus ... DailyMail news highlights corpus"], "num_words": 3981, "gpt4_answer": "The authors use two datasets: the DUC 2002 single document summarization corpus and their own DailyMail news highlights corpus."}
{"id": "1603.07252.0f9c1586f1b4b531fa4fd113e767d06af90b1ae8", "question": "Do they compare to abstractive summarization methods?", "answer": ["Yes"], "num_words": 3981, "gpt4_answer": "Yes, the paper compares the neural sentence extraction model (nn-se) and the word extraction model (nn-we) to a neural abstractive baseline (nn-abs)."}
{"id": "1708.00549.52faf319e37aa15fff1ab47f634a5a584dc42e75", "question": "What types of commonsense knowledge are they talking about?", "answer": ["Hypernymy or is-a relations between words or phrases", "Knowledge than an ordinary person would have such as transitive entailment relation, complex ordering, compositionality, multi-word entities", "hypernym relations", "the collection of information that an ordinary person would have"], "num_words": 1837, "gpt4_answer": "The paper focuses on learning commonsense knowledge representations, particularly ontologies of hierarchical data, using the order-embedding model for hierarchical relation learning."}
{"id": "1708.00549.0c7cb3010ed92b8d46583a67e72946a6c0115f1f", "question": "What do they mean by intrinsic geometry of spaces of learned representations?", "answer": ["In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.", "The intrinsic geometry is that the general concept embedding should be smaller than the specific concept embedding in every coordinate of the embeddings", "the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than local relation predictions"], "num_words": 1837, "gpt4_answer": "Intrinsic geometry of spaces of learned representations refers to the underlying structure and relationships within the embedding space that can enforce consistency in predictions and capture complex structural constraints."}
{"id": "1905.00472.9c2cacf77041e02d38f92a4c490df1e04552f96f", "question": "Did they pre-train on existing sentiment corpora?", "answer": ["No, they used someone else's pretrained model. ", "Yes"], "num_words": 3436, "gpt4_answer": "Yes, they used pre-trained models on existing sentiment corpora for their approach."}
{"id": "1905.00472.35cdaa0fff007add4a795850b139df80af7d1ffc", "question": "What were the most salient features extracted by the models?", "answer": ["Unanswerable", "unigrams and bigrams ... word2vec ... manually constructed lexica ... sentiment embeddings"], "num_words": 3436, "gpt4_answer": "The most salient features extracted by the models were word unigrams and bigrams, term frequencies (TF) and Inverse document-frequency (IDF), 300-dimensional word2vec word representations, Empath lexical categories, and pre-trained sentiment embeddings."}
{"id": "1905.00472.3de3a083b8ba3086792d38ae9667e095070f7f37", "question": "How many languages are in the dataset?", "answer": ["2", "2 (Spanish and English)"], "num_words": 3436, "gpt4_answer": "The dataset includes English and Spanish languages."}
{"id": "1905.00472.04914917d01c9cd8718cd551dc253eb3827915d8", "question": "Did the system perform well on low-resource languages?", "answer": ["Unanswerable"], "num_words": 3436, "gpt4_answer": "The system outperformed other competitors for both English and Spanish in the Sentiment, Emotion and Cognitive State Task pilot."}
{"id": "1912.02866.20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea", "question": "What are the parts of the \"multimodal\" resources?", "answer": ["grouping, connectivity, and discourse structure ", "node types that represent different diagram elements ... The same features are used for both AI2D and AI2D-RST for nodes with layout information ... discourse relations ... information about semantic relations", "spatial organisation  ... discourse structure"], "num_words": 4134, "gpt4_answer": "The parts of the multimodal resources include text, graphics, arrows, arrowheads (in AI2D only), groups of diagram elements (in AI2D-RST), and the image constant."}
{"id": "1912.02866.a57e266c936e438aeeab5e8d20d9edd1c15a32ee", "question": "Are annotators familiar with the science topics annotated?", "answer": ["The annotation for AI2D was\ncreated by crowd-sourced non-expert annotators on AMT while AI2D-RST covers a subset of diagrams from AI2D annotated by trained experts", "Unanswerable"], "num_words": 4134, "gpt4_answer": "The paper does not explicitly mention the annotators' familiarity with the science topics annotated."}
{"id": "1912.02866.27356a99290fcc01e3e5660af3405d2a6c6f6e7c", "question": "How are the expert and crowd-sourced annotations compared to one another?", "answer": ["Expert annotators incorporate domain knowledge from multimodality theory while non-expert cannot but they are less time-consuming and use less resources.", "by using them as features in classifying diagrams and\ntheir parts using various graph neural networks.", "results are not entirely comparable due to different node types ... more reasonable to compare architectures"], "num_words": 4134, "gpt4_answer": "The expert and crowd-sourced annotations are compared through two tasks, graph and node classification, using four different architectures for graph neural networks. The comparison aims to evaluate the effectiveness of the annotations in representing the multimodal structure of diagrams for representation learning."}
{"id": "1912.02866.6e37f43f4f54ffc77c785d60c6058fbad2147922", "question": "What platform do the crowd-sourced workers come from?", "answer": ["Amazon Mechanical Turk", "Unanswerable"], "num_words": 4134, "gpt4_answer": "The crowd-sourced workers come from Amazon Mechanical Turk."}
{"id": "1912.02866.fff1ed2435ba622d884ecde377ff2de127167638", "question": "Who are considered trained experts?", "answer": ["Annotators trained on multimodality theory", "Those who have domain knowledge on multimodal communication and annotation.", "Unanswerable", "domain knowledge from multimodality theory"], "num_words": 4134, "gpt4_answer": "Trained experts are individuals with domain knowledge in multimodal communication and experience in annotating diagrams based on state-of-the-art theories and approaches."}
{"id": "1812.00382.7ff7c286d3118a8be5688e2d18e9a56fe83679ad", "question": "Which model architecture do they opt for?", "answer": ["CNN ... RNN", "HAN BIBREF10 ... CNN BIBREF11", "RNNs and CNNs", "Recurrent Neural Networks ... Convolutional Neural Networks"], "num_words": 2610, "gpt4_answer": "The authors opt for two model architectures: Recurrent Neural Networks (RNNs) with Hierarchical Attention Network (HAN) and Convolutional Neural Networks (CNNs)."}
{"id": "1812.00382.1ecbbb60dc44a701e9c57c22167dd412711bb0be", "question": "Which dataset do they use?", "answer": ["Clueweb09", "Clueweb09 derived dataset ... Wikipedia crawl data", "Clueweb09 derived dataset ... new dataset based on Wikipedia crawl data", "the Clueweb09 derived dataset  ... dataset based on Wikipedia crawl data"], "num_words": 2610, "gpt4_answer": "The authors use the Clueweb09 derived dataset for baseline comparison and generate a new dataset based on Wikipedia crawl data for cross-temporal, cross-topic, and cross-domain training & evaluation."}
{"id": "1812.00382.592df9831692b8fde213257ed1894344da3e0594", "question": "Which setup shows proves to be the hardest: cross-topic, cross-domain, cross-temporal, or across annotators?", "answer": ["Unanswerable"], "num_words": 2610, "gpt4_answer": "The paper does not explicitly rank the difficulty of these setups, but it demonstrates that neural methods perform well and show robustness in cross-topic, cross-domain, and cross-temporal tasks while being in line with human annotators regarding certainty and disagreement."}
{"id": "1812.00382.6822ca5f7a19866ffc3c985b790a4aadcecf2d1c", "question": "Which weak signal data do they use?", "answer": ["Unanswerable", "semantic representations of word embeddings"], "num_words": 2610, "gpt4_answer": "The paper does not explicitly mention the use of specific weak signal data."}
{"id": "1812.00382.60e6296ca2a697892bd67558a21a83ef01a38177", "question": "Do they compare their semantic feature approach to lexical approaches?", "answer": ["Yes"], "num_words": 2610, "gpt4_answer": "Yes, the authors compare their semantic feature approach (neural methods) to lexical approaches in terms of robustness, performance, and human agreement."}
{"id": "1903.02930.9b868c7d17852f46a8fe725f24cb9548fdbd2b05", "question": "what dataset was used for training?", "answer": ["64M segments from YouTube videos", "About 64M segments from YouTube videos comprising a total of 1.2B tokens.", "YouCook2  ... sth-sth"], "num_words": 1300, "gpt4_answer": "The training data consist of about 64M segments from YouTube videos."}
{"id": "1903.02930.243cf21c4e34c4b91fcc4905aa4dc15a72087f0c", "question": "what is the size of the training data?", "answer": ["64M", "64M segments from YouTube videos ... INLINEFORM0 B tokens ... vocabulary of 66K wordpieces", "64M video segments with 1.2B tokens"], "num_words": 1300, "gpt4_answer": "The training data consist of about 64 million segments from YouTube videos."}
{"id": "1903.02930.488e3c4fd1103c46e12815d1bf414a0356fb0d0e", "question": "what features were derived from the videos?", "answer": ["1500-dimensional vectors similar to those used for large scale image classification tasks.", "1500-dimensional vectors, extracted from the video frames at 1-second intervals", "features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks"], "num_words": 1300, "gpt4_answer": "The features derived from the videos are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks."}
{"id": "1911.04873.84765903b8c7234ca2919d0a40e3c6a5bcedf45d", "question": "Do any of the models use attention?", "answer": ["Yes"], "num_words": 1729, "gpt4_answer": "Yes, the models use an established NMT architecture based on LSTMs and implementing the attention mechanism."}
{"id": "1911.04873.38363a7ed250bc729508c4c1dc975696a65c53cb", "question": "What translation models are explored?", "answer": ["LSTM with attention", "NMT architecture BIBREF10", "architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism"], "num_words": 1729, "gpt4_answer": "The paper explores neural machine translation (NMT) models based on LSTMs and implementing the attention mechanism."}
{"id": "1911.04873.e862ebfdb1b3425af65fec81c8984edca6f89a76", "question": "What is symbolic rewriting?", "answer": ["It is a process of translating a set of formal symbolic data to another set of formal symbolic data.", "Symbolic rewriting is the method to rewrite ground and nonground data from one to another form using rules.", "Unanswerable"], "num_words": 1729, "gpt4_answer": "Symbolic rewriting is the process of transforming one symbolic expression into another based on a set of rules or equivalences."}
{"id": "1606.07043.ec8f39d32084996ab825debd7113c71daac38b06", "question": "How do they incorporate expert knowledge into their topic model?", "answer": ["The experts define anchors and the model learns correlations between the anchors and latent topics.", "They use an anchored information theoretic topic modeling using Correlation Explanation and  information bottleneck.", "anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors"], "num_words": 2183, "gpt4_answer": "They incorporate expert knowledge into their topic model by using anchor words specified by an expert to guide the discovery of interpretable latent topics in text."}
{"id": "1606.07043.a67a2d9acad1787b636ca2681330f4c29a0b0254", "question": "On which corpora do they evaluate on?", "answer": [" i2b2 2008 Obesity Challenge BIBREF22 ... 20 Newsgroups", "20 Newsgroups  ... i2b2 2008 Obesity Challenge", "20 Newsgroups ... i2b2 2008 Obesity Challenge BIBREF22 data set"], "num_words": 2183, "gpt4_answer": "They evaluate on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge data set."}
{"id": "1606.07043.1efaf3bcd66d1b6bdfb124f0cec0cfeee27e6124", "question": "Do they compare against popular topic models, such as LDA?", "answer": ["No"], "num_words": 2183, "gpt4_answer": "The paper does not directly compare against popular topic models like LDA."}
{"id": "1611.04234.fcdbaa08cccda9968f3fd433c99338cc60f596a7", "question": "What is F-score obtained?", "answer": ["50.60 on Named Entity and 59.32 on Nominal Mention", "Best F1 score obtained is 54.82% overall", "Best proposed model achieves F1 score of 50.60, 59.32, 54.82, 20.96 on Named Entity, Nominam Mention, Overall, Out of vocabulary respectively.", "For Named Entity, F-Score Driven I model had 49.40 F1 score,  and F-Score Driven II model had 50.60  F1 score. In case of Nominal Mention, the scores were 58.16 and 59.32"], "num_words": 1895, "gpt4_answer": "Unanswerable"}
{"id": "1611.04234.2e4688205c8e344cded7a053b6014cce04ef1bd5", "question": "What is the state-of-the-art?", "answer": ["Peng and Dredze peng-dredze:2016:P16-2"], "num_words": 1895, "gpt4_answer": "The state-of-the-art NER system in Chinese social media is by Peng and Dredze (peng-dredze:2016:P16-2). The integrated training model (F-Score Driven Model II) proposed in this paper achieves a new state-of-the-art NER system in Chinese social media."}
{"id": "1611.04234.fc436a4f3674e42fb280378314bfe77ba0c99f2e", "question": "Which Chinese social media platform does the data come from?", "answer": ["Sina Weibo", "Sina Weibo service", "Unanswerable"], "num_words": 1895, "gpt4_answer": "The data comes from Sina Weibo service in China."}
{"id": "1611.04234.a71fb012631e6a8854d5945b6d0ab2ab8e7b7ee6", "question": "What dataset did they use?", "answer": ["Peng and Dredze peng-dredze:2016:P16-2", "Peng and Dredze peng-dredze:2016:P16-2 ... Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service", "a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2"], "num_words": 1895, "gpt4_answer": "They used a modified labelled corpus as Peng and Dredze for NER in Chinese social media and unlabelled text from Sina Weibo service in China."}
{"id": "1909.00437.b70e4c49300dc3eab18e907ab903afd2a0c6075a", "question": "What are the five downstream tasks?", "answer": ["NLI (XNLI dataset) ... document classification (MLDoc dataset) ...  intent classification ... sequence tagging tasks: POS tagging ... NER", "NLI (XNLI dataset) ... document classification (MLDoc dataset) ... intent classification ... POS tagging ... NER", "NLI ... document classification ... intent classification ... POS tagging ... NER", "These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER."], "num_words": 3870, "gpt4_answer": "The five downstream tasks are natural language inference (NLI), document classification, intent classification, part-of-speech (POS) tagging, and named entity recognition (NER)."}
{"id": "1909.00437.088d42ecb1e15515f6a97a0da2fed81b61d61a23", "question": "Is this more effective for low-resource than high-resource languages?", "answer": ["Yes", "we see that the gains are more pronounced in low resource languages"], "num_words": 3870, "gpt4_answer": "Yes, the massively multilingual neural machine translation system shows improved translation performance on low-resource languages, hinting at potential cross-lingual transfer capability for downstream tasks."}
{"id": "1909.00437.8599d6d14ac157169920c73b98a79737c7a68cf5", "question": "Is mBERT fine-tuned for each language?", "answer": ["No", "Yes"], "num_words": 3870, "gpt4_answer": "No, mBERT is not fine-tuned for each language individually; it is trained on text from multiple languages and used for cross-lingual transfer tasks."}
{"id": "1909.00437.f1d61b44105e651925d02a51e6d7ea10ea28ebd8", "question": "How did they select the 50 languages they test?", "answer": ["For a given language pair, $l$, let $D_l$ be the size of the available parallel corpus. Then if we adopt a naive strategy and sample from the union of the datasets, the probability of the sample being from language pair $l$ will be $p_l=\\frac{D_l}{\\Sigma _lD_l}$. However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy. To control the contribution of each language pair when constructing the vocabulary, we use the same temperature based sampling strategy with $T=5$. Our SPM vocabulary has a character coverage of $0.999995$.", "These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model", "intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model"], "num_words": 3870, "gpt4_answer": "The paper does not provide explicit details on how they selected the 50 languages for testing. However, they mention using 5 downstream tasks that cover a diverse set of over 50 languages. The selection seems to be based on the availability of data for those tasks in the respective languages."}
{"id": "2003.07568.108f99fcaf620fab53077812e8901870896acf36", "question": "What kind of evaluations do use to evaluate dialogue?", "answer": ["They use automatic evaluation using perplexity and BLEU scores with reference to the human-annotated responses and human evaluation on interestingness, engagingness, and humanness.", "Unanswerable", "perplexity (ppl.) and BLEU ... which of the two dialogues is better in terms of engagingness, interestingness, and humanness", "perplexity ... BLEU ... ACUTE-EVA"], "num_words": 3827, "gpt4_answer": "The paper uses both automatic and human evaluations to evaluate dialogue. Automatic evaluations include perplexity and BLEU scores, while human evaluations use the ACUTE-EVAL method, which is an A/B test evaluation for dialogue systems."}
{"id": "2003.07568.6c8dc31a199b155e73c84173816c1e252137a0af", "question": "By how much do their cross-lingual models lag behind other models?", "answer": ["BLUE score is lower by 4 times than that of the best multilingual model.", "significant gap between the cross-lingual model and other models ... Table TABREF20"], "num_words": 3827, "gpt4_answer": "The cross-lingual models show inferior performance compared to other models, generating repetitive responses and struggling with multi-turn dialogue tasks."}
{"id": "2003.07568.7125db8334a7efaf9f7753f2c2f0048a56e74c49", "question": "Which translation pipelines do they use to compare against?", "answer": ["Google Translate API", "M-Bert2Bert ... M-CausalBert ... Bert2Bert ... CausalBert ... Poly-encoder BIBREF75 ... XNLG", "Translate source sentence to English with Google Translate API and then translate the result to the target language with Poly-encoder."], "num_words": 3827, "gpt4_answer": "The paper compares against the Poly-encoder, a translation-based model that uses the Google Translate API for translating target languages to English and back to the target language."}
{"id": "2003.07568.43729be0effb5defc62bae930ceacf7219934f1e", "question": "Which languages does their newly created dataset contain?", "answer": ["Chinese ... French ... Indonesian ... Italian ... Korean ... Japanese", "Chinese, French, Indonesian, Italian, Korean, and Japanese", "English ... Chinese ... French ... Indonesian ... Italian ... Korean ... Japanese"], "num_words": 3827, "gpt4_answer": "The newly created dataset, XPersona, contains six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese."}
{"id": "1810.02268.ae2142ee9e093ce485025168f4bcb3da4602739d", "question": "did they collect their own contrastive test set?", "answer": ["It is automatically created from the OpenSubtitles corpus.", "No", "Yes"], "num_words": 3922, "gpt4_answer": "Yes, they collected their own large-scale contrastive test set for evaluating pronoun translation in neural machine translation."}
{"id": "1810.02268.ebe1084a06abdabefffc66f029eeb0b69f114fd9", "question": "what are the baselines?", "answer": [" standard bidirectional RNN model with attention ... A standard context-agnostic Transformer", "bidirectional RNN model with attention ... concat22 ... s-hier ... s-t-hier ... s-hier-to-2 ... Transformer-base ... concat22 ... concat21", "standard bidirectional RNN model with attention ... concat22 ... s-hier A multi-encoder architecture with hierarchical attention ... s-t-hier  ... s-hier-to-2  ... A standard context-agnostic Transformer. ... concat22 ... concat21 ... BIBREF8"], "num_words": 3922, "gpt4_answer": "The baselines are a standard bidirectional RNN model with attention (baseline) and a standard context-agnostic Transformer (baseline)."}
{"id": "1810.02268.cfdd583d01abaca923f5c466bb20e1d4b8c749ff", "question": "what context aware models were experimented?", "answer": ["a standard bidirectional RNN model with attention ... concat22  ... s-hier ... s-t-hier ... s-hier-to-2 ... concat21  ... BIBREF8 ", "bidirectional RNN ... concat22 ... s-hier ... s-t-hier ... s-hier-to-2 ... Transformer-base ... concat22 ... concat21 ... BIBREF8", "standard bidirectional RNN model with attention ... concat22 ... s-hier ... s-t-hier ... s-hier-to-2 ... standard context-agnostic Transformer ... concat22 ... concat21 ... BIBREF8"], "num_words": 3922, "gpt4_answer": "The experimented context-aware models include s-hier, s-t-hier, s-hier-to-2, their tied variants, concat22, concat21, and the Transformer model by BIBREF8."}
{"id": "1810.02268.554d798e4ce58fd30820200c474d7e796dc8ba89", "question": "what languages did they experiment on?", "answer": ["English  ... German ", "English ... German", "English ... German "], "num_words": 3922, "gpt4_answer": "They experimented on English to German translation."}
{"id": "1909.12079.91e361e85c6d3884694f3c747d61bfcef171bab0", "question": "How do they obtain the entity linking results in their model?", "answer": ["The mention is linked to the entity with the greatest commonness score.", "They use an EL algorithm that links the mention to the entity with the help of the greatest commonness score.", "we use a simple EL algorithm that directly links the mention to the entity with the greatest commonness score. Commonness BIBREF17, BIBREF18 is calculated base on the anchor links in Wikipedia. It estimates the probability of an entity given only the mention string."], "num_words": 2845, "gpt4_answer": "They obtain the entity linking results in their model by using a simple EL algorithm that directly links the mention to the entity with the greatest commonness score, which is calculated based on the anchor links in Wikipedia."}
{"id": "1909.12079.6295951fda0cfa2eb4259d544b00bc7dade7c01e", "question": "Which model architecture do they use?", "answer": ["BiLSTM", "BiLSTM with a three-layer perceptron", "BiLSTMs  ... MLP "], "num_words": 2845, "gpt4_answer": "The authors use a deep neural model that includes two layers of BiLSTMs, a three-layer MLP, and type embeddings."}
{"id": "1909.12079.3f717e6eceab0a066af65ddf782c1ebc502c28c0", "question": "Which datasets do they evaluate on?", "answer": ["FIGER (GOLD)  ... BBN", "FIGER (GOLD) ... BBN", "FIGER (GOLD) BIBREF0 ... BBN BIBREF5"], "num_words": 2845, "gpt4_answer": "They evaluate on two datasets: FIGER (GOLD) and BBN."}
{"id": "2003.11687.f5603271a04452cbdbb07697859bef2a2030d75c", "question": "How many domain experts were involved into creation of dataset?", "answer": ["1", "One domain expert.", "Unanswerable"], "num_words": 1911, "gpt4_answer": "One domain expert was involved in the creation of the dataset."}
{"id": "2003.11687.6575ffec1844e6fde5a668bce2afb16b67b65c1f", "question": "What metrics are used for evaluation?", "answer": ["F1-score", "Precision, recall, f1-score, and support.", "precision, recall, f1-score, and support"], "num_words": 1911, "gpt4_answer": "The model gives an F1-score of 0.89 for the concept recognition task."}
{"id": "2003.11687.77c3416578b52994227bae7f2529600f02183e12", "question": "What is the performance of fine tuned model on this dataset?", "answer": [" F1-score of $0.89$", "F1-score of $0.89$", "The model gives an F1-score of $0.89$ for the concept recognition task."], "num_words": 1911, "gpt4_answer": "The fine-tuned BERT model achieves an F1-score of 0.89 for the concept recognition task."}
{"id": "2003.11687.2abcff4fdedf9b17f76875cc338ba4ab8d1eccd3", "question": "Are constructed datasets open sourced?", "answer": ["Yes"], "num_words": 1911, "gpt4_answer": "Yes, the implementation, demo, and dataset are available at: https://github.com/jitinkrishnan/NASA-SE."}
{"id": "2003.11687.6df57a21ca875e63fb39adece6a9ace5bb2b2cfa", "question": "How does labeling scheme look like?", "answer": ["1. abb\n2. grp\n3. syscon\n4. opcon\n5. seterm\n6. event\n7. org\n8. art\n9. cardinal\n10. loc\n11. mea", "BIO Labelling Scheme\nabb: represents abbreviations such as TRL representing Technology Readiness Level.\n\ngrp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.\n\nsyscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.\n\nopcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.\n\nseterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.\n\nevent: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.\n\norg: represents an organization such as `NASA', `aerospace industry', etc.\n\nart: represents names of artifacts or instruments such as `AS1300'\n\ncardinal: represents numerical values such as `1', `100', 'one' etc.\n\nloc: represents location-like entities such as component facilities or centralized facility.\n\nmea: represents measures, features, or behaviors such as cost, risk, or feasibility.", "Based on operation and system concepts, the labels are abb, grp, syscon, opcon, seterm, event, org, art, cardinal, loc and mea."], "num_words": 1911, "gpt4_answer": "The labeling scheme consists of tags like abb (abbreviations), grp (group of people/individual), syscon (system concepts), opcon (operational concepts), seterm (generic SE terms), event (event-like information), org (organization), art (artifacts/instruments), cardinal (numerical values), loc (location-like entities), and mea (measures/features/behaviors)."}
{"id": "2003.11687.b39b278aa1cf2f87ad4159725dff77b387f2df84", "question": "What pretrained language model is used?", "answer": ["BERT", "BERT "], "num_words": 1911, "gpt4_answer": "BERT is used as the pre-trained language model."}
{"id": "2003.11687.814e945668e2b6f31b088918758b120fb00ada7d", "question": "How big is constructed dataset?", "answer": ["3700 sentences", "3700 sentences ", "roughly 3700 sentences at the word-token level"], "num_words": 1911, "gpt4_answer": "The constructed dataset contains roughly 3700 annotated sentences at the word-token level."}
{"id": "1703.10152.d4456e9029fcdcb6e0149dd8f57b77d16ead1bc4", "question": "What metric is considered?", "answer": ["Precision, recall and F-measure.", "precision ... recall ... F-measure", "precision, recall and F-measure"], "num_words": 2486, "gpt4_answer": "Precision, recall, and F-measure are considered as metrics for evaluating the classification performance."}
{"id": "1703.10152.d0b967bfca2039c7fb05b931c8b9955f99a468dc", "question": "What hand-crafted features are used?", "answer": [" sentences with their rhetorical status ", "Unanswerable", "position of sentence ... sentence length ... tense ... qualifying adjectives ... meta-discourse features"], "num_words": 2486, "gpt4_answer": "Teufel et al. used hand-crafted features such as the position of the sentence, sentence length, and tense for sentence classification."}
{"id": "1703.10152.31e6062ba45d8956791e1b86bad7efcb6d1b191a", "question": "What word embeddings are used?", "answer": ["INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 )", "Sentiment-Specific Word Embedding ... word2vec", "word2vec ... Sentiment-Specific Word Embedding"], "num_words": 2486, "gpt4_answer": "Word2vec embeddings are used in this paper."}
{"id": "1703.10152.38b29b0dcb87868680f9934af71ef245ebb122e4", "question": "Do they annotate their own dataset?", "answer": ["No"], "num_words": 2486, "gpt4_answer": "No, they use the Argumentative Zoning Corpus (AZ corpus) consisting of 80 AZ annotated conference articles in computational linguistics."}
{"id": "1703.10152.6e134d51a795c385d72f38f36bca4259522bcf51", "question": "How are the sentence embeddings generated?", "answer": [" average the vectors in word sequence ... training paragraph vectors ... Sentiment-Specific Word Embedding", "Averaging the vectors of the words in a sentence, directly learning paragraph vectors using PV-DM, taking average of the SSWE of the words in a sentence.", "sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors"], "num_words": 2486, "gpt4_answer": "Sentence embeddings are generated by exploring three models: averaging the vectors of the words in one sentence, paragraph vectors, and specific word vectors."}
{"id": "1703.10152.0778cbbd093f8b779f7cf26302b2a8e081ccfb40", "question": "What is argumentative zoning?", "answer": [" Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences", "a process of assigning rhetorical status to the extracted sentences", "process of assigning rhetorical status to the extracted sentences"], "num_words": 2486, "gpt4_answer": "Argumentative zoning (AZ) is a process of assigning rhetorical status to extracted sentences from scientific articles, providing readers with general discourse context from which scientific ideas can be better linked, compared, and analyzed."}
{"id": "1907.04072.578add9d3dadf86cd0876d42b03bf0114f83d0e7", "question": "How did they obtain the tweets?", "answer": ["By crawling YouLikeHits and Like4Like sites and then using Twitter's REST API", "We used Twitter's REST API", "crawled two blackmarket sites ... used Twitter's REST API"], "num_words": 2648, "gpt4_answer": "They crawled two blackmarket sites, YouLikeHits and Like4Like, between February and April 2019, and used Twitter's REST API to collect the tweet objects of the tweets posted for gaining retweets. They also collected the timelines of the authors of these tweets to find genuine tweets not posted on blackmarket sites."}
{"id": "1907.04072.4d5b74499804ea5bc5520beb88d0f9816f67205a", "question": "What baseline do they compare to?", "answer": [" spam detection method proposed by Wu et al. BIBREF4 ,  spam detection method  proposed by Rajdev et. al. BIBREF11,   feature vector by concatenating the tweet content features with Random Forest,  feature vector by concatenating the tweet content features with Multi-layer Perception and  feature vector by concatenating the tweet content features with Support Vector Machine.", "Word2Vec and Doc2Vec to encode the tweets, then MLP classifier; Random Forest classifier on a standard set of features", "Wu et al. BIBREF4 ... Rajdev et. al. BIBREF11"], "num_words": 2648, "gpt4_answer": "They compare their multitask learning-based model to state-of-the-art Twitter spam detection methods, such as the method proposed by Wu et al. (Spam Detection 1) and the approach proposed by Rajdev et al. (Spam Detection 2). Additionally, they use Random Forest (RF), Multi-layer Perceptron (MLP), and Support Vector Machine (SVM) classifiers on the combined feature vector."}
{"id": "1907.04072.baec99756b80eec7c0234a08bc2855e6770bcaeb", "question": "What language is explored in this paper?", "answer": ["English"], "num_words": 2648, "gpt4_answer": "This paper explores the English language."}
{"id": "1907.04072.46d051b8924ad0ef8cfba9c7b5b84707ee72f26a", "question": "What blackmarket services do they look at?", "answer": ["Credit-based Freemium services", "YouLikeHits and Like4Like"], "num_words": 2648, "gpt4_answer": "The authors mainly concentrate on freemium blackmarket services, specifically focusing on credit-based services, and they collected data from two blackmarket sites: YouLikeHits and Like4Like."}
{"id": "1909.10481.dae2f135e50d77867c3f57fc3cb0427b2443e126", "question": "What languages do they use during pretraining?", "answer": ["English ... Chinese ... French", "English ... French ... Chinese", "English/French/Chinese"], "num_words": 3529, "gpt4_answer": "During pre-training, they use English, Chinese, and French languages."}
{"id": "1909.10481.38055717edf833566d912f14137b92a1d9c4f65a", "question": "What is the architecture of the decoder?", "answer": ["6 transformer layers, each layer containing 1024 hidden units, 8 attention heads, and GELU activations.", "denoising auto-encoding (DAE) objective BIBREF24", "pre-trained Xnlg ... 6-layer decoder"], "num_words": 3529, "gpt4_answer": "The decoder is based on the Transformer architecture."}
{"id": "1909.10481.b6aa5665c981e3b582db4760759217e2979d5626", "question": "What is the architecture of the encoder?", "answer": ["10 transformer layers, each layer containing 1024 hidden units, 8 attentions heads, and GELU activations.", "denoising auto-encoding (DAE) objective BIBREF24", "pre-trained Xnlg with a 10-layer encoder"], "num_words": 3529, "gpt4_answer": "The encoder in Xnlg is based on the Transformer architecture."}
{"id": "1909.10481.c0355afc7871bf2e12260592873ffdb5c0c4c919", "question": "What is their baseline?", "answer": ["CorefNqg ... Mp-Gsn ... Xlm ... Pipeline (Xlm) ... Pipeline (Xlm) with Google Translator", "CorefNqg BIBREF33  ... Mp-Gsn BIBREF31 ... Xlm BIBREF5", "CorefNqg BIBREF33 ... Mp-Gsn BIBREF31 ... Xlm BIBREF5 ... Xlm Fine-tuning ... Pipeline (Xlm) ... Pipeline (Xlm) with Google Translator"], "num_words": 3529, "gpt4_answer": "The baselines for their experiments include CorefNqg, Mp-Gsn, and Xlm, which are different sequence-to-sequence models and a cross-lingual pre-training model."}
{"id": "1805.04833.afeceee343360d3fe715f405dac7760d9a6754a7", "question": "What human evaluation metrics do they look at?", "answer": ["Accuracy at pairing stories with the prompts used to generate them; accuracy of prompt ranking", "human preference", "triple pairing task ... hierarchical generation"], "num_words": 3434, "gpt4_answer": "They use human evaluation metrics such as triple pairing tasks, selecting the correct pairing for prompts and stories, and comparing stories from hierarchical generation with and without prompts."}
{"id": "1805.04833.cc3dd701f3a674618de95a4196e9c7f4c8fbf1e5", "question": "Which automated evaluation metrics are used?", "answer": ["model perplexity on the test set  ... prompt ranking accuracy", "perplexity  ... prompt ranking accuracy", "perplexity ... prompt ranking accuracy"], "num_words": 3434, "gpt4_answer": "The automated evaluation metrics used are model perplexity on the test set and prompt ranking accuracy."}
{"id": "1805.04833.d66550f65484696c1284903708b87809ea705786", "question": "What baselines do they compare against?", "answer": ["Language Models ... seq2seq ... Ensemble ... KNN", "Language Models ... seq2seq: using LSTMs and convolutional seq2seq architectures ... Conv seq2seq with decoder self-attention ... an ensemble of two Conv seq2seq with self-attention models ... KNN model", "gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism ... LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention ... an ensemble of two Conv seq2seq with self-attention models ... KNN model"], "num_words": 3434, "gpt4_answer": "The baselines they compare against are Language Models (GCNN and self-attention), seq2seq (using LSTMs and convolutional seq2seq architectures), Ensemble (of two Conv seq2seq with self-attention models), and KNN."}
{"id": "1805.04833.29ba93bcd99c2323d04d4692d3672967cca4915e", "question": "Do they use pre-trained embeddings like BERT?", "answer": ["No", "Unanswerable"], "num_words": 3434, "gpt4_answer": "No, they do not use pre-trained embeddings like BERT in this paper."}
{"id": "1805.04833.804bf5adc6dc5dd52f8079cf041ed3a710e03f8a", "question": "What model is used to generate the premise?", "answer": [" convolutional language model from BIBREF4", "convolutional language model", "convolutional language model from BIBREF4"], "num_words": 3434, "gpt4_answer": "The model used to generate the premise is a self-attentive GCNN language model."}
{"id": "1805.04833.f2dba5bf75967407cce5d0a9c2618269225081f5", "question": "Are the stories in the dataset fictional stories?", "answer": ["No", "Yes"], "num_words": 3434, "gpt4_answer": "Yes, the stories in the dataset are fictional stories."}
{"id": "1805.04833.b783ec5cb9ad595da7db2c0ddf871152ae382c5f", "question": "Where are the stories collected from?", "answer": ["Reddit's WritingPrompts forum", "online forum"], "num_words": 3434, "gpt4_answer": "The stories are collected from Reddit's WritingPrompts forum."}
{"id": "1805.07882.3eb107f35f4f5f5f527a93ffb487aa2e3fe51efd", "question": "which pretrained embeddings were experimented with?", "answer": ["word2vec  ... fastText  ... GloVe  ... Baroni  ... SL999 ", "word2vec ... fastText ... GloVe ... Baroni ... SL999"], "num_words": 3301, "gpt4_answer": "The paper experimented with five pre-trained word embeddings: word2vec, fastText, GloVe, Baroni, and SL999."}
{"id": "1805.07882.47d54a6dd50cab8dab64bfa1f9a1947a8190080c", "question": "what datasets where used?", "answer": ["SICK ... STSB ... MRPC", "STSB  ... SICK ... MRPC", "STSB, SICK, MRPC"], "num_words": 3301, "gpt4_answer": "The datasets used were STS Benchmark dataset, SICK dataset from SemEval, and Microsoft Research Paraphrase Corpus (MRPC)."}
{"id": "1805.07882.67cb001f8ca122ea859724804b41529fea5faeef", "question": "what are the state of the art methods they compare with?", "answer": ["ECNU ... HCTI", "ECNU BIBREF6 ... HCTI BIBREF5", "HCTI BIBREF5 ... InferSent BIBREF23 "], "num_words": 3301, "gpt4_answer": "The state-of-the-art methods they compare with include InferSent, HCTI, and ECNU."}
{"id": "2004.01820.42eb7c5311fc1ac0344f0b38d3184ccd4faad3be", "question": "What agreement measure is used?", "answer": ["Fleiss's Kappa", "Fleiss's Kappa "], "num_words": 6059, "gpt4_answer": "Fleiss's Kappa is used as the inter-annotator agreement measure."}
{"id": "2004.01820.8d14dd9c67d71494b4468000ff9683afdd11af7e", "question": "Do they report the annotation agreement?", "answer": ["Yes"], "num_words": 6059, "gpt4_answer": "Yes, they computed the inter-annotator agreement using Fleiss's Kappa for their dataset."}
{"id": "2004.01820.b857f3e3f1dad5df55f69d062978967fe023ac6f", "question": "How many annotators participated?", "answer": ["170", "Unanswerable", "three "], "num_words": 6059, "gpt4_answer": "170 annotators participated in labeling the dataset."}
{"id": "2004.01820.5a473f86052cf7781dfe40943ddf99bc9fe8a4e4", "question": "What social-network features are used?", "answer": ["Downward overlap, upward overlap, inward overlap, outward overlap, bidirectional overlap, count of friends of each user, count of followers of each user, users verified status, number of tweets posted within six-month snapshots", "Neighborhood Overlap ...  count the friends and followers of each user, their verified status, and the number of tweets posted within six-month snapshots of their timelines", "Relative positions of the author and target accounts in the directed following network by\ncomputing modified versions of Jaccards similarity index, friends count, followers count, verified status, number of tweets posted within 6 months."], "num_words": 6059, "gpt4_answer": "The social network features used include neighborhood overlap (downward overlap, upward overlap, inward overlap, and outward overlap), user-based features (friends and followers count, verified status, and number of tweets in six-month timelines), and timeline features (message behavior, timeline similarity, and language models)."}
{"id": "2004.01820.235c7c7ca719068136928b18e19f9661e0f72806", "question": "What are the five factors considered?", "answer": ["Aggressive language ... Repetition ... Harmful intent ... Visibility among peers ... Power imbalance"], "num_words": 6059, "gpt4_answer": "The five factors considered for cyberbullying classification are aggressive language, repetition, harmful intent, visibility among peers, and power imbalance."}
{"id": "2004.01820.c87966e7f497975b76a60f6be50c33d296a4a4e7", "question": "How is cyberbullying defined?", "answer": ["A public display of intention to inflict injury or discomfort upon a weaker victim through repeated acts of aggression.", "They define cyberbullying as aggressive language, repetition, harmful intent, visibility among peers, and power imbalance", "cyberbullying is a complex social phenomenon, different from the simpler notion of cyberaggression"], "num_words": 6059, "gpt4_answer": "Cyberbullying is defined as a complex social phenomenon involving aggression (aggr), repetition (rep), harmful intent (harm), visibility among peers (peer), and power imbalance (power) in online interactions."}
{"id": "1806.04387.c9eae337edea0edb12030a7d4b01c3a3c73c16d3", "question": "What evaluation was performed on the output?", "answer": ["For measuring the similarity of the generated texts we used Phrase Overlap match and K-gram-Jaccard similarity as our criteria ... To evaluate the quality of the generated jokes, quotes, or tweets we rely on human judgment", "Phrase Overlap match and K-gram-Jaccard similarity", "similarity of the generated texts with training data objectively ... humor content subjectively ... syntactic correctness of the generated sentences"], "num_words": 2988, "gpt4_answer": "The evaluation of the output involved measuring the similarity with training data using Phrase Overlap match and K-gram-Jaccard similarity, evaluating syntactic correctness using the Link Grammar Parser, and relying on human judgment for the quality of generated jokes, quotes, or tweets."}
{"id": "1806.04387.9f1d81b2a6fe6835042a5229690e1951b97ff671", "question": "Where did the joke data come from?", "answer": ["CrowdTruth  ... Subreddits", "CrowdTruth ... Subreddits", "CrowdTruth and Subreddits"], "num_words": 2988, "gpt4_answer": "The joke data came from two sources: CrowdTruth and Subreddits."}
{"id": "1806.04387.fae930129c2638ba6f9c9b3383e85aa130a73876", "question": "What type of quotes is this system trying to generate?", "answer": ["Unanswerable", "inspirational"], "num_words": 2988, "gpt4_answer": "The system is trying to generate inspirational quotes."}
{"id": "1808.04122.1acfbdc34669cf19a778aceca941543f11b9a861", "question": "What size filters do they use in the convolution layer?", "answer": ["1x3", "1x3 filter size is used in convolutional layers.", "Unanswerable"], "num_words": 4347, "gpt4_answer": "The paper does not explicitly mention the size of the filters used in the convolution layer."}
{"id": "1808.04122.864295caceb1e15144c1746ab5671d085d7ff7a1", "question": "By how much do they outperform state-of-the-art models on knowledge graph completion?", "answer": [" improvements of INLINEFORM0 in MRR (which is about 25.1% relative improvement) ... INLINEFORM1 % absolute improvement in Hits@10", "0.105 in MRR and 6.1 percent points in Hits@10 on FB15k-237", "On FB15k-237 dataset it outperforms 0.105 in MRR and  6.1% absolute improvement in Hits@10"], "num_words": 4347, "gpt4_answer": "CapsE outperforms other state-of-the-art models on two benchmark datasets WN18RR and FB15k-237 for knowledge graph completion, with significant improvements of about 25.1% relative improvement in MRR and around 4% absolute improvement in Hits@10 on FB15k-237."}
{"id": "1907.05338.79e61134a6e29141cd19252571ffc92a0b4bc97f", "question": "did they test with other pretrained models besides bert?", "answer": ["No"], "num_words": 3325, "gpt4_answer": "No, the experiments in this paper focus on using BERT as the pre-trained language model."}
{"id": "1907.05338.18fbfb1f88c5487f739aceffd23210a7d4057145", "question": "what models did they compare with?", "answer": ["BERT ... BERT adding a Bi-LSTM on top ... DenseNet BIBREF33 and HighwayLSTM BIBREF34 ... BERT+ BIMPM ... remove the first bi-LSTM of BIMPM ... Sim-Transformer", "BERT, BERT + Bi-LSTM, BERT + HighwayLSTM, BERT + DenseNet, Ensembled Model, BERT + BIMPM, BERT + Sim-Transformer", "BERT, BERT+ Bi-LSTM ,  BERT+ DenseNet, BERT+HighwayLSTM,   Ensembled model, BERT+ BIMPM, BERT+ BIMPM(first bi-LSTM removed),  BERT + Sim-Transformer ."], "num_words": 3325, "gpt4_answer": "They compared the strategy stack-and-finetune and strategy finetune-only in three NLP tasks: sequence labeling, text classification, and question similarity."}
{"id": "1907.05338.5d3e87937ecebf0695bece08eccefb2f88ad4a0f", "question": "what datasets were used for testing?", "answer": ["CoNLL03  ... Yahoo Answer Classification Dataset ... Quora-Question-Pair dataset 1", "CoNLL03 ...  Yahoo Answer Classification Dataset ... Quora-Question-Pair dataset 1", "CoNLL03 dataset BIBREF5 ... Yahoo Answer Classification Dataset ...  Quora-Question-Pair dataset"], "num_words": 3325, "gpt4_answer": "The datasets used for testing were CoNLL03 for named entity recognition, Yahoo Answer Classification Dataset for text classification, and Quora-Question-Pair dataset for semantic similarity tasks."}
{"id": "2003.08437.7d539258b948cd5b5ad1230a15e4b739f29ed947", "question": "What inter-annotator agreement did they obtain?", "answer": [" two inter-annotator agreement  ... aw agreement and Cohen's kappa across three annotators computed by averaging three pairwise comparisons", "Raw agreement is around .90 for this dataset.", "The average agreement on scene, function and construal is 0.915"], "num_words": 3627, "gpt4_answer": "The inter-annotator agreement levels on scene role, function, and full construal are high for both phases of the annotation process."}
{"id": "2003.08437.9c1f70affc87024b4280f0876839309b8dddd579", "question": "How did they annotate the corpus?", "answer": ["The corpus is jointly annotated by three native Mandarin Chinese speakers ... Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication ... Annotation was conducted in two phases", "The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.", "Tokenization ... Adposition Targets ... Data Format ... Reliability of Annotation"], "num_words": 3627, "gpt4_answer": "The corpus was annotated by three native Mandarin Chinese speakers with advanced training in theoretical and computational linguistics. They performed cooperative annotation for 25% of the adposition targets and independent annotation for the remainder, followed by cooperative adjudication."}
{"id": "2003.08437.2694a679a703ccd6139897e4d9ff8e053dabd0f2", "question": "What is the size of the corpus?", "answer": ["20287", "933 manually identified adpositions"], "num_words": 3627, "gpt4_answer": "The corpus contains 933 manually identified adpositions with a total of 20k tokens."}
{"id": "2003.04978.65c9aee2051ff7c47112b2aee0d928d9b6a8c2fe", "question": "Which datasets do they use?", "answer": ["Unanswerable", "https://github.com/Sairamvinay/Fake-News-Dataset\n\n"], "num_words": 5539, "gpt4_answer": "The training dataset used in this paper has five features: ID, title, author, text, and label. It contains 20800 odd number of samples. The test dataset is not used in this study."}
{"id": "2003.04978.f8264609a44f059b74168995ffee150182a0c14f", "question": "What models are explored in this paper?", "answer": ["Artificial Neural Network (ANN) ... Long Short Term Memory networks (LSTMs) ...  Random Forest ... Logistic Regression ...  Support Vector Machine (SVM)", "SVM ... Logistic Regression ... ANN ... LSTM ... Random Forest ... TFIDF ... CV ... W2V", "SVM, Logistic Regression, ANN, LSTM, and Random Forest"], "num_words": 5539, "gpt4_answer": "This paper explores models such as convolutional neural networks (CNN), recurrent neural networks (RNN), feed-forward neural networks (FFNN), long-short term memory (LSTM), logistic regression, random forest, artificial neural networks (ANN), and support vector machines (SVM)."}
{"id": "1809.08935.c728fe6137f114c02e921f9be4a02a5bd83ae787", "question": "what features of the essays are extracted?", "answer": ["Following groups of features are extracted:\n- Numerical Features\n- Language Models\n- Clusters\n- Latent Dirichlet Allocation\n- Part-Of-Speech\n- Bag-of-words", "Numerical features, Language Models, Clusters, Latent Dirichlet Allocation, Part-Of-Speech tags, Bag-of-words", "Numerical features, language models features, clusters, latent Dirichlet allocation, Part-of-Speech tags, Bag-of-words."], "num_words": 974, "gpt4_answer": "The extracted features are partitioned into six groups, including readability and syntactic complexity scores and topic models."}
{"id": "1809.08935.50bda708293532f07a3193aaea0519d433fcc040", "question": "what were the evaluation metrics?", "answer": ["Accuracy", "Accuracy metric", "accuracy"], "num_words": 974, "gpt4_answer": "The evaluation metric used was a cost measure that uses the confusion matrix of the prediction and prior knowledge to evaluate the system's performance."}
{"id": "1809.08935.46e660becd727c994a2a35c6587e15ea8bf8272d", "question": "what model is used?", "answer": ["Light Gradient Boosting Machine", "gradient boosted trees"], "num_words": 974, "gpt4_answer": "Gradient Boosted Trees are used as the model."}
{"id": "1809.08935.d1a4529ea32aaab5ca3b9d9ae5c16f146c23af6b", "question": "what future work is described?", "answer": ["Investigate the effectiveness of LDA to capture the subject of the essay.", "investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used", "the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not"], "num_words": 974, "gpt4_answer": "Future work includes investigating potential biases in the process, such as whether LDA's effectiveness is due to its expressiveness and modeling power or an artifact of the dataset, and further analysis and validation if essay topics are released for validation splits based on these topics."}
{"id": "1809.08935.7fba61426737394304e307cdc7537225f6253150", "question": "what was the baseline?", "answer": ["Unanswerable"], "num_words": 974, "gpt4_answer": "Unanswerable"}
{"id": "1910.07924.46aa61557c8d20b1223a30366a0704d7af68bbbe", "question": "How is the sentence alignment quality evaluated?", "answer": ["5-point scale used in KocabiyikogluETAL:18", "The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18:\n\nWrong alignment\n\nPartial alignment with slightly compositional translational equivalence\n\nPartial alignment with compositional translation and additional or missing information\n\nCorrect alignment with compositional translation and few additional or missing information\n\nCorrect alignment and fully compositional translation\n\nThe evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.\n\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.", "Through human evaluation on a 5-point scale for text alignment and 3-point scale for audio-text"], "num_words": 2157, "gpt4_answer": "The sentence alignment quality is evaluated according to a 5-point scale, ranging from wrong alignment to correct alignment and fully compositional translation."}
{"id": "1910.07924.b3b9d7c8722e8ec41cbbae40e68458485a5ba25c", "question": "How is the speech alignment quality evaluated?", "answer": ["The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.\n\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.", "Through a 3-point scale by annotators.", "Wrong alignment ... Partial alignment, some words or sentences may be missing ... Correct alignment, allowing non-spoken syllables at start or end."], "num_words": 2157, "gpt4_answer": "The speech alignment quality was evaluated according to a 3-point scale: 1) Wrong alignment, 2) Partial alignment with some words or sentences missing, and 3) Correct alignment, allowing non-spoken syllables at the start or end."}
{"id": "1911.11899.b569827ecd04ae8757dc3c9523ab97e3f47a6e00", "question": "Is their gating mechanism specially designed to handle one sentence bags?", "answer": ["No", "Unanswerable", "Yes"], "num_words": 4240, "gpt4_answer": "Yes, the proposed gating mechanism is designed to handle one-sentence bags and address the issues in the selective attention framework."}
{"id": "1911.11899.0d42bd759c84cbf3a293ab58283a3d0d5e27d290", "question": "Do they show examples where only one sentence appears in a bag and their method works, as opposed to using selective attention?", "answer": ["Yes"], "num_words": 4240, "gpt4_answer": "Yes, the authors conduct case studies in Table TABREF38, which include examples where only one sentence appears in a bag, and their proposed SeG framework works better than using selective attention."}
{"id": "1911.11899.9f1e60ee86a5c46abe75b67ef369bf92a5090568", "question": "By how much do they outperform previous state-of-the-art in terms of top-n precision?", "answer": ["5.3 percent points", "Compared to previous state-of-the-art approaches (i.e., PCNN+HATT and PCNN+BAG-ATT), the proposed model can also outperform them by a large margin, i.e., 10.3% and 5.3%", "Outperforms PCNN+HATT by 10.3% and PCNN+BAG-ATT by 5.3%"], "num_words": 4240, "gpt4_answer": "Our proposed SeG outperforms previous state-of-the-art approaches by 10.3% in terms of top-N precision."}
{"id": "1603.09405.4dc4180127761e987c1043d5f8b94512bbe74d4f", "question": "By how much do they outperform existing methods?", "answer": ["Best proposed result had 0.851 and  0.842 compared to best previous result of 0.828 and 0.846 on person correlation and accuracy respectively.", "In Semantic Relatedness task their model outperforms existing methods by more than 0.023 Pearson Correlation. In Textual Entailment task their model scores 0.004 accuracy lesser than MaxEnt", "Their best implementation for semantic relatedness task comparison outperforms standard MaxEnt by 0,052 Pearson  Correlation.\nTheir best implementation for Textual Entailment task comparison (84,2 accuracy) DOES NOT outperform standard SVM (84,6 accuracy).\n"], "num_words": 3616, "gpt4_answer": "Our approach consistently outperforms existing methods on standard evaluation datasets."}
{"id": "1603.09405.420862798054f736128a6f0c4393c7f9cc648b40", "question": "Which datasets do they evaluate on?", "answer": ["SICK (Sentences Involving Compositional Knowledge) dataset", "SICK (Sentences Involving Compositional Knowledge) dataset "], "num_words": 3616, "gpt4_answer": "They evaluate on the standard SICK (Sentences Involving Compositional Knowledge) dataset."}
{"id": "1603.09405.ad8411edf11d3429c9bdd08b3e07ee671464d73c", "question": "Do they separately evaluate performance of their learned representations (before forwarding them to the CNN layer)?", "answer": ["No", "Yes"], "num_words": 3616, "gpt4_answer": "No, the paper does not separately evaluate the performance of their learned representations before forwarding them to the CNN layer."}
{"id": "1912.11585.11360385dff0a9d7b8f4b106ba2b7fe15ca90d7c", "question": "What was the baseline?", "answer": ["Unanswerable"], "num_words": 1355, "gpt4_answer": "Unanswerable"}
{"id": "1912.11585.875fbf4e5f93c3da63e28a233ce1d8405c7dfe63", "question": "What dataset was used in this challenge?", "answer": ["SRE04/05/06/08/10/MIXER6\nLDC98S75/LDC99S79/LDC2002S06/LDC2001S13/LDC2004S07\nVoxceleb 1/2\nFisher + Switchboard I\nCallhome+Callfriend", "SRE18 development and SRE18 evaluation datasets", "SRE19"], "num_words": 1355, "gpt4_answer": "The NIST 2019 speaker recognition evaluation (SRE) CTS challenge was used in this paper."}
{"id": "1912.11585.56b66d19dbc5e605788166e168f36d25f5beb774", "question": "Which subsystem outperformed the others?", "answer": ["eftdnn", "eftdnn ", "primary system is the linear fusion of all the above six subsystems"], "num_words": 1355, "gpt4_answer": "Unanswerable"}
{"id": "1707.09816.2d924e888a92dc0b14cdb5584e73e87254c3d1ee", "question": "Do they reduce language variation of text by enhancing frequencies?", "answer": ["Unanswerable", "Variation decreases when frequencies of synonyms is enhanced; variation increases when frequencies of synonyms, hyponyms, hypernyms are enhanced"], "num_words": 3428, "gpt4_answer": "Yes, the approach enhances frequencies of semantically related words and phrases when they co-occur in the same text, leading to their larger contribution to the topics found in the text."}
{"id": "1707.09816.3ed8ac1ba4df6609fa7de5077d83e820641edc5e", "question": "Which domains do they explore?", "answer": [" news articles related to Islam and articles discussing Islam basics", "economic ... political"], "num_words": 3428, "gpt4_answer": "The domains explored in the experiments include English text collections, European Union collections, Russian banking collection, and an Islam informational portal."}
{"id": "1707.09816.e1ab241059ef1700738f885f051d724a7fcf283a", "question": "Which thesauri did they use?", "answer": ["WordNet  ... EuroVoc  ... RuThes ", "WordNet ... EuroVoc ...  RuThes", "WordNet ... European Union EuroVoc ... RuThes"], "num_words": 3428, "gpt4_answer": "They used three thesauri: WordNet, EuroVoc, and RuThes."}
{"id": "1703.04009.a4b77a20e067789691e0ab246bc5b11913d77ae1", "question": "What is their definition of hate speech?", "answer": ["language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group"], "num_words": 3079, "gpt4_answer": "The authors define hate speech as language that expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. In extreme cases, it may also include language that threatens or incites violence."}
{"id": "1703.04009.ba39317e918b4386765f88e8c8ae99f9a098c935", "question": "What type of model do they train?", "answer": ["logistic regression ... nave Bayes ... decision trees ... random forests ... linear SVM", "logistic regression ... nave Bayes ... decision trees ... random forests ... linear SVMs", "logistic regression, nave Bayes, decision trees, random forests, and linear SVMs"], "num_words": 3079, "gpt4_answer": "They train a logistic regression model with L2 regularization for the final model."}
{"id": "1703.04009.22c125c461f565f5437dac74bf19c2ef317bad86", "question": "How many users does their dataset have?", "answer": ["33,458", "33,458 Twitter users are orginally used, but than random sample of tweets is extracted resulting in smaller number or users in final dataset.", "33458"], "num_words": 3079, "gpt4_answer": "The dataset contains tweets from 33,458 Twitter users."}
{"id": "1703.04009.4a91432abe3f54fcbdd00bb85dc0df95b16edf42", "question": "How long is their dataset?", "answer": ["24,802 ", "24,802 labeled tweets", "85400000"], "num_words": 3079, "gpt4_answer": "The dataset consists of 24,802 labeled tweets."}
{"id": "1911.03090.7c398615141ca416a32c9f72dbb785d3a6986a0f", "question": "In what tasks does fine-tuning all layers hurt performance?", "answer": ["SST-2", "Unanswerable"], "num_words": 1711, "gpt4_answer": "In the large variants of BERT and RoBERTa on the SST-2 (sentiment classification) dataset, fine-tuning all layers does not always help and may hurt performance."}
{"id": "1911.03090.441be93e2830cc0fc65afad6959db92754c9f5a8", "question": "Do they test against the large version of RoBERTa?", "answer": ["For GLUE bencmark no, for dataset MRPC, SST-B, SST-2 and COLA yes.", "Yes"], "num_words": 1711, "gpt4_answer": "Yes, they test against the large version of RoBERTa."}
{"id": "1909.04242.7f11f128fd39b8060f5810fa84102f000d94ea33", "question": "What is the performance improvement of their method over state-of-the-art models on the used datasets? ", "answer": ["Average improvement in accuracy is 2.26 points", "Unanswerable"], "num_words": 2495, "gpt4_answer": "Unanswerable"}
{"id": "1909.04242.2a55076a66795793d79a3edfae1041098404fbc3", "question": "Could the proposed training framework be applied to other NLP problems?", "answer": ["Unanswerable"], "num_words": 2495, "gpt4_answer": "Yes, the proposed training framework could potentially be applied to other NLP problems as it aims to mitigate annotation artifacts and improve generalization ability."}
{"id": "1909.04242.ecaa10a2d9927fa6ab6a954488f12aa6b42ddc1a", "question": "How does the proposed training framework mitigate the bias pattern?", "answer": ["Artifacts in biased datasets are balanced by assigning specific weights for every sample", "Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading", "by balancing or, smoothing the artifacts across different classes by assigning specific weights for every sample"], "num_words": 2495, "gpt4_answer": "The proposed training framework mitigates the bias pattern by assigning specific weights to every sample, making artifacts in biased datasets balanced across different classes, and using cross predictions to estimate P(y|h) in the original datasets."}
{"id": "1909.04242.8b49423b7d1fa834128aa5038aa16c6ef3fdfa32", "question": "Which datasets do they use in the cross-dataset evaluation?", "answer": ["SNLI ... MultiNLI ... JOCI ... SICK", "SNLI BIBREF0 ... MultiNLI BIBREF1 ... JOCI BIBREF13 ... SICK BIBREF14"], "num_words": 2495, "gpt4_answer": "They use SNLI, MultiNLI, JOCI, and SICK for cross-dataset evaluation."}
{"id": "2003.12139.0aca0a208a1e28857fab44e397dc7880e010dbca", "question": "Which was the most helpful strategy?", "answer": ["Entropy algorithm is the best way to build machine learning models. Vote entropy and KL divergence are helpful for the training of machine learning ensemble classifiers.", "Vote entropy and KL divergence ...  all the active learning strategies we tested do not work well with deep learning model", "entropy"], "num_words": 3298, "gpt4_answer": "The most helpful strategy for building machine learning models fast and efficiently was the entropy algorithm."}
{"id": "2003.12139.471683ba6251b631f38a24d42b6dba6f52dee429", "question": "How large is their tweets dataset?", "answer": ["3,685,984 unique tweets"], "num_words": 3298, "gpt4_answer": "The dataset consists of 3,685,984 unique tweets."}
{"id": "1809.03391.5dfd58f91e7740899c23ebfe04b7176edce9ead2", "question": "what is the size of the idn tagged corpus?", "answer": ["10K", "10K sentences ... 250K tokens", "10K sentences and 250K tokens"], "num_words": 2673, "gpt4_answer": "The IDN Tagged Corpus consists of 10K sentences and 250K tokens."}
{"id": "1809.03391.c09bceea67273c10a0621da1a83b409f53342fd9", "question": "what neural network models were explored?", "answer": ["Feedforward, biLSTM", "feedforward ... bidirectional LSTM (biLSTM)", "feedforward network  ... bidirectional LSTM"], "num_words": 2673, "gpt4_answer": "The paper explored various neural network models for Indonesian POS tagging, including feedforward networks, bidirectional LSTMs (biLSTM), and recurrent neural networks with CRF layers."}
{"id": "1809.03391.732bd97ae34541f215c436e2a1b98db1649cba27", "question": "what rule based models were evaluated?", "answer": ["Rashel et al. BIBREF14", "rule-based tagger designed by Rashel et al. BIBREF14"], "num_words": 2673, "gpt4_answer": "The rule-based tagger designed by Rashel et al. was evaluated as one of the comparisons."}
{"id": "1809.03391.183b385fb59ff1e3f658d4555a08b67c005a8734", "question": "what datasets have been used for this task?", "answer": [" IDN Tagged Corpus", "IDN Tagged Corpus", "IDN Tagged Corpus "], "num_words": 2673, "gpt4_answer": "The IDN Tagged Corpus and the PANL10N dataset have been used for this task."}
{"id": "1906.04287.5f7f4a1d4380c118a58ed506c057d3b7aa234c1e", "question": "How much data do they use to train the embeddings?", "answer": ["11,529,432 segmented words", "11,529,432 segmented words and 20,402 characters"], "num_words": 1947, "gpt4_answer": "They use 11,529,432 segmented words from Chinese Wikipedia articles for training the embeddings."}
{"id": "1906.04287.a79a23573d74ec62cbed5d5457a51419a66f6296", "question": "Do they evaluate their embeddings in any downstream task appart from word similarity and word analogy?", "answer": ["No"], "num_words": 1947, "gpt4_answer": "No, they only evaluate their embeddings on word similarity and word analogy tasks."}
{"id": "1906.04287.d427e9d181434078c78b7ee33a26b269f160f6d2", "question": "What dialects of Chinese are explored?", "answer": ["Unanswerable"], "num_words": 1947, "gpt4_answer": "Unanswerable"}
{"id": "1912.10162.0a5fd0e5f4ab12be57be20416a5ea7c3db5fb662", "question": "What are the issues identified for out-of-vocabulary words?", "answer": ["Also, the model with the dataset vectors did not have the flexibility to classify unknown words. ... the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results", "for unknown words the model assigned a zero vector", "model did not have a flexibility in OOV words ... One disadvantage that the previous model had is that for unknown words the model assigned a zero vector ... It was noticed that the model performed better when using the vectors from different FastText models"], "num_words": 2700, "gpt4_answer": "The issues identified for out-of-vocabulary words are the lack of flexibility in classifying unknown words and the model assigning a zero vector to unknown words, which affects the testing results."}
{"id": "1912.10162.5d03a82a70f7b1ab9829891403ec31607828cbd5", "question": "Is the morphology detection task evaluated?", "answer": ["Yes"], "num_words": 2700, "gpt4_answer": "Yes, the morphology detection task is evaluated in the experiments conducted in the paper."}
{"id": "1912.10162.6cad6f074b0486210ffa4982c8d1632f5aa91d91", "question": "How does the model proposed extend ENAMEX?", "answer": ["Extended with facility (FAC) type.", "SpaCy is an open source, Natural Language Processing library that supports a variety of tasks, including POS Tagging, Named Entity Recognition, Dependency Parsing, etc. SpaCy uses sophisticated neural network-based models", "The keyword list had at this point about 350,000 records and consisted of 4 classes: location (LOC), organization (ORG), person (PERSON) and facility (FAC)"], "num_words": 2700, "gpt4_answer": "The model extends the standard ENAMEX type (organization, location, person) for named entity recognition in the Greek language using spaCy."}
{"id": "1912.10162.d38b3e0896b105d171e69ce34c689e4a7e934522", "question": "Which morphological features are extracted?", "answer": ["Information about the dataset includes the tokens of a set of articles and their position in a sentence, the lemma and the part of speech of every token ... The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case", "The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers", "like the gender, the number, and the case"], "num_words": 2700, "gpt4_answer": "The morphological features extracted include gender, number, and case."}
{"id": "1909.13184.4379a3ece3fdb93b71db43f62833f5f724c49842", "question": "Do the authors report results on only English datasets?", "answer": ["Unanswerable", "Yes"], "num_words": 2125, "gpt4_answer": "Unanswerable"}
{"id": "1909.13184.0abc2499195185c94837e0340d00cd3b83ee795e", "question": "What are the characteristics of the dataset of Twitter users?", "answer": ["413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\"", "Tweet Diversity ... URL score ... Mean Daily Posts ... Topics ... Mean Post Length ... Profile Picture", "a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter ... Two professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites ... Users were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information ...  Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation"], "num_words": 2125, "gpt4_answer": "The dataset consists of 10,417 users from a database containing more than 400 million publicly available tweets posted by over 100,000 users who have announced their pregnancy on Twitter. Users were manually categorized as \"bot,\" \"non-bot,\" or \"unavailable.\" In the final dataset, 413 (4%) users were annotated as \"bot,\" 7849 (75.35%) as \"non-bot,\" and 2155 (20.69%) as \"unavailable.\""}
{"id": "1909.13184.138ad61b43c85d5db166ea9bd3d3b19bb2e2bbfb", "question": "How can an existing bot detection system by customized for health-related research?", "answer": ["An existing bot detection score for each user  can be used as a feature in training", "Using a machine learning algorithm on top of an existing bot detection system, and a set of simple derived features, we were able to significantly improve bot detection performance in health-related data. ", "simple derived features, we were able to significantly improve bot detection performance in health-related data"], "num_words": 2125, "gpt4_answer": "An existing bot detection system can be customized for health-related research by incorporating additional features and a statistical machine learning classifier, such as tweet diversity, URL score, mean daily posts, topics, mean post length, profile picture, and user name, to significantly improve bot detection performance."}
{"id": "1909.13184.7e906dc00e92088a25df3719104d1750e5a27485", "question": "What type of health-related research takes place in social media?", "answer": [" drug reaction detection ... syndromic surveillance ... subject recruitment for cancer trials ... characterizing drug abuse", "Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth. ...  Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. ", "almost exclusively on population-level studies ... very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women"], "num_words": 2125, "gpt4_answer": "Health-related research in social media includes tasks such as adverse drug reaction detection, syndromic surveillance, subject recruitment for cancer trials, and characterizing drug abuse."}
{"id": "1910.13793.0d9241e904bd2bbf5b9a6ed7b5fc929651d3e28e", "question": "Do the QA tuples fall under a specific domain?", "answer": ["No", "conversations, which consist of at least one question and one free-form answer"], "num_words": 2705, "gpt4_answer": "The QA tuples are sourced from Twitter and focus on customer support conversations."}
{"id": "1910.13793.95646d0ac798dcfc15b43fa97a1908df9f7b9681", "question": "What is the baseline model?", "answer": ["pre-trained version of BERT without special emoji tokens"], "num_words": 2705, "gpt4_answer": "The baseline model is the pre-trained version of BERT without special emoji tokens."}
{"id": "1910.13793.12dc04e0ec1d3ba5ec543069fe457dfa4a1cac07", "question": "How large is the corpus of QA tuples?", "answer": ["2000 tuples"], "num_words": 2705, "gpt4_answer": "The corpus of QA tuples consists of 2000 tuples."}
{"id": "1910.13793.647f6e6b168ec38fcdb737d3b276f78402282f9d", "question": "What corpus did they use?", "answer": [" customer support dataset with a relatively high usage of emoji", "2000 tuples collected by BIBREF24 that are sourced from Twitter", "a customer support dataset"], "num_words": 2705, "gpt4_answer": "They used a customer support dataset with 2000 tuples collected by BIBREF24, sourced from Twitter."}
{"id": "1906.11085.04796aaa59eeb2176339c0651838670fd916074d", "question": "what boosting techniques were used?", "answer": ["Light Gradient Boosting Machine", "Light Gradient Boosting Machine (LGBM)"], "num_words": 2090, "gpt4_answer": "The boosting technique used was Light Gradient Boosting Machine (LGBM) algorithm, which is an implementation of fast gradient boosting on decision trees."}
{"id": "1906.11085.ebb33f3871b8c2ffd2c451bc06480263b8e870e0", "question": "did they experiment with other text embeddings?", "answer": ["No", "Yes"], "num_words": 2090, "gpt4_answer": "Yes, they experimented with the original BERT embedding and BioBERT embedding, which was trained on biomedical corpora."}
{"id": "1906.11085.afd1c482c311e25fc42b9dd59cdc32ac542f5752", "question": "what is the size of this improved dataset?", "answer": ["363,078", "363,078 structured abstracts", "Unanswerable"], "num_words": 2090, "gpt4_answer": "The size of the improved dataset (PICONET) is not explicitly mentioned in the paper."}
{"id": "1906.11085.ae1c4f9e33d0cd64d9a313c318ad635620303cdd", "question": "how was the new dataset collected?", "answer": ["By searching for structured abstracts on PubMed using specific filters.", "The new dataset was collected from structured abstracts from PubMed and filtering abstract headings representative of the desired categories.", "collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories"], "num_words": 2090, "gpt4_answer": "The new dataset, PICONET, was collected by gathering structured abstracts from PubMed, carefully choosing abstract headings representative of the desired categories (Population/Problem, Intervention, and Outcome), and performing automatic labeling while removing as much ambiguity as possible."}
{"id": "1906.11085.7066f33c373115b1ead905fe70a1e966f77ebeee", "question": "who annotated the new dataset?", "answer": ["The P, I, and O labels were automatically assigned after clustering lemmatized labels from the structured abstract sections.", "Unanswerable", "automatic labeling ... lemmatization of the abstract section labels in order to cluster similar categories ... manually looked at a small number of samples for each label to determine if text was representative"], "num_words": 2090, "gpt4_answer": "The paper does not explicitly mention who annotated the new dataset."}
{"id": "1906.11085.018b81f810a39b3f437a85573d24531efccd835f", "question": "what shortcomings of previous datasets are mentioned?", "answer": ["In the previous dataset a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset.", "Information about the intervention and study design is mistakenly marked by a P label; a P-labeled section that contained more than one sentence would be split into multiple P-labeled sentences.", "using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label. ... Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset."], "num_words": 2090, "gpt4_answer": "The shortcomings of previous datasets include redundancy, ambiguity, and the use of a keyword-based approach that does not isolate specific PIO sequences."}
{"id": "1908.09892.e2c8d7f3ef5913582503e50244ca7158d0a62c42", "question": "Do single-language BERT outperforms multilingual BERT?", "answer": ["For some language yes, but not for another.", "No", "Unanswerable"], "num_words": 2688, "gpt4_answer": "Unanswerable"}
{"id": "1908.09892.654fe0109502f2ed2dc8dad359dbbce4393e03dc", "question": "What types of agreement relations do they explore?", "answer": ["Subject-verb agreement,  noun-determiner agreement,  noun -attributive adjective agreement and noun-predicate adjective agreement.", "The subject and verb in (UNKREF3) agree for number, while the noun and determiner in (UNKREF4), the noun and attributive adjective in (UNKREF5) and the subject and predicated adjective in (UNKREF6) agree for both number and gender.", "subject-verb ... noun-determiner ... noun-attributive adjective ... subject-predicate adjective"], "num_words": 2688, "gpt4_answer": "They explore four types of agreement relations: subject-verb, noun-determiner, noun-attributive adjective, and subject-predicate adjective."}
{"id": "2004.01970.da21bcaa8e3a9eadc8a5194fd57ae797e93c3049", "question": "what text classification datasets do they evaluate on?", "answer": ["Amazon ... Yelp ... IMDB  ... MR  ... MPQA  ... Subj ... TREC", "Amazon ... Yelp ... IMDB ... MR ... MPQA ... Subj ... TREC", "Amazon, Yelp, IMDB ... MR BIBREF16 ... MPQA BIBREF17 ... Subj BIBREF18 ... TREC BIBREF19"], "num_words": 1941, "gpt4_answer": "They evaluate on Amazon, Yelp, IMDB, MR, MPQA, Subj, and TREC text classification datasets."}
{"id": "2004.01970.363a24ecb8ab45215134935e7e8165fff72ff90f", "question": "which models is their approach compared to?", "answer": ["TextFooler", "word-LSTM BIBREF20 ... word-CNN BIBREF21 ...  fine-tuned BERT BIBREF12 base-uncased ", "word-LSTM BIBREF20, word-CNN BIBREF21 and a fine-tuned BERT BIBREF12 base-uncased classifier"], "num_words": 1941, "gpt4_answer": "BAE is compared to TextFooler, word-LSTM, word-CNN, and a fine-tuned BERT base-uncased classifier."}
{"id": "1806.03369.74396ead9f88a9efc7626240ce128582ab69ef2b", "question": "by how much did their approach outperform previous work?", "answer": ["By 0,008 F1,  0, 02 Recall and 0,02 Precision.", "New best result is F1 score of 0.752 compared to 0.744 of the best previous work.", "by 0.008 in terms of F1 score,  and 0.02 in terms of recall and precision "], "num_words": 3253, "gpt4_answer": "Their approach outperformed previous work by reducing the error rate by 14% relative to prior best results on the Amazon dataset."}
{"id": "1806.03369.8a7a9d205014c42cb0e24a0f3f38de2176fe74c0", "question": "what was the previous best results model?", "answer": [" BIBREF12 buschmeier-cimiano-klinger:2014:W14-26", " F1 (0.744)", "logistic regression classifier"], "num_words": 3253, "gpt4_answer": "The previous best results model was reported by Buschmeier et al., which achieved an F1 of 0.744 using a logistic regression classifier on the Amazon dataset."}
{"id": "1806.03369.eaed0b721cc3137b964f5265c7ecf76f565053e9", "question": "what are the baseline models?", "answer": ["All Sarcasm ... Random case", "All Sarcasm case assumes that every instance is sarcastic ...  Random case randomly assigns each instance as sarcastic or non-sarcastic", "the All Sarcasm case ... the Random case"], "num_words": 3253, "gpt4_answer": "The baseline models are the \"All Sarcasm\" case, which assumes that every instance is sarcastic, and the \"Random\" case, which randomly assigns each instance as sarcastic or non-sarcastic."}
{"id": "1806.03369.ba7fea78b0b888a714cb7d89944b69c5038a1ef1", "question": "what domains are explored?", "answer": ["Data was taken from two domains: Twitter, and Amazon product reviews. ", "Twitter ... Amazon ", "Twitter, and Amazon product reviews"], "num_words": 3253, "gpt4_answer": "The domains explored in this paper are Twitter and Amazon product reviews."}
{"id": "1806.03369.38af3f25c36c3725a31304ab96e2c200c55792b4", "question": "what training data was used?", "answer": ["Twitter dataset ...  Amazon product reviews", "Twitter product reviews containing exactly one of the trailing hashtags #sarcasm, #happiness, #sadness, #anger, #surprise, #fear, and #disgust, and Amazon product reviews from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661.", "Twitter, and Amazon product reviews"], "num_words": 3253, "gpt4_answer": "The training data used was taken from two domains: Twitter and Amazon product reviews."}
{"id": "2003.07459.9465d96a1368299fd3662d91aa94ba85347b4ccd", "question": "What is the performance of the best model?", "answer": ["0.89 F1 score", "F1 Macro of 0.89", "LSTMs and GRU with attention which achieved 0.89 F1 score"], "num_words": 3557, "gpt4_answer": "The best model, LSTM and GRU with Attention, achieved an F1-macro score of 0.89."}
{"id": "2003.07459.e8c3f59313df20db0cdd49b84a37c44da849fe17", "question": "What are the models tested on the dataset?", "answer": ["Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF) ... The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning ... The final classifier was two models based on the Bayes theorem: Multinomial Nave Bayes, which works with occurrence counts, and Bernoulli Nave Bayes, which is designed for binary features ... Pooled GRU ...  Stacked LSTM with Attention ... LSTM and GRU with Attention ... 2D Convolution with Pooling ... GRU with Capsule ... LSTM with Capsule and Attention ... BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF) ... linear classifier with Stochastic Gradient Descent (SGDC) learning ... Multinomial Nave Bayes ... Bernoulli Nave Bayes ... Pooled GRU  ... Stacked LSTM with Attention  ... LSTM and GRU with Attention ... 2D Convolution with Pooling  ... GRU with Capsule ...  LSTM with Capsule and Attention ... BERT", "linear SVM, RBF SVM, linear classifier with SGDC, multinomial naive bayes, bernoulli naive bayes, pooled GRU, stacked LSTM with attention, LSTM and GRU with attention, 2d convolution with pooling, GRU with Capsule, LSTM with Capsule and attention, and BERT"], "num_words": 3557, "gpt4_answer": "The models tested on the dataset include Linear SVM, RBF SVM, Stochastic Gradient Descent (SGDC), Multinomial Nave Bayes, Bernoulli Nave Bayes, Pooled GRU, Stacked LSTM with Attention, LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, and BERT."}
{"id": "2003.07459.f61268905626c0b2a715282478a5e373adda516c", "question": "Which method best performs on the offensive language identification task?", "answer": [" a system using LSTMs and GRU with attention", "LSTM and GRU with Attention can be considered as the best model trained for OGTD", "LSTMs and GRU with attention"], "num_words": 3557, "gpt4_answer": "The best method for offensive language identification in the study is the LSTM and GRU with Attention model, which achieved a 0.89 F1-macro score."}
{"id": "2003.07459.d9949dd4865e79c53284932d868ca8fd10d55e70", "question": "Did they use crowdsourcing for the annotations?", "answer": ["No", "Yes"], "num_words": 3557, "gpt4_answer": "No, they used a team of volunteer annotators to manually annotate the dataset."}
{"id": "2003.07459.de689a17b0b9fb6bbb80e9b85fb44b36b56de2fd", "question": "How many annotators did they have?", "answer": ["Three, plus 2 in case of disagreement below 66%.", "three", "three volunteers "], "num_words": 3557, "gpt4_answer": "The paper had a team of three volunteer annotators."}
{"id": "2003.07459.5a90871856beeefaa69a1080e1b3c8b5d4b2b937", "question": "Is the dataset balanced?", "answer": ["No"], "num_words": 3557, "gpt4_answer": "The dataset is not balanced, as it contains over 29% offensive content."}
{"id": "2003.07459.6cb3007a09ab0f1602cdad20cc0437fbdd4d7f3e", "question": "What models do they experiment on?", "answer": [" Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF) ... linear classifier with Stochastic Gradient Descent (SGDC) learning ... The final classifier was two models based on the Bayes theorem: Multinomial Nave Bayes, which works with occurrence counts, and Bernoulli Nave Bayes, which is designed for binary features ... Pooled GRU ... Stacked LSTM with Attention ... LSTM and GRU with Attention ... 2D Convolution with Pooling ... GRU with Capsule ... LSTM with Capsule and Attention ... BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF) ... linear classifier with Stochastic Gradient Descent (SGDC) learning ... Multinomial Nave Bayes ... Bernoulli Nave Bayes ... Pooled GRU  ... Stacked LSTM with Attention  ... LSTM and GRU with Attention ... 2D Convolution with Pooling ... GRU with Capsule ... LSTM with Capsule and Attention ... BERT", "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF) ... linear classifier with Stochastic Gradient Descent (SGDC) learning ... Multinomial Nave Bayes ... Bernoulli Nave Bayes ... Pooled GRU BIBREF25 ... Stacked LSTM with Attention BIBREF25 ... LSTM and GRU with Attention BIBREF25 ... 2D Convolution with Pooling BIBREF26 ... GRU with Capsule BIBREF27 ... LSTM with Capsule and Attention BIBREF26 ... BERT BIBREF24"], "num_words": 3557, "gpt4_answer": "They experimented on several machine learning and deep learning models, including Linear SVM, RBF SVM, Stochastic Gradient Descent (SGDC), Multinomial Nave Bayes, Bernoulli Nave Bayes, Pooled GRU, Stacked LSTM with Attention, LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, and BERT."}
{"id": "1803.08614.211c242c028b35bb9cbd5e303bb6c750f859fd34", "question": "Do any of their reviews contain translations for both Catalan and Basque?", "answer": ["No"], "num_words": 2208, "gpt4_answer": "Unanswerable"}
{"id": "1803.08614.9b05d5f723a8a452522907778a084b52e27fd924", "question": "What is the size of their published dataset?", "answer": ["910", "911", "The final Catalan corpus contains 567 annotated reviews and the final Basque corpus 343."], "num_words": 2208, "gpt4_answer": "The final Catalan corpus contains 567 annotated reviews and the final Basque corpus contains 343 reviews."}
{"id": "1803.08614.21175d8853fd906266f884bced85c598c35b1cbc", "question": "How many annotators do they have for their dataset?", "answer": ["Unanswerable"], "num_words": 2208, "gpt4_answer": "The paper does not specify the exact number of annotators for the dataset."}
{"id": "1909.08250.87c00edc497274ae6a972c3097818de85b1b384f", "question": "How does sentence construction component works?", "answer": ["Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation", "Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph."], "num_words": 4303, "gpt4_answer": "The sentence construction component relies on the Grammatical Framework (GF) Library to generate natural language descriptions."}
{"id": "1909.08250.de4e949c6917ff6933f5fa2a3062ba703aba014c", "question": "What are two use cases that demonstrate capability of created system?", "answer": ["The first application is to build a natural language description of the ontologies built in an evolutionary biology project called Phylotastic, so that biologists can understand the output, without knowledge of ontologies. The second aims to create an abstract or intermediate representation of the Wikipedia pages from the BlueSky session in 2018.", "The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project ... The second application targets the challenge of creating an abstract Wikipedia from the BlueSky session of 2018 International Semantic Web Conference", "natural language description for workflow created by the system built in the Phylotastic project ... about people and includes descriptions for certain class"], "num_words": 4303, "gpt4_answer": "The two use cases that demonstrate the capability of the created system are: 1) generating natural language descriptions for annotated ontologies, and 2) creating an intermediate representation for Wiki pages."}
{"id": "1612.07486.4cf05da602669a4c09c91ff5a1baae6e30adefdf", "question": "Do they explore how their word representations vary across languages?", "answer": ["No", "Yes"], "num_words": 2059, "gpt4_answer": "No, the paper focuses on language vectors and their properties, not on word representations across languages."}
{"id": "1612.07486.7380e62edcb11f728f6d617ee332dc8b5752b185", "question": "Which neural language model architecture do they use?", "answer": ["LSTM", "character-level RNN", "standard stacked character-based LSTM BIBREF4"], "num_words": 2059, "gpt4_answer": "The authors use a standard stacked character-based LSTM with two layers, followed by a hidden layer and a final output layer with softmax activations."}
{"id": "1612.07486.f37b01e0c366507308fca44c20d3f69621b94a6e", "question": "How do they show genetic relationships between languages?", "answer": ["By applying hierarchical clustering on language vectors found during training", "By doing hierarchical clustering of word vectors", "hierarchical clustering"], "num_words": 2059, "gpt4_answer": "They show genetic relationships between languages by clustering the language vectors, which closely match the established genetic relationships in language families."}
{"id": "1612.03762.95af7aaea3ce9dab4cf64e2229ce9b98381dd050", "question": "Did they test the idea that the system reduces the time needed to encode ADR reports on real pharmacologists? ", "answer": ["Yes"], "num_words": 10452, "gpt4_answer": "The paper does not specifically mention testing the reduced time needed to encode ADR reports on real pharmacologists."}
{"id": "1612.03762.ab37ae82e38f64d3fa95782f2c791488f26cd43f", "question": "Do the authors offer a hypothesis as to why the system performs better on short descriptions than longer ones?", "answer": ["No"], "num_words": 10452, "gpt4_answer": "The authors do not explicitly offer a hypothesis for why the system performs better on short descriptions than longer ones."}
{"id": "1612.03762.6c9b3b2f2e5aac1de1cbd916dc295515301ee2a2", "question": "What are the steps in the MagiCoder algorithm?", "answer": ["Definition of ad hoc data structures ... Preprocessing of the original text ... Word-by-word linear scan of the description and voting task ... Weights calculation ... Sorting of voted terms and winning terms release", "Preprocessing of the original text ... Word-by-word linear scan of the description and voting task ... Weights calculation ... Sorting of voted terms and winning terms release", "Procedure INLINEFORM0 takes the narrative description, performs tokenization and stop-word removal and puts it into an array of words. ... Procedures INLINEFORM0 and INLINEFORM1 get LLTs and create a dictionary of words and of their stemmed versions, respectively ... By the functional notation INLINEFORM0 (resp., INLINEFORM1 ), we refer to the set of LLTs containing the word INLINEFORM2 (resp., the stem of INLINEFORM3 ). ... Function INLINEFORM0 returns the stemmed version of word INLINEFORM1 .\n\n ... Function INLINEFORM0 returns the position of word INLINEFORM1 in term INLINEFORM2 .\n\n ... INLINEFORM0 is a flag, initially set to 0, which holds 1 if at least a stemmed matching with the MedDRA term INLINEFORM1 is found. ... INLINEFORM0 , INLINEFORM1 , INLINEFORM2 are arrays and INLINEFORM3 appends INLINEFORM4 to array INLINEFORM5 , where INLINEFORM6 may be an element or a sequence of elements. ... INLINEFORM0 ( INLINEFORM1 ) are the weights related to the criteria defined in Section UID23 .\n\n ... Procedure INLINEFORM0 performs the multi-value sorting of the array INLINEFORM1 based on the values of the properties INLINEFORM2 of its elements. ... Procedure INLINEFORM0 , where INLINEFORM1 is a set of terms and INLINEFORM2 is a term, tests whether INLINEFORM3 (considered as a string) is prefix of a term in INLINEFORM4 . ... Dually, procedure INLINEFORM5 tests if in INLINEFORM6 there are one or more prefixes of INLINEFORM7 , and eventually remove them from INLINEFORM8 . ... Function INLINEFORM0 specifies whether a word INLINEFORM1 has been already covered (i.e., a term voted by INLINEFORM2 has been selected) in the (partial) solution during the term release: INLINEFORM3 holds 1 if INLINEFORM4 has been covered (with or without stemming) and it holds 0 otherwise. ... Procedures INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 is a set of terms, implement ordered-phrases and maximal-set-of-voters criteria (defined in Section UID28 ), respectively. ... Function INLINEFORM0 , returns the first INLINEFORM1 elements of an ordered set INLINEFORM2 . If INLINEFORM3 , the function returns the complete list of ordered terms and INLINEFORM4 nil values."], "num_words": 10452, "gpt4_answer": "The MagiCoder algorithm consists of five steps: 1) Definition of ad hoc data structures, 2) Preprocessing of the original text, 3) Word-by-word linear scan of the description and \"voting task\", 4) Weights calculation, and 5) Sorting of voted terms and winning terms release."}
{"id": "1612.03762.71413505d7d6579e2a453a1f09f4efd20197ab4b", "question": "How is the system constructed to be linear in the size of the narrative input and the terminology?", "answer": ["The main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms. ... INLINEFORM0 scans the text word-by-word (remember that each word corresponds to a token) once and performs a voting task: at the INLINEFORM1 -th step, it marks (i.e., votes) with index INLINEFORM2 each LLT INLINEFORM3 containing the current ( INLINEFORM4 -th) word of the ADR description.", "The system scans the text word-by-word once and performs a voting task for each word. It also keeps track of the position of the previous words.", "main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms"], "num_words": 10452, "gpt4_answer": "MagiCoder is designed with efficient data structures like hash tables, which allow for quick access to both the narrative input and the terminology. This ensures that the computational complexity of the algorithm remains linear in the size of the narrative input and the terminology."}
{"id": "1906.01010.3e6b6820e7843209495b4f9a72177573afaa4bc3", "question": "What conclusions do the authors draw about the aspects and mechanisms of personal recovery in bipolar disorder?", "answer": ["Occurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related personal recovery aspects", "Unanswerable", "a recovery approach as leading principle for national or global health service strategies, should be informed by voices of individuals ... expect the proposed investigations of views on recovery by previously under-researched ethnic, language, and cultural groups to yield valuable insights on the appropriateness of the recovery approach for a wider population ... The datasets collected in this project can serve as useful resources for future research"], "num_words": 3601, "gpt4_answer": "The paper does not provide specific conclusions about the aspects and mechanisms of personal recovery in bipolar disorder, as it is a proposal for an interdisciplinary project to investigate these aspects using computational linguistic methods and social media data."}
{"id": "1906.01010.a926d71e6e58066d279d9f7dc3210cd43f410164", "question": "What languages were included in this multilingual population?", "answer": ["Unanswerable"], "num_words": 3601, "gpt4_answer": "The paper does not specify the exact languages included in the multilingual population."}
{"id": "1906.01010.3d547a7dda18a2dd5dc89f12d25d7fe782d66450", "question": "What computational linguistic methods were used for the analysis?", "answer": ["Unanswerable", "language identification"], "num_words": 3601, "gpt4_answer": "The paper proposes using mixed-methods approach with tools like Wmatrix for comparative analysis, semantic tagging with USAS, distributional semantics methods, LIWC, topic modeling, and emotion and sentiment analysis for the analysis."}
{"id": "1906.01010.4a32adb0d54da90434d5bd1c66cc03a7956d12a0", "question": "Was permission sought from the bipolar patients to use this data?", "answer": ["For Twitter and Reddit users , implicit consent is assumed to use their public tweets. Blog users are  contacted to obtain consent for using their texts.", "No", "Unanswerable"], "num_words": 3601, "gpt4_answer": "Permission was not sought from the account holders of Twitter and Reddit posts included in the datasets, but the researchers ensured anonymity and only shared anonymized and paraphrased excerpts from the texts."}
{"id": "1906.01010.c17ece1dad42d92c78fca2e3d8afa9a20ff19598", "question": "How are the individuals with bipolar disorder identified?", "answer": [" Twitter and Reddit users with a BD diagnosis will be identified automatically via self-reported diagnosis statements, such as `I was diagnosed with BD-I last week'.", "Twitter and Reddit users  are identified automatically  via self-reported diagnosis statements. Blog users are identified manually.", "characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12"], "num_words": 3601, "gpt4_answer": "Individuals with bipolar disorder are identified automatically via self-reported diagnosis statements on Twitter and Reddit, such as \"I was diagnosed with BD-I last week.\""}
{"id": "2003.11528.c2ce25878a17760c79031a426b6f38931cd854b2", "question": "What is the source of the training/testing data?", "answer": ["CCPC1.0", "Chinese poem corpus with 250,000 Jueju and Lvshi, 20,000 CIs, 700,000 pairs of couplets", "Two major forms(Jueju and Lvshi) of SHI and 121 major forms of CI from Chinese Classical Poerty Corpus (CCPC1.0)"], "num_words": 3172, "gpt4_answer": "The source of the training/testing data is the Chinese Classical Poetry Corpus (CCPC1.0), which consists of 834,902 poems."}
{"id": "2003.11528.1d263356692ed8cdee2a13f103a82d98f43d66eb", "question": "What are the types of chinese poetry that are generated?", "answer": ["SHI  ... CI ", "two major forms of SHI, Jueju, and Lvshi, ... 121 major forms (Cipai) of CI ", "two primary categories, SHI and CI ... SHI and CI can be further divided into many different types"], "num_words": 3172, "gpt4_answer": "The framework generates major types of Chinese classical poems, including SHI (Jueju and Lvshi) and CI (121 major forms or Cipai)."}
{"id": "1801.03615.68f1df3fb0703ff694a055d23e7ec3f6fb449b8d", "question": "what is the previous work they are comparing to?", "answer": ["RNN and Transformer baseline systems utilize BPE BIBREF3 ... fully character-based baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work", "RNN and Transformer baseline systems utilize BPE BIBREF3 to transfer the original word sequence to subword sequence on both the source and the target sides", "Subword based NMT, Character-based NMT"], "num_words": 4173, "gpt4_answer": "The authors compare their method with two previous methods: subword (Byte-Pair Encoding, BPE) and fully character-based methods, which can also address the problem of morphologically rich languages to some extent."}
{"id": "1910.09362.c7f43c95db3d0c870407cd0e7becdd802463683b", "question": "Do they use skip-gram word2vec?", "answer": ["Yes"], "num_words": 3602, "gpt4_answer": "Yes, they use the skip-gram (SG) architecture in Word2Vec."}
{"id": "1910.09362.4e2b12cfc530a4682b06f8f5243bc9f64bd41135", "question": "How is quality of the word vectors measured?", "answer": ["They evaluate it on the word analogy, word similarity and synonym selection tasks using Pearson correlation coefficient as the metric.", "Unanswerable", "correlation between the word similarity scores by human judgment and the word distances in vector space ... select the semantically closest word, from the candidate answers"], "num_words": 3602, "gpt4_answer": "The quality of word vectors is measured using evaluation tasks such as word similarity, synonym selection, and word analogy tasks."}
{"id": "1911.12559.bc7081aaa207de2362e0bea7bc8108d338aee36f", "question": "Do they report results only on English data?", "answer": ["No", "Unanswerable", "Yes"], "num_words": 2148, "gpt4_answer": "Yes, the results are reported only on English data."}
{"id": "1911.12559.c72e05dd41ed5a85335ffeca5a03e71514e60e84", "question": "Where do the news texts come from?", "answer": ["New York Times ... Japan Times", "online news websites ... New York Times ... Japan Times", "the New York Times"], "num_words": 2148, "gpt4_answer": "The news texts in KPTimes primarily come from the New York Times, with additional data collected from the Japan Times."}
{"id": "1911.12559.07edc082eb86aecef3db5cad2534459c1310d6e8", "question": "What baseline is used for this task?", "answer": [" FirstPhrases baseline ... MultipartiteRank", "FirstPhrase ... MultipartiteRank", "FirstPhrases baseline ... MultipartiteRank BIBREF17"], "num_words": 2148, "gpt4_answer": "The baselines used for this task are FirstPhrase and MultipartiteRank."}
{"id": "1911.12559.eaacee4246f003d29a108fe857b5dd317287ecf1", "question": "What type of nerual keyphrase generation models are trained?", "answer": ["CopyRNN BIBREF2", "CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes)", "encoder-decoder model"], "num_words": 2148, "gpt4_answer": "The paper trains CopyRNN, an encoder-decoder model with a copying mechanism, for keyphrase generation."}
{"id": "1911.12559.3ea82a5ca495ffbd1e30e8655aef1be4ba423efe", "question": "How do the editors' annotations differ from those in existing datasets?", "answer": [" news articles are annotated in a semi-automatic way ... first the editors revise a set of tags proposed by an algorithm ... provide additional tags which will be used by a taxonomy team to improve the algorithm", "Existing datasets are annotated by non-experts who use a larger, less controlled indexed vocabulary lacking the domain expertise shown by the editors", "Exper annotators use a smaller, more controlled indexing vocabulary."], "num_words": 2148, "gpt4_answer": "Editors' annotations in KPTimes are more uniform and consistent, with a larger number of absent keyphrases and a more controlled indexing vocabulary compared to existing datasets."}
{"id": "1802.09059.b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "question": "How long is their dataset?", "answer": ["Unanswerable"], "num_words": 3379, "gpt4_answer": "Unanswerable"}
{"id": "1802.09059.4ae0b50c88a174cfc283b90cd3c9407de13fd370", "question": "Do they use pretrained word embeddings?", "answer": ["No", "Yes"], "num_words": 3379, "gpt4_answer": "Yes, they use pre-trained word embeddings from GloVe (Global Vectors for word representation)."}
{"id": "1802.09059.a18d74109ed55ed14c33913efa62e12f207279c0", "question": "How many layers does their model have?", "answer": ["6", "6 layers"], "num_words": 3379, "gpt4_answer": "The model has 6 layers."}
{"id": "1802.09059.1d6d21043b9fd0ed3ccccdc6317dcf5a1347ef03", "question": "What metrics do they use?", "answer": ["F-measure"], "num_words": 3379, "gpt4_answer": "The authors use F-measure as a metric to evaluate their model."}
{"id": "1803.09000.e90425ac05a15dc145bbf3034e78b56e7cec36ac", "question": "what dataset did they use?", "answer": ["DUC-2001 ... Inspec  ...  NUS Keyphrase Corpus ...  ICSI Meeting Corpus ", "DUC-2001 dataset ... Inspec dataset ... NUS Keyphrase Corpus ... ICSI Meeting Corpus", "DUC-2001 dataset BIBREF6 ... Inspec dataset ... NUS Keyphrase Corpus BIBREF10 ... ICSI Meeting Corpus"], "num_words": 2146, "gpt4_answer": "They used four datasets: DUC-2001, Inspec, NUS Keyphrase Corpus, and ICSI Meeting Corpus."}
{"id": "1803.09000.b677952cabfec0150e028530d5d4d708d796eedc", "question": "what was their model's f1 score?", "answer": ["27.53, 27.01, 4.30 and 9.10 for DUC, Inspec, ICSI and Nus datasets respectively.", "F1 score their system achieved is 27.53, 27.01, 4.30 and 9.10 on DUC, Inspec, ICSI and NUS dataset respectively.", "On DUC 27.53, on Inspec 27.01, on ICSI 4.30, and on Nus 9.10"], "num_words": 2146, "gpt4_answer": "Unanswerable"}
{"id": "1803.09000.d7799d26fe39302c4aff5b530aa691e8653fffe8", "question": "what are the state of the art models?", "answer": [" SingleRank and Topical PageRank", "SingleRank ... Topical PageRank", "SingleRank and Topical PageRank"], "num_words": 2146, "gpt4_answer": "The paper does not specifically mention the state-of-the-art models by name, but it compares WikiRank with SingleRank and Topical PageRank in the results section."}
{"id": "1909.06708.2711ae6dd532d136295c95253dbf202e37ecd3e7", "question": "How do you know the word alignments are correct?", "answer": ["In particular, we minimize KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model,", "we conduct an ablation study on IWSLT14 De-En task. As shown in Table TABREF18, the hints from word alignments provide an improvement of about 1.6 BLEU points, and the hints from hidden states improve the results by about 0.8 BLEU points. We also test these models on a subsampled set whose source sentence lengths are at least 40. Our model outperforms the baseline model by more than 3 BLEU points (20.63 v.s. 17.48).", "we use the word alignment information from the ART model"], "num_words": 2227, "gpt4_answer": "The word alignments are considered correct if they lead to accurate translations, and the paper uses KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and student models to encourage the student to have similar word alignments to the teacher model."}
{"id": "1909.06708.96356c1affc56178b3099ce4b4aece995032e0ff", "question": "How slow is the unparallelizable ART model in the first place?  ", "answer": ["784 miliseconds", "Unanswerable", "While the ART models have achieved great success in terms of translation quality, the time consumption during inference is still far away from satisfactory"], "num_words": 2227, "gpt4_answer": "The paper does not provide a specific speed value for the ART model."}
{"id": "1909.06708.92fc94a4999d1b25a0593904025eb7b8953bb28b", "question": "What metric is used to measure translation accuracy?", "answer": ["BLEU ", "BLEU score", "BLUE and the percentage of repetitive words"], "num_words": 2227, "gpt4_answer": "The metric used to measure translation accuracy is BLEU (Bilingual Evaluation Understudy) score."}
{"id": "1909.06708.e56c1f0e9eabda41f929d0dfd5cfa50edd69fa89", "question": "Were any datasets other than WMT used to test the model?", "answer": ["Yes"], "num_words": 2227, "gpt4_answer": "Yes, the IWSLT14 German-to-English (De-En) dataset was also used to test the model."}
{"id": "1909.06708.a86758696926f2db71f982dc1a4fa4404988544e", "question": "Are the results applicable to other language pairs than German-English?", "answer": ["Unanswerable"], "num_words": 2227, "gpt4_answer": "The paper focuses on German-English language pairs, and it is not clear if the results are directly applicable to other language pairs."}
{"id": "1705.10754.9262292ca4cc78de515b5617f6a91e540eb2678c", "question": "What dicrimating features are discovered?", "answer": ["Highest gain is obtained by average, maximum, minimum, and standard deviation. Probability and proportionality features have low information gain", "a document's terms' minimum, maximum, average (relative to all terms and to in-vocabulary terms), and standard deviation of weights; and proportion of terms that are in-vocabulary", "average ... maximum and minimum ... standard deviation"], "num_words": 2669, "gpt4_answer": "The most discriminant features discovered are average, maximum and minimum, and standard deviation, while probability and proportionality features have low information gain."}
{"id": "1705.10754.d796a251792eca01cea31ba5cf3e54ff9acf543f", "question": "What results are obtained on the alternate datasets?", "answer": [" three representations obtained comparative results and support the robustness of the low dimensionality representation", "Accuracy results range from 74.4 to 100 ", "Comparable to state-of-the-art"], "num_words": 2669, "gpt4_answer": "The LDR, Skip-gram, and SenVec representations obtained comparative results on the alternate DSLCC corpus dataset, showing the robustness of the low dimensionality representation."}
{"id": "1706.08568.a526c63fc8dc1b79702b481b77e3922d7002d973", "question": "Are answers in this dataset guaranteed to be substrings of the text? If not, what is the coverage of answers being substrings?", "answer": ["No, the answers can also be summaries or yes/no.", "Unanswerable", "Yes"], "num_words": 1542, "gpt4_answer": "Yes, the system's responses are restricted to substrings in the provided snippets."}
{"id": "1706.08568.0f9678e11079ee9ea1a1ce693f017177dd495ee5", "question": "How much is the gap between pretraining on SQuAD and not pretraining on SQuAD?", "answer": ["Unanswerable"], "num_words": 1542, "gpt4_answer": "Unanswerable"}
{"id": "1909.05190.0f1f81b6d4aa0da38b4cc8b060926e7df61bb646", "question": "What is the machine learning method used to make the predictions?", "answer": ["Compositional Neural Network ... Element-wise Multiplicative Composition ... Neural Tensor Network", "SGNN", "SGNN ... Word, BIBREF23 ... Event, BIBREF24 ... NTN, BIBREF4 ... KGEB, BIBREF18 "], "num_words": 4080, "gpt4_answer": "The paper uses a neural tensor network (NTN) to learn baseline event embeddings and incorporates intent and sentiment information using bi-directional LSTMs (BiLSTM) and a softmax binary classifier."}
{"id": "1909.05190.ec62df859ad901bf0848f0a8b91eedc78dba5657", "question": "How is the event prediction task evaluated?", "answer": ["accuracy", "replacing the event embeddings on SGNN and running it on the MCNC dataset", "we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings"], "num_words": 4080, "gpt4_answer": "The event prediction task is evaluated on the standard multiple choice narrative cloze (MCNC) dataset, comparing the performance of intent and sentiment-powered event embedding models with state-of-the-art baselines on tasks like event similarity, script event prediction, and stock prediction."}
{"id": "1909.05190.ccec4f8deff651858f44553f8daa5a19e8ed8d3b", "question": "What are the datasets used in the paper?", "answer": ["ATOMIC  ... MCNC", "ATOMIC ... hard similarity small and big dataset ... the transitive sentence similarity dataset ... the standard multiple choice narrative cloze (MCNC) dataset", "ATOMIC, New York Times Gigaword, an unreleased extension of the dataset by BIBREF5, MCNC"], "num_words": 4080, "gpt4_answer": "The datasets used in the paper are the New York Times Gigaword Corpus (LDC2007T07), ATOMIC, Event2Mind, hard similarity dataset, transitive sentence similarity dataset, and multiple choice narrative cloze (MCNC) dataset."}
{"id": "1606.02601.d38745a3910c380e6df97c7056a5dd9643fd365b", "question": "Do they compare to other models that include subword information such as fastText?", "answer": ["No"], "num_words": 3780, "gpt4_answer": "Unanswerable"}
{"id": "1606.02601.2b75df325c98b761faf2fecf6e71ac7366eb15ea", "question": "Is there a difference between the model's performance for morphologically impoverished and morphologically complex languages?", "answer": ["They did not report results for English but expect that morphologically complex languages will perform better.", "Unanswerable", "Yes"], "num_words": 3780, "gpt4_answer": "The paper only reports results for English, which is morphologically impoverished. The authors expect the model to perform even better for languages with richer morphology than English, such as Turkish and German."}
{"id": "1606.02601.649e77ac2ecce42ab2efa821882675b5a0c993cb", "question": "What languages do they apply the model to?", "answer": ["English"], "num_words": 3780, "gpt4_answer": "The model is only reported for English in this paper."}
{"id": "1606.02601.0bc305d6b90f77f835bc4c904b22a4be07f963b2", "question": "How are the embeddings evaluated in the human judgement comparison?", "answer": ["Using cosine similarity between the embeddings which is then correlated with human judgement", "human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments"], "num_words": 3780, "gpt4_answer": "Embeddings are evaluated by ranking the word pairs according to their cosine similarity and then measuring the correlation (using Spearman's ) between model judgments and human judgments."}
{"id": "1602.04341.041529e15b70b21986adb781fd9b94b595e451ed", "question": "what was the margin their system outperformed previous ones?", "answer": ["15.6 and 16.5 for accuracy and NDCG on MCTest-150, 7.3 and 4.6 on MCTest-500.", "15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500"], "num_words": 4253, "gpt4_answer": "The margin between the best-performing HABCNN-TE and NR is 15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500."}
{"id": "1602.04341.da2350395867b5fd4dbf968b5a1cd6921ab6dd37", "question": "what prior approaches did they compare to?", "answer": ["Addition ... Addition-proj ... Neural Reasoner ... Attentive Reader", "Neural Reasoner ... Attentive Reader", "The Neural Reasoner ... The Attentive Reader"], "num_words": 4253, "gpt4_answer": "The authors compared their HABCNN system to several baselines, including Addition, Addition-proj, NR (Neural Reasoner), and AR (Attentive Reader)."}
{"id": "1908.02284.eb653a5c59851eda313ece0bcd8c589b6155d73e", "question": "what are the baselines?", "answer": ["a one-stage RNN system", "one-stage RNN system", "one-stage RNN system containing 2-layer BLSTM"], "num_words": 2237, "gpt4_answer": "The baseline used for comparison is a one-stage RNN system, containing a 2-layer BLSTM and directly trained to recognize dialect categories."}
{"id": "1908.02284.0caa3162abe588f576a568d63ab9fd0e9c46ceda", "question": "what results do they achieve?", "answer": [" The relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task (88.88 and 87.24)  relative to the baseline  78.85.", " relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline ... accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi", "state-of-the-art in the Chinese dialect recognition task"], "num_words": 2237, "gpt4_answer": "The two-stage system achieves a 10% relative accuracy increase on every task compared to the baseline and performs best among the tested systems, with high accuracy in recognizing several dialects like Shanghai (98.8%) and Hefei (99.8%)."}
{"id": "1908.02284.cbe42bf7c99ee248cdb2c5d6cf86b41106e66863", "question": "what chinese dialects are explored?", "answer": ["Ningxia ... Hefei ... Sichuan ... Shanxi ... Changsha ... Hebei ... Nanchang ... Shanghai ... Kekka ... Fujian", "Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian"], "num_words": 2237, "gpt4_answer": "The paper explores 10 Chinese dialects: Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka, and Fujian."}
{"id": "1907.00168.94d794df4a3109522c2ea09dad5d40e55d35df51", "question": "Which neural machine translation model was used?", "answer": ["SGNMT", "SGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8", "Unanswerable"], "num_words": 2474, "gpt4_answer": "The paper used Transformer architecture for their neural machine translation (NMT) models."}
{"id": "1907.00168.044c66c6b7ff7378682f24887b05e1af79dcd04f", "question": "What position did this entry finish in, in the overall shared task?", "answer": ["Unanswerable"], "num_words": 2474, "gpt4_answer": "Unanswerable"}
{"id": "1907.00168.903ac8686ed7e6e3269a5d863f06ff11c50e49e8", "question": "What are the restrictions of the restricted track?", "answer": ["The organizers provided a dataset allowed to use for training", "explore the potential of purely neural models for grammatical error correction", "goal on the restricted track was to explore the potential of purely neural models for grammatical error correction"], "num_words": 2474, "gpt4_answer": "The restricted track provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences for training."}
{"id": "1907.00168.ab95ca983240ad5289c123a2774f8e0db424f4a1", "question": "What does BEA stand for?", "answer": ["Unanswerable"], "num_words": 2474, "gpt4_answer": "BEA stands for Building Educational Applications."}
{"id": "1911.09709.fcf9377fc3fce529d4bab1258db3f46b15ae5872", "question": "Which works better according to human evaluation, the concurrent or the modular system?", "answer": ["Modular", "Overall, while modular does a better job at reducing bias, concurrent appears to better preserve the meaning and fluency of the original text.", "They are equal"], "num_words": 4348, "gpt4_answer": "The modular system is better at reducing bias and has higher accuracy, while the concurrent system produces more fluent responses, preserves meaning better, and has higher BLEU. There is no clear winner between the two proposed systems."}
{"id": "1911.09709.5422a3f2a083395416d6f99c57d28335eb2e44e1", "question": "Were the Wikipedia edits that removed framings, presuppositions and attitudes from biased sentences a Wiki community effort, or were annotators trained to do it?", "answer": [" Wikipedia edits", "Wiki community effort", "Wikipedia editors"], "num_words": 4348, "gpt4_answer": "The Wikipedia edits were made by English Wikipedia editors as part of their efforts to ensure texts had a neutral point of view."}
{"id": "1911.09709.7c2d6bc913523d77e8fdc82c60598ee95b445d84", "question": "How is subjective text automatically neutralized?", "answer": [" Identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy and suggest edits that would make it more neutral.", "The text is modified to remove the subjective bias while preserve the meaning as much as possible", "algorithm is given an input sentence and must produce an output sentence whose meaning is as similar as possible to the input but with the subjective bias removed"], "num_words": 4348, "gpt4_answer": "The paper proposes two algorithms for automatically neutralizing subjective text: a modular algorithm with BERT-based detection and LSTM-based editing, and a concurrent algorithm using a BERT encoder and LSTM decoder. Both methods leverage denoising autoencoders and a token-weighted loss function."}
{"id": "1909.11232.1a0794ebbc9ee61bbb7ef2422d576a10576d9d96", "question": "What is the sign language recognition task investigated?", "answer": [" American Sign Language recognition ", "Unanswerable", "We present a deep learning based approach for ASL recognition that leverages skeletal and video data. The proposed model captures the underlying temporal dynamics associated with sign language and also identifies specific hand shape patterns from video data to improve recognition performance. "], "num_words": 3997, "gpt4_answer": "The sign language recognition task investigated is the recognition of American Sign Language (ASL) signs using skeletal and RGB video data with deep learning architectures."}
{"id": "1909.11232.256dfa501a71d7784520a527f43aec0549b1afea", "question": "What is the performance of the best model in the sign language recognition task?", "answer": ["Accuracy 81%", "Best performing model is Spatial AI-LSTM with accuracy 81% and Std. Deviation 6%", "Spatial AI-LSTM"], "num_words": 3997, "gpt4_answer": "The best model, Spatial AI-LSTM, achieves an average test accuracy of 68% across all 12 subjects."}
{"id": "1909.11232.f85520bbc594918968d7d9f33d11639055458344", "question": "What are the deep learning architectures used?", "answer": ["3D CNN, Axis independent LSTM,  spatial axis independent LSTM, and combined network ", "Axis Independent Architecture (AI-LSTM) ... Spatial AI-LSTM ...  Max CNN-LSTM ... 3D CNN", "Recurrent Neural Networks (RNN) ... 3D Convolutional Neural Network ... Axis Independent LSTM ... Spatial AI-LSTM ... Max CNN-LSTM network"], "num_words": 3997, "gpt4_answer": "The deep learning architectures used in this study are Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM) networks, and 3D Convolutional Neural Networks (3D CNN)."}
{"id": "1808.09180.e4f2d59030b17867449cf5456118ab722296bebd", "question": "Who made the stated claim (that \"this is because character-level models learn morphology\")?", "answer": ["Chung et al. (2016)", "Unanswerable"], "num_words": 4249, "gpt4_answer": "The paper does not specifically attribute the claim to a particular person or source."}
{"id": "1808.09180.e664b58ea034a638e7142f8a393a88aadd1e215e", "question": "Which languages do they use?", "answer": ["Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, Hebrew", "Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, and Hebrew"], "num_words": 4249, "gpt4_answer": "They experiment on twelve languages with varying morphological typologies."}
{"id": "1808.09180.c4b621f573bbb411bdaa84a7562c9c4795a7eb3a", "question": "Do the character-level models perform better than models with access to morphological analyses only?", "answer": ["No"], "num_words": 4249, "gpt4_answer": "No, character-level models rarely match the accuracy of oracle models with access to explicit morphology."}
{"id": "1808.09180.3ccc4ccebc3b0de5546b1208e8094a839fd4a4ab", "question": "What is case syncretism?", "answer": ["A situation in which a noun's syntactic function is ambiguous without context.", "The phenomena where words that have the same form express different morphological cases", "when noun case is ambiguous"], "num_words": 4249, "gpt4_answer": "Case syncretism is a phenomenon in which noun caseand thus syntactic functionis ambiguous due to functionally distinct words having the same form."}
{"id": "1909.04251.2dba0b83fc22995f83e7ac66cc8f68bcdcc70ee9", "question": "Do humans assess the quality of the generated responses?", "answer": ["Yes"], "num_words": 4333, "gpt4_answer": "Yes, human evaluation was conducted to assess the quality of the generated responses."}
{"id": "1909.04251.a8cc891bb8dccf0d32c1c9cd1699d5ead0eed711", "question": "What models are used to generate responses?", "answer": ["Seq2Seq ... Variational Auto-Encoder (VAE) ... Reinforcement Learning (RL)", "Seq2Seq BIBREF25, BIBREF24 ... Variational Auto-Encoder (VAE) BIBREF26 ... Reinforcement Learning (RL)"], "num_words": 4333, "gpt4_answer": "The models used to generate responses are Seq2Seq, Variational Auto-Encoder (VAE), and Reinforcement Learning (RL)."}
{"id": "1909.04251.8330242b56b63708a23c6a92db4d4bcf927a4576", "question": "What types of hate speech are considered?", "answer": [" Potentially hateful comments are identified using hate keywords.", "Unanswerable", "race, ethnicity, national origin, religious affiliation, sexual orientation, caste, sex, gender, gender identity, and serious disease or disability."], "num_words": 4333, "gpt4_answer": "Hate speech is considered as a direct attack on people based on protected characteristics, such as race, ethnicity, national origin, religious affiliation, sexual orientation, caste, sex, gender, gender identity, and serious disease or disability."}
{"id": "1912.11980.a4cf0cf372f62b2dbc7f31c600c6c66246263328", "question": "Which baselines to they compare to?", "answer": ["AllText ... F8W"], "num_words": 4217, "gpt4_answer": "The baseline systems they compare to include AllText and F8W."}
{"id": "1912.11980.f7b91b99279833f9f489635eb8f77c6d13136098", "question": "Which sentence compression technique works best?", "answer": ["ESC model", "ESC model ", "semi-supervised ESC model "], "num_words": 4217, "gpt4_answer": "The semi-supervised ESC (Explicit Sentence Compression) model works best among the proposed techniques."}
{"id": "1912.11980.99e514acc0109b7efa4e3860ce1e8c455f5bb790", "question": "Do they compare performance against state of the art systems?", "answer": ["Yes"], "num_words": 4217, "gpt4_answer": "Yes, the proposed models are compared against state-of-the-art systems in the experiments."}
{"id": "2004.04060.2fec84a62b4028bbe6500754d9c058eefbc24d9a", "question": "What is the performance of large state-of-the-art models on these datasets?", "answer": ["93.5", "Akbik et al. (2019) -  89.3 on Ontonotes 5\nBaevski et al. (2019) 93.5 on CoNLL-03", "Average  92.87 for CoNLL-01 and Average 8922 for Ontonotes 5"], "num_words": 2322, "gpt4_answer": "The performance of large state-of-the-art models on CoNLL-03 and Ontonotes 5 datasets show F1 improvements over the baseline model from 92.34 to 92.86 and 89.11 to 89.32, respectively."}
{"id": "2004.04060.2803709fba74e6098aae145abcbf0e9a3f4c35e5", "question": "What is used as a baseline model?", "answer": ["Neural CRF model with and without ELMo embeddings"], "num_words": 2322, "gpt4_answer": "The baseline model used is the Neural CRF model with and without ELMo embeddings."}
{"id": "2004.04060.ec39120fb879ae10452d3f244e1e32237047005a", "question": "How do they build gazetter resources from Wikipedia knowlege base?", "answer": ["Extract entity type tuples at appropriate level of granularity depending on the NER task.", "To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\\rightarrow $ City and Massachusetts $\\rightarrow $ State.", "process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet ... Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long ... we use the sitelink count to keep the six most popular types ... To move from fine-grained to coarse-grained types, we use the Wikidata hierarchical structure"], "num_words": 2322, "gpt4_answer": "The authors build gazetteer resources from the open source Wikidata knowledge base, which is part of the Wikipedia project. They extract general gazetteers from Wikidata and use entity popularity filtering to build the resources."}
{"id": "1807.08089.ac87dd34d28c3edd9419fa0145f3d38c87d696aa", "question": "What is the dataset that is used to train the embeddings?", "answer": [" LibriSpeech BIBREF46", "LibriSpeech"], "num_words": 4160, "gpt4_answer": "The dataset used to train the embeddings is LibriSpeech, a corpus of read speech in English derived from audiobooks."}
{"id": "1807.08089.e66a88eecf8d5d093caec1f487603534f88dd7e7", "question": "What speaker characteristics are used?", "answer": ["Acoustic factors such as speaker characteristics, microphone characteristics, background noise.", "Unanswerable", "speaker characteristics ... microphone characteristics ... background noise"], "num_words": 4160, "gpt4_answer": "Speaker characteristics include factors such as speaker identity, microphone characteristics, and background noise."}
{"id": "1807.08089.fef5b65263c81299acc350a101dabaf5a8cb9c6e", "question": "What language is used for the experiments?", "answer": ["English"], "num_words": 4160, "gpt4_answer": "The language used for the experiments is English."}
{"id": "1807.08089.f40e23adc8245562c8677f0f86fa5175179b5422", "question": "Is the embedding model test in any downstream task?", "answer": ["Yes"], "num_words": 4160, "gpt4_answer": "Yes, the embedding model is tested in a downstream task of spoken document retrieval."}
{"id": "1703.05320.50bcbb730aa74637503c227f022a10f57d43f1f7", "question": "what is the baseline model", "answer": ["For the first task they have two baseline models, TF-IDF and LSI which both use cosine similarity. For the QA task, they baseline models were the original CNN and CNN with separate TF-IDF, LSI features.", "The baseline models used for this paper are based on the TF-IDF and LSI features and cosine similarity as a retrieval method.", "two baseline models TF-IDF and LSI", "two baseline models TF-IDF and LSI which only use Cosine similarity"], "num_words": 3797, "gpt4_answer": "The baseline models for information retrieval are TF-IDF and LSI, which use Cosine similarity to retrieve relevant articles."}
{"id": "1703.05320.fac273ecb3e72f2dc94cdbc797582d7225a8e070", "question": "What contribute to improve the accuracy on legal question answering task?", "answer": ["Adding more features to the traditional sets such as TF-IDF, BM25 and PL2F as well as using voting in a ranking system help to improve accuracy on a legal question answering task", "two additional statistic features: TF-IDF and LSI"], "num_words": 3797, "gpt4_answer": "Adding auxiliary features and using a combination of Ranking SVM and CNN contribute to improving the accuracy on legal question answering tasks."}
{"id": "1910.10762.7c561db6847fb0416bca8a6cb5eebf689a4b1438", "question": "What sizes were their datasets?", "answer": ["150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data", "20 hours of training data ... dev and test sets comprise 4.5 hours of speech", "ast-20h: 20 hours,\nzh-ai-small: 20 hours,\nzh-ai-large: 150 hours,\nzh-ai-hanzi: 150 hours,\nhr-gp: 12 hours,\nsv-gp: 18 hours,\npl-gp: 19 hours,\npt-gp: 23 hours,\nfr-gp: 25 hours,\nzh-gp: 26 hours,\ncs-gp: 27 hours,\nmultilin6: 124 hours"], "num_words": 3182, "gpt4_answer": "The paper used datasets of varying sizes, including a 150-hour AISHELL corpus of Chinese and seven GlobalPhone languages, each with about 20 hours of data."}
{"id": "1910.10762.13eb64957478ade79a1e81d32e36ee319209c19a", "question": "How many layers does their model have?", "answer": ["10 ", "two ", "two CNN layers ... three-layer bi-directional long short-term memory network (LSTM) ... followed by a three-layer LSTM"], "num_words": 3182, "gpt4_answer": "The model has two CNN layers followed by a three-layer bi-directional LSTM in the encoder, and a three-layer LSTM in the decoder."}
{"id": "1910.10762.3cfe464052f0a248b6e22c9351279403dfe34f3c", "question": "What is their model's architecture?", "answer": [" the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2", "encoder-decoder model ... end-to-end system architecture", "two CNN layers ... three-layer bi-directional long short-term memory network (LSTM) ...  followed by a three-layer LSTM"], "num_words": 3182, "gpt4_answer": "The model's architecture consists of a stack of two CNN layers followed by a three-layer bi-directional LSTM for the encoder, and a 128-dimensional embedding layer followed by a three-layer LSTM with an attention mechanism for the decoder."}
{"id": "1910.10762.119c404da6e42d4879eee10edeab4b2851162659", "question": "What languages did they use?", "answer": ["Spanish ... English  ... Chinese  ... Mandarin Chinese  ... Croatian  ... Czech  ... French  ... Polish  ... Portuguese  ... Swedish ", "Spanish ... English ... Mandarin Chinese ... Croatian ... Czech ... French ... Polish ... Portuguese ... Swedish", "Spanish-English"], "num_words": 3182, "gpt4_answer": "The authors used GlobalPhone corpora from seven languages: Mandarin Chinese (zh), Croatian (hr), Czech (cs), French (fr), Polish (pl), Portuguese (pt), and Swedish (sv). They also used the AISHELL-1 corpus of Mandarin Chinese and Switchboard English for some experiments."}
{"id": "1711.01567.32f2aa2df0152050cbcd27dd2f408b2fa5894031", "question": "Are there experiments with real data?", "answer": ["No", "Yes", "Yes. They show results on the Wall Street Journal Corpus, which consists of recordings of real speech."], "num_words": 2344, "gpt4_answer": "Unanswerable"}
{"id": "1706.04206.065623cc1d5f5b19ec1f84d286522fc2f805c6ce", "question": "What supervised machine learning models do they use?", "answer": ["They use four classifiers: ZeroR, Naive Bayes, J48, and random forest.", "ZeroR, Nave Bayes, J48, and random forest ", "ZeroR, Nave Bayes, J48, and random forest classifiers"], "num_words": 2377, "gpt4_answer": "The authors use ZeroR, Nave Bayes, J48, and random forest classifiers in their study."}
{"id": "1706.04206.5c17559749810c67c50a7dbe34580d5e3b4f9acb", "question": "Does the supervised machine learning approach outperform previous work?", "answer": ["No"], "num_words": 2377, "gpt4_answer": "No, the supervised machine learning approach's performance is still inferior to a collection of manually created extraction rules."}
{"id": "1706.04206.1c0a575e289eb486d3e6375d6f783cc2bf18adf9", "question": "How large is the released data set?", "answer": ["1470 sentences", "316 sentences in Hypertension corpus, 877 sentences in Rhinosinusitis corpus", "Unanswerable"], "num_words": 2377, "gpt4_answer": "The paper does not provide the exact size of the released dataset."}
{"id": "1706.04206.4efe0d62bba618803ec12b63f32debb8b757dd68", "question": "What is an example of a condition-action pair?", "answer": ["Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation", "If patients have asthma, then beta-blockers, including eye drops, are contraindicated", "Unanswerable"], "num_words": 2377, "gpt4_answer": "An example of a condition-action pair is: \"If the A1C is 7.0% and a repeat result is 6.8%, the diagnosis of diabetes is confirmed.\""}
{"id": "1707.06875.97708d93bccc832ea671dc31a76dad6a121fcd60", "question": "Which metrics were considered?", "answer": ["ter ... bleu  ... rouge  ... nist ... lepor ... cider ... meteor ... Semantic Text Similarity ...  Flesch Reading Ease  ... characters per utterance (len) and per word (cpw) ... words per sentence ... syllables per sentence (sps) and per word (spw) ... polysyllabic words per utterance (pol) and per word (ppw) ... the number of misspellings (msp) and the parsing score as returned by the Stanford parser (prs)", "ter ... bleu ... rouge ... nist ... lepor ... cider ... meteor ... Semantic Similarity (sim) ... readability and grammaticality", "ter BIBREF18 , bleu BIBREF0 , rouge BIBREF19 , nist BIBREF20 , lepor BIBREF21 , cider BIBREF22 , and meteor BIBREF23 ... Semantic Similarity (sim) ... readability and grammaticality"], "num_words": 4076, "gpt4_answer": "The paper considered a wide range of metrics, including word-based metrics (WOMs) such as ter, bleu, rouge, nist, lepor, cider, and meteor, as well as grammar-based metrics (GBMs) like Flesch Reading Ease score (re), characters per utterance (len), characters per word (cpw), words per sentence (wps), syllables per sentence (sps), syllables per word (spw), polysyllabic words per utterance (pol), polysyllabic words per word (ppw), misspellings (msp), and parsing score by the Stanford"}
{"id": "1707.06875.f11856814a57b86667179e1e275e4f99ff1bcad8", "question": "What NLG tasks were considered?", "answer": ["provide information about hotels and restaurants in San Francisco ... provides information about restaurants in Cambridge", "rnnlg ... TGen ...  lols", "utterance generation for spoken dialogue systems ... provide information about hotels and restaurants in San Francisco ... provides information about restaurants in Cambridge"], "num_words": 4076, "gpt4_answer": "The paper focuses on recent end-to-end, data-driven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data. The systems compared are rnnlg, TGen, and lols."}
{"id": "1708.07690.0bb97991fc297aa5aed784568de52d5b9121f920", "question": "what state of the art methods are compared to?", "answer": ["CLASSY04, ICSI, Submodular, DPP and RegSum.", "CLASSY04, ICSI, Submodular, DPP, RegSum"], "num_words": 2183, "gpt4_answer": "The paper compares the modified centroid-based method to several baseline and state-of-the-art methods on the DUC2004 dataset, including SumRepo, DPP model, and ICSI method."}
{"id": "1708.07690.7ba6330d105f49c7f71dba148bb73245a8ef2966", "question": "what are the performance metrics?", "answer": ["Rouge-1 recall, Rouge-2 recall, Rouge-4 recall", "Rouge-1, Rouge-2 and Rouge-4 recall"], "num_words": 2183, "gpt4_answer": "The performance metrics used are Rouge-1, Rouge-2, and Rouge-4 recall scores."}
{"id": "1708.07690.157de5175259d6f25db703efb299f948dae597b7", "question": "what is the original model they refer to?", "answer": ["BIBREF0 , BIBREF6", "Original centroid-based model by BIBREF5", "it represents sentences as bag-of-word (BOW) vectors with TF-IDF weighting and uses a centroid of these vectors to represent the whole document collection"], "num_words": 2183, "gpt4_answer": "The \"original\" centroid-based model referred to in the paper is the one that represents sentences as bag-of-word (BOW) vectors with TF-IDF weighting, uses a centroid vector to represent the whole document collection, and ranks sentences by their cosine similarity to the centroid vector. This model includes an anti-redundancy filter and the selection of top-ranking features."}
{"id": "1708.07690.cf3fab54b2b289b66e7dba4706c47a62569627c5", "question": "how are sentences selected prior to making the summary?", "answer": ["All words in the vocabulary are ranked by their value in the centroid vector. Then the ranked list of sentences is de-queued in decreasing order.", "Sentences are selected using 3 different greedy selection algorithms.", "Using three algorithms: N-first, N-best and New-TF-IDF."], "num_words": 2183, "gpt4_answer": "The paper explores different methods for selecting a small number of candidate sentences from each document before constructing the summary: selecting the first N sentences, choosing N best sentences based on cosine similarity to the centroid vector, or scoring sentences by the sum of the TF-IDF scores of the terms mentioned for the first time in the document."}
{"id": "1804.05253.000549a217ea24432c0656598279dbb85378c113", "question": "Do they evaluate only on English datasets?", "answer": ["The twitter dataset is English-only; no information for the reddit dataset is given", "Yes"], "num_words": 1947, "gpt4_answer": "Yes, they evaluate only on English datasets."}
{"id": "1804.05253.63d2e97657419a0185127534f4ff9d0039cb1a63", "question": "What type of frequency analysis was used?", "answer": ["Mean of occurrence per utterance and the standard deviation is calculated for every marker type; the means between each pair of types is compared via independent t-tests", "based on their occurrence", "mean of occurrence per utterance and the standard deviation (SD) of each marker"], "num_words": 1947, "gpt4_answer": "The frequency analysis involved investigating the occurrence of markers in the two platforms (Twitter and Reddit) by reporting the mean of occurrence per utterance and the standard deviation (SD) of each marker."}
{"id": "1804.05253.43f43b135109ebd1d2d1f9af979c64ce550b5f0f", "question": "What type of classifiers were used?", "answer": ["Support Vector Machines (SVM) classifier with linear kernel", "Support Vector Machines (SVM) classifier with linear kernel BIBREF16", "Support Vector Machines (SVM) classifier with linear kernel BIBREF16 "], "num_words": 1947, "gpt4_answer": "Support Vector Machines (SVM) classifier with a linear kernel was used."}
{"id": "1804.05253.e797634fa77e490783b349034f9e095ee570b7a9", "question": "Who annotated the Twitter and Reddit data for irony?", "answer": ["Authors of the tweets and reddit posts", "Twitter and Reddit users of the original data ", "collected using hashtags, such as #irony, #sarcasm, and #sarcastic"], "num_words": 1947, "gpt4_answer": "The Twitter data was collected using hashtags like #irony, #sarcasm, and #sarcastic, while the Reddit data was collected using the /s marker added by authors to indicate sarcasm."}
{"id": "1805.11598.475e698a801be0ad9e4f74756d1fff4fe0728009", "question": "what resources are combined to build the labeler?", "answer": ["a sequence of pretrained embeddings for the surface forms of the sentence tokens ... annotations for a single predicate ... CoNLL 2009 dataset", "multilingual word vectors ... concatenate a language ID vector to each multilingual word embedding", "multilingual word vectors ... training data across languages"], "num_words": 1989, "gpt4_answer": "The polyglot semantic role labeler is built by combining resources from a pair of languages in the CoNLL 2009 shared task dataset."}
{"id": "1805.11598.8246d1eee1482555d075127ac84f2e1d0781a446", "question": "what datasets were used?", "answer": ["CoNLL 2009 dataset", "semantic role labeling portion of the CoNLL-2009 shared task", "semantic role labeling portion of the CoNLL-2009 shared task BIBREF0"], "num_words": 1989, "gpt4_answer": "The authors used the semantic role labeling portion of the CoNLL-2009 shared task dataset, which includes seven languages: Catalan, Chinese, Czech, English, German, Japanese, and Spanish."}
{"id": "1805.11598.1ec0be667a6594eb2e07c50258b120e693e040a8", "question": "what is the monolingual baseline?", "answer": [" basic model adapts the span-based dependency SRL model of He2017-deepsrl", "For each of the shared task languages,  they produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection and trained 300-dimensional vectors then reduced them to 100 dimensions with principal component analysis for efficiency.", "biLSTM with pre-trained GloVe embeddings."], "num_words": 1989, "gpt4_answer": "The monolingual baseline is a model trained on a single language, using pretrained word embeddings as input, and adapted from the span-based dependency SRL model of He2017-deepsrl."}
{"id": "1805.11598.e3bafa432cd3e1225170ff04de2fdf1ede38c6ef", "question": "what languages are explored in this paper?", "answer": [" Catalan, Chinese, Czech, English, German, Japanese and Spanish", "Catalan ... Chinese ... Czech ... English ... German ... Japanese ... Spanish"], "num_words": 1989, "gpt4_answer": "The languages explored in this paper are Catalan, Chinese, Czech, English, German, Japanese, and Spanish."}
{"id": "1610.03955.dde29d9ea5859aa5a4bcd613dca80aec501ef03a", "question": "Does their model use MFCC?", "answer": ["No"], "num_words": 2866, "gpt4_answer": "Unanswerable"}
{"id": "1610.03955.9b1382b44dc69f7ee20acf952f7ceb1c3ef83965", "question": "What is the problem of session segmentation?", "answer": ["Retaining relevant contextual information from previous utterances. ", "To retain near and context relevant dialog session utterances and to discard far, irrelevant ones.", "ot all sentences in the current conversation session are equally important ...  irrelevant to the current context, and should not be considered when the computer synthesizes the reply"], "num_words": 2866, "gpt4_answer": "The problem of session segmentation in open-domain dialogue systems involves dividing conversation sessions into meaningful segments to retain relevant context utterances and eliminate irrelevant ones, improving the efficiency and effectiveness of the system."}
{"id": "1610.03955.3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f", "question": "What dataset do they use?", "answer": ["chatting corpus from DuMi", "chatting corpus from DuMi and conversation data from Douban forum", "real-world chatting corpus from DuMi ... unlabeled massive dataset of conversation utterances"], "num_words": 2866, "gpt4_answer": "The authors used a real-world chatting corpus from DuMi, a state-of-the-practice open-domain conversation system in Chinese, and an unlabeled massive dataset of conversation utterances from the Douban forum."}
{"id": "1610.03807.6157567c5614e1954b801431fec680f044e102c6", "question": "Was the filtering based on fluency and domain relevance done automatically?", "answer": ["Yes"], "num_words": 1928, "gpt4_answer": "Yes, the filtering based on fluency and domain relevance was done automatically using thresholds for domain relevance and fluency scores."}
{"id": "1610.03807.8ea4a75dacf6a39f9d385ba14b3dce715a47d689", "question": "How was domain relevance estimated?", "answer": ["For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$ (Eq. 7)\n\nwhere $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document.", "the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$ (Eq. 7)\n\nwhere $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document", "we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$"], "num_words": 1928, "gpt4_answer": "Domain relevance was estimated using a skip-gram model, with the seed question set as the in-domain data, and the domain relevance of expanded question q defined as the cosine similarity between the document embeddings of q and the in-domain data."}
{"id": "1610.03807.1e11e74481ead4b7635922bbe0de041dc2dde28d", "question": "How many hand-crafted templates did they have to make?", "answer": ["106 ... 163", "269", "269."], "num_words": 1928, "gpt4_answer": "They had to make 106 hand-crafted templates."}
{"id": "1610.03807.597d3fc9b8c0c036f58cea5b757d0109d5211b2f", "question": "How was the fluency measured?", "answer": ["$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count", "For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count", "For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count."], "num_words": 1928, "gpt4_answer": "The fluency was measured by training a 4-gram language model on gigaword and using the averaged language model score as an index."}
{"id": "1607.03895.f0404673085517eea708c5e91f32fb0f7728fa08", "question": "What data is used in this work?", "answer": ["Post-match interviews for tennis singles matches from ASAP Sport's website with match information from a dataset provided by Tennis-Data", "post-game interviews from ASAP Sport's website", "tennis post-match interview transcripts ... live text play-by-play commentaries"], "num_words": 2334, "gpt4_answer": "In this work, data is used from tennis post-match interview transcripts and corresponding match information, as well as live text play-by-play commentaries collected from the website Sports Mole."}
{"id": "1704.06960.d6b0c71721ed24ef1d9bd31ed3a266b0c7fc9b57", "question": "What dataset is used?", "answer": ["XKCD color dataset ... CaltechUCSD Birds dataset ... actions and messages generated by pairs of human Amazon Mechanical Turk workers playing the driving game", "XKCD color dataset; Caltech-UCSD Birds dataset; game data from Amazon Mechanical Turk workers ", "the XKCD color dataset ... the CaltechUCSD Birds dataset"], "num_words": 4444, "gpt4_answer": "The paper uses the XKCD color dataset, the CaltechUCSD Birds dataset with accompanying natural language descriptions, and a driving game dataset collected from pairs of human workers on Mechanical Turk."}
{"id": "1904.03670.63cdac43a643fc1e06da44910458e89b2c7cd921", "question": "How was the dataset collected?", "answer": ["crowdsourcing", "data was collected using crowdsourcing where speakers were recorded saying random ordered phrases for each intent twice", "using crowdsourcing"], "num_words": 2822, "gpt4_answer": "The dataset was collected using crowdsourcing, where each speaker was recorded saying each wording for each intent twice, with phrases presented in a random order."}
{"id": "2004.02363.37ac705166fa87dc74fe86575bf04bea56cc4930", "question": "what evaluation metrics were used?", "answer": ["Accuracy ...  MAE: Mean Absolute Error ", "MAE: Mean Absolute Error ... Accuracy$\\pm k$"], "num_words": 2181, "gpt4_answer": "The evaluation metrics used were Mean Absolute Error (MAE) and Accuracyk, where k represents the percentage of cases where the predicted price lies within k percent of the ground-truth."}
{"id": "2004.02363.90aba75508aa145475d7cc9a501bbe987c0e8413", "question": "What datasets are used?", "answer": ["Craigslist Bargaining dataset (CB)", "Craigslist Bargaining dataset (CB) "], "num_words": 2181, "gpt4_answer": "The paper uses the Craigslist Bargaining dataset (CB) for their experiments."}
{"id": "1601.03313.e6204daf4efeb752fdbd5c26e179efcb8ddd2807", "question": "how did they measure grammatical correctness?", "answer": ["Identify  POS tags for each sentence,  check whether one sentence from the corpus has the same sequence of POS tags. If the same POS sequence has been found, that points in a certain direction, if not found, the evaluation for that sentence is performed manually.", "They measure grammatical correctness by checking whether a sentence has the same sequence of POS tags.", "identify for each sentence of the speech its POS tags ... Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct. ... points in a certain direction ... evaluate those sentences manually"], "num_words": 7713, "gpt4_answer": "For evaluating the grammatical correctness, they identified the POS tags for each sentence in the speech and checked if any sentences in the entire corpus had the same sequence of POS tags."}
{"id": "1601.03313.95c3907c5e8f57f239f3b031b1e41f19ff77924a", "question": "how was quality of sentence transition measured?", "answer": ["Manually evaluated on scale 0 to 3.", "Manually, using the criterion score between 0 and 3.", "The quality of sentence transition was measured manually by checking how well do consecutive sentences connect"], "num_words": 7713, "gpt4_answer": "The quality of sentence transitions was measured through manual evaluation, where each generated speech was assessed based on a list of evaluation criteria, including sentence transitions, and assigned a score between 0 and 3."}
{"id": "1601.03313.b900122c7d6c2d6161bfca8a95eae11952d1cb58", "question": "what is the size of the dataset?", "answer": ["2771 speeches containing 50871 sentences", "3857 speech segments", "3857 speech segments from 53 US Congressional floor debates"], "num_words": 7713, "gpt4_answer": "The Convote dataset contains 3857 speech segments from 53 US Congressional floor debates."}
{"id": "1601.03313.5206b6f40a91fc16179829041c1139a6c6d91ce7", "question": "what manual evaluation is presented?", "answer": ["Manual evaluation of four evaluation criteria:   grammatical correctness,  sentence transitions, speech structure, and speech content. ", "The manual evaluation contains 4 criteria to check grammatical correctness, sentence transitions, speech structure, and speech content of the generated speech and assigning a score between 0 to 3 for each criterion", "generated speech is evaluated by assessing each of the criterion and assigning a score between 0 and 3 to it"], "num_words": 7713, "gpt4_answer": "The manual evaluation consists of assessing a list of criteria and assigning a score between 0 and 3 to each criterion, with a maximum total score of 12."}
{"id": "1910.01108.48ff9645a506aa2c17810d2654d1f0f0d9e609ee", "question": "What downstream tasks are analyzed?", "answer": ["General Language Understanding ... question answering task (SQuAD v1.1 - BIBREF14) ... classification task (IMDb sentiment classification - BIBREF13)", "a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14).", "sentiment classification ... question answering"], "num_words": 1814, "gpt4_answer": "Downstream tasks analyzed include IMDb sentiment classification and SQuAD v1.1 question answering task."}
{"id": "1910.01108.84ee6180d3267115ad27852027d147fb86a33135", "question": "How much time takes the training of DistilBERT?", "answer": ["90 hours", "Unanswerable", "on 8 16GB V100 GPUs for approximately 90 hours"], "num_words": 1814, "gpt4_answer": "DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours."}
{"id": "1707.06519.c7ffef8bf0100eb6148bd932d0409b21759060b1", "question": "Which datasets do they use?", "answer": ["LibriSpeech ... GlobalPhone", "LibriSpeech corpus ... GlobalPhone corpus", "LibriSpeech corpus BIBREF46 ... GlobalPhone corpus BIBREF47"], "num_words": 3598, "gpt4_answer": "The authors used two corpora across five languages: the LibriSpeech corpus (English) and the GlobalPhone corpus, which includes French, German, Czech, and Spanish."}
{"id": "1707.06519.1ff0ffeb2d0b2e150abdb2f559d8b31f4dd8aa2c", "question": "How do they compare representations performance obtained from a naive encoder versus ones learned from large amount of source language data?", "answer": ["MAP ... MAP results on large testing database (250K segments)", "They compare MAP performance of query-by-example STD using representations obtained from naive encoder and their method", "They compare retrieval performance in MAP."], "num_words": 3598, "gpt4_answer": "The representations from the Sequence-to-sequence Autoencoder (SASA) generally achieve higher Mean Average Precision (MAP) values than the naive encoder (NENE) in the query-by-example Spoken Term Detection (STD) task, with the highest MAP score for SASA being 0.881 compared to 0.490 for NENE."}
{"id": "1707.06519.3cc0d773085dc175b85955e95911a2cfaab2cdc4", "question": "Which pairs of languages do they consider similar enough to capture phonetic structure?", "answer": ["English paired with any of the following: French, German, Czech, Spanish.", "English, German and French", "German and French"], "num_words": 3598, "gpt4_answer": "English, German, and French are considered more similar compared to Czech and Spanish."}
{"id": "1710.07394.dfd07a8e2de80c3a8d075a0f400fb13a1f1d4c60", "question": "Did they try Roberta?", "answer": ["No", "Unanswerable"], "num_words": 4532, "gpt4_answer": "Unanswerable"}
{"id": "1912.06905.2e70d25f14357ad74c085a9454a2ce33bb988a6f", "question": "What are their results on this task?", "answer": [" BiLSTM based framework and the linear classifier reaches a 97.97% accuracy ... SVM classifier reaches a remarkable 98.11% accuracy", "98.11% accuracy with a 0.4% improvement upon the benchmark model", "F1 score of 97.97 for a linear classifier and 98.11 for a SVM classifier"], "num_words": 2815, "gpt4_answer": "The model with the BiLSTM based framework and the linear classifier reached a 97.97% accuracy with a 1.1% improvement upon the benchmark model. The second model with the SVM classifier reached a 98.11% accuracy with a 0.4% improvement upon the benchmark model."}
{"id": "1912.06905.de84972c5d1bbf664d0f8b702fce5f161449ec23", "question": "How is the text segmented?", "answer": ["They simply split document in chunks, get embedding for each chunk and train BiLSTM models with embeddings.", "Unanswerable", "dividing documents into chunks before processing"], "num_words": 2815, "gpt4_answer": "The text is segmented by splitting the lengthy legal documents into smaller chunks before processing them."}
{"id": "1708.03312.bab4e8881f4d75e266bce6fbfa4c3bcd3eacf30f", "question": "what are the state of the art models?", "answer": ["The character-aware neural language model ... Hierarchical attention networks ... FastText", "The character-aware neural language model BIBREF1 ... Hierarchical attention networks BIBREF10 ... FastText BIBREF11 ", "character-aware neural language model BIBREF1 ... Hierarchical attention networks BIBREF10 ... FastText BIBREF11"], "num_words": 2606, "gpt4_answer": "The state-of-the-art models mentioned in the paper include the character-aware neural language model, bi-directional RNN with word embeddings, hierarchical attention networks, and FastText."}
{"id": "1908.05731.11dd2913d1517a1d47b367acb29fe9d79a9c95d1", "question": "How many parameters does their noisy channel model have?", "answer": ["Unanswerable"], "num_words": 2319, "gpt4_answer": "Unanswerable"}
{"id": "1908.05731.8701ec7345ccc2c35eca4e132a8e16d58585cd63", "question": "Which language pairs do they evaluate on?", "answer": ["En-De ... De-En ... Zh-En ... Englsh-Russian and Russian-English", "English-German ... Chinese-English", "English-German; Chinese-English; German-English"], "num_words": 2319, "gpt4_answer": "They evaluate on four language pairs: English-German (En-De), German-English (De-En), Chinese-English (Zh-En), and IWSLT'14 De-En."}
{"id": "1808.10059.d20fd6330cb9d03734e2632166d6c8f780359a94", "question": "How large the improvement margin is?", "answer": ["+7.24 for train size of 2000, +11.03 for train size of 1000, and +14.67 for train size of 500", "+7.24, +11.03, +14.67, +5.07 for 2000, 1000, 500 and zero training instances respectively", "Average F1 improvement of 5.07"], "num_words": 3018, "gpt4_answer": "The improvement margin is 7.24% in absolute F1-score when training with 2000 instances per domain and 14.57% when only 500 training instances are used."}
{"id": "1911.12848.1a1d94c981c58e2f2ee18bdfc4abc69fd8f15e14", "question": "Which languages do they explore?", "answer": ["Irish, Gujarati, Hindi, Arabic, English, Spanish, French, German, Tamil, Bengali, Odia, Marathi, Telugu, Hinglish", "Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil", "Irish, Tibetian, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil, Hinglish, Bengali,Arabic, French, German, Odia"], "num_words": 6573, "gpt4_answer": "They explore indigenous languages such as Hindi, Marathi, Gujarati, Telugu, Tamil, and code-mix languages like Hinglish (a combination of English and Hindi)."}
{"id": "1911.01770.5d790459b05c5a3e6f1e698824444e55fc11890c", "question": "What are two baseline methods?", "answer": ["Answer with content missing: (Table1 merged with Figure 3) Joint Neural\nEmbedding (JNE) and AdaMine", "JNE and AdaMine", "Joint Neural Embedding (JNE)\nAdaMine"], "num_words": 3169, "gpt4_answer": "Two baseline methods are AdaMine (BIBREF17) and the method proposed by BIBREF19."}
{"id": "1911.01770.1ef6471cc3e1eb10d2e92656c77020ca1612f08e", "question": "How does model compare to the baselines?", "answer": ["Answer with content missing: (Table1 part of Figure 3):\nProposed vs Best baseline result\n- Median Rank: 2.9 vs 3.0 (lower better)\n- Rank 1 recall: 34.6 vs 33.1 (higher better)", "The model improved over the baseline with scores of 34.6, 66.0 and 76.6 for Recall at 1, 5 and 10 respectively", "The model outperforms the two baseline models,  since it has higher recall values. "], "num_words": 3169, "gpt4_answer": "The proposed model performs similarly to the baseline implementation of BIBREF17 in the recipe retrieval task, but with increased efficiency regarding training time and a more flexible approach for dataset alterations."}
{"id": "1903.10548.d976c22e9d068e4e31fb46e929023459f8290a63", "question": "Are the two paragraphs encoded independently?", "answer": ["Yes"], "num_words": 712, "gpt4_answer": "No, the two paragraphs are not encoded independently."}
{"id": "1908.06893.a1ac4463031bbc42c80893b57c0055b860f12e10", "question": "What is their baseline?", "answer": ["Dada engine", "Dada engine BIBREF6", "synthetic emails generated by Dada engine"], "num_words": 5024, "gpt4_answer": "The baseline in this paper is synthetic emails generated by the Dada Engine, which uses manually constructed rules to pre-define the structure of the fake emails."}
{"id": "1908.06893.3216dfc233be68206bd342407e2ba7da3843b31d", "question": "Is human evaluation of the malicious content performed?", "answer": ["No"], "num_words": 5024, "gpt4_answer": "No, the paper does not mention human evaluation of the malicious content."}
{"id": "1908.06893.4f57ac24f3f4689a2f885715cd84b7d867fe3f12", "question": "Do they compare to previous work?", "answer": ["Yes"], "num_words": 5024, "gpt4_answer": "Yes, the paper compares their system's output against a small set of automatically generated emails provided by the authors of a previous work that used the Dada Engine and manually generated grammar rules."}
{"id": "2002.12699.46146ff3ef3430924e6b673a28df96ccb869dee4", "question": "by how much did their model outperform the other models?", "answer": ["In terms of macro F1 score their model has 0.65 compared to 0.58 of best other model.", "Their model outperforms other models by 0.01 micro F1 and 0.07 macro F1", "Unanswerable"], "num_words": 5179, "gpt4_answer": "The CNN model outperformed the other models with the highest macro average F1-score of 0.65."}
{"id": "1901.01590.3499d5feeb3a45411d8e893516adbdc14e72002a", "question": "What is reordering in the context of the paper?", "answer": ["Changing the word order of the translation so it is in the right order of the target language.", "Re-arranging translated words so that they are in the correct order in the target language", "changing the order of the word-by-word translation so it matches the target language"], "num_words": 2445, "gpt4_answer": "Reordering in the context of the paper refers to correcting the word order in the translated sentences, as word-by-word translation may not produce the correct word order in the target language."}
{"id": "1901.01590.d0048ef1cba3f63b5d60c568d5d0ba62ac4d7e75", "question": "How does the paper use language model for context aware search?", "answer": ["It is used to calculate the probability of a possible target word given the history of target words that come before it.", "combining a language model (LM) with cross-lingual word embedding ... Let $f$ be a source word in the current position and $e$ a possible target word. Given a history $h$ of target words before $e$ , the score of $e$ to be the translation of $f$ would be:\n\n$ L(e;f,h) = \\lambda _\\text{emb}\\log q(f,e) + \\lambda _\\text{LM}\\log p(e|h) $\n\nHere, $q(f,e)$ is a lexical score defined as:\n\n$ q(f,e) = \\frac{d(f,e) + 1}{2} $\n\nwhere $d(f,e) \\in [-1,1]$ is a cosine similarity between $f$ and $e$ . ... Accumulating the scores per position, we perform a beam search to allow only reasonable translation hypotheses.", "the language model is combined with cross-lingual word embedding to obtain context information in the word-by-word translation"], "num_words": 2445, "gpt4_answer": "The paper integrates context information into word-by-word translation by combining a language model (LM) with cross-lingual word embedding and performs a beam search to allow only reasonable translation hypotheses."}
{"id": "1603.03876.47ffc9811b037613c9c4d1ec1e4f13c08396ed1c", "question": "What datasets are used?", "answer": ["PDTB 2.0", "PDTB 2.0 "], "num_words": 4271, "gpt4_answer": "The datasets used are the Penn Discourse Treebank 2.0 (PDTB 2.0) corpus, which contains discourse annotations over 2,312 Wall Street Journal articles."}
{"id": "1708.01065.4e63454275380787ebd0e38aa885977332ab33af", "question": "what evaluation metrics were used?", "answer": ["F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4", "ROUGE", "ROUGE-1 ... ROUGE-2  ... ROUGE-SU4"], "num_words": 3314, "gpt4_answer": "The evaluation metric used is ROUGE score with standard options, reporting F-measures of ROUGE-1, ROUGE-2, and ROUGE-SU4."}
{"id": "1708.01065.dfaeb8faf04505a4178945c933ba217e472979d8", "question": "what is the source of their dataset?", "answer": [" Google News ... Twitter", "Google News ... follow the related tags on Twitter ... scan the list of event archives on the Web, such as earthquakes happened in 2017", "Topics were taken from category names in Google News, tags on Twitter, event archives on the Web. News articles were taken from news websites."], "num_words": 3314, "gpt4_answer": "The dataset was created by the authors themselves by selecting topics, collecting news documents and reader comments, and employing experts for aspect annotation, summary writing, and scrutinizing."}
{"id": "1708.01065.342ada55bd4d7408e1fcabf1810b92d84c1dbc41", "question": "by how much did the performance improve?", "answer": ["The proposed RAVAESum method improves from 0.001 to  0.059  Rouge1.", "They improved by 0.007 on average across R-1, R-2, R-SU4 over the best baseline.", "Unanswerable"], "num_words": 3314, "gpt4_answer": "The experimental results show that RAVAESum significantly outperforms RA-Sparse, but the exact improvement percentage is not provided in the text."}
{"id": "1708.01065.86d1c990c1639490c239c3dbf5492ecc44ab6652", "question": "how many experts were there?", "answer": ["5", "Each topic is assigned to 4 experts"], "num_words": 3314, "gpt4_answer": "There were 4 experts assigned to each topic for summary writing, and an additional expert for scrutinizing the summaries."}
{"id": "1708.01065.b065c2846817f3969b39e355d5d017e326d6f42e", "question": "what is the size of the data collected?", "answer": [" The dataset contains 19k annotated aspect facets, 45 topics, 6 predefined categories, 450 news document, 180 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words", "19000", "45 topics from those 6 predefined categories ... On average, each topic contains 215 pieces of comments and 940 comment sentences. ... 19k annotated aspect facets"], "num_words": 3314, "gpt4_answer": "The dataset contains 45 topics, each with 10 news documents and 4 model summaries. On average, each topic has 215 pieces of comments and 940 comment sentences."}
{"id": "1708.01065.9536e4a2455008007067f23cc873768374c8f664", "question": "did they use a crowdsourcing platform?", "answer": ["No", "Unanswerable"], "num_words": 3314, "gpt4_answer": "Unanswerable"}
{"id": "1708.01065.cfa44bb587b0c05906d8325491ca9e0f024269e8", "question": "how was annotation conducted?", "answer": ["Each topic is assigned to 4 experts to conduct the summary writing in two phases: facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets.", "Experts identified aspect facets and wrote summaries.", "employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing"], "num_words": 3314, "gpt4_answer": "Annotation was conducted by assigning each topic to 4 experts in journalism who performed aspect facet identification and summary generation based on the annotated aspect facets."}
{"id": "1708.01065.b3dc9a35e8c3ed7abcc4ca0bf308dea75be9c016", "question": "what does their dataset contain?", "answer": ["45 topics from those 6 predefined categories ... Each topic contains 10 news documents and 4 model summaries ... On average, each topic contains 215 pieces of comments and 940 comment sentences ... Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. ... dataset contains 19k annotated aspect facets", "topics ... categories ... news documents ... model summaries ...  comments ... annotated aspect facets"], "num_words": 3314, "gpt4_answer": "The dataset contains 45 topics from 6 predefined categories, each topic having 10 news documents, 4 model summaries, and an average of 215 pieces of comments and 940 comment sentences."}
{"id": "1905.07562.693cdb9978749db04ba34d9c168e71534f00a226", "question": "Do they report results only on English data?", "answer": ["Unanswerable", "Yes"], "num_words": 3812, "gpt4_answer": "Unanswerable"}
{"id": "1905.07562.71fd0efea1b441d86d9a75255815ba3efe09779b", "question": "How do the authors measure the extent to which LGI has learned the task?", "answer": ["classify figures in various morphology with correct identity (accuracy = 72.7%) ... demonstrates that LGI can understand the verbs and nouns", "precision ... accuracy"], "num_words": 3812, "gpt4_answer": "The authors measure the extent to which LGI has learned the task by its ability to correctly interact with the vision system and perform various tasks using the learned syntaxes."}
{"id": "1905.07562.fb9e333a4e5d5141fe8e97b24b8f7e5685afbf09", "question": "Which 8 tasks has LGI learned?", "answer": ["move left ... move right ... this is  ... the size is big/small ... give me a  ... enlarge/shrink ... rotate ", "move left ... move right ... this is  ... the size is big/small ... the size is not small/big ... give me a  ... enlarge/shrink ... rotate ", "move left ... move right ... this is  ... the size is big/small ... the size is not small/big ... give me a  ... enlarge/shrink ... rotate "], "num_words": 3812, "gpt4_answer": "LGI has learned eight tasks: move left, move right, this is, the size is big/small, the size is not small/big, give me a, enlarge/shrink, and rotate."}
{"id": "1905.07562.cb029240d4dedde74fcafad6a46c1cfc2621b934", "question": "In what was does an LSTM mimic the prefrontal cortex?", "answer": ["It combines language and vision streams similar to the human prefrontal cortex.", "Unanswerable", "the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output"], "num_words": 3812, "gpt4_answer": "An LSTM mimics the human IntraParietal Sulcus (IPS) in the language subsystem of the LGI network to extract quantity information from language text."}
{"id": "1905.07562.11a8531699952f5a2286a4311f0fe80ed1befa1e", "question": "In what way does an LSTM mimic the intra parietal sulcus?", "answer": [" mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output", "It mimics the number processing functionality of human Intra-Parietal Sulcus.", "textizer to produce text symbols output ... extract the quantity information from language text "], "num_words": 3812, "gpt4_answer": "An LSTM mimics the intra parietal sulcus by extracting quantity information from the input texts in the language subsystem of the LGI network."}
{"id": "1905.07562.bcf222ad4bb537b01019ed354ea03cd6bf2c1f8e", "question": "How do the authors define imagination, or imagined scenarios?", "answer": [" transmitting the output image from the decoder to the encoder, an imagination loop is formed, which enables the continual operation of a human-like thinking process involving both language and image", "Ability to change the answering contents by considering the consequence of the next few output sentences.", "Continual thinking requires the capacity to generate mental imagination guided by language, and extract language representations from a real or imagined scenario"], "num_words": 3812, "gpt4_answer": "The authors define imagination as the capacity to generate mental images guided by language and extract language representations from a real or imagined scenario."}
{"id": "1911.12893.af45ff2c4209f14235482329d0729864fb2bd4b0", "question": "Which classifiers did they experiment with?", "answer": ["Long Short Term Memory (LSTM) language model ... logistic regression model", "logistic regression classifier", "logistic regression classifier ... trainable embedding layer, three layers of a stacked recurrent neural network, and a softmax classifier"], "num_words": 3649, "gpt4_answer": "The authors experimented with a logistic regression classifier."}
{"id": "1911.12893.d2451d32c5a11a0eb8356a5e9d94a9231b59f198", "question": "Is the distribution of the edits uniform across all languages?", "answer": ["No", "Yes"], "num_words": 3649, "gpt4_answer": "No, the distribution of languages is heavily skewed towards English, although the dataset includes a diverse set of other languages."}
{"id": "1911.12893.90dde59e1857a0d2b1ee4615ab017fee0741f29f", "question": "How did they identify what language the text was?", "answer": [" We used NanigoNet, a language detector based on GCNNs", "NanigoNet", "used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages"], "num_words": 3649, "gpt4_answer": "They used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) that supports human languages as well as programming languages."}
{"id": "1911.12893.811b67460e65232b8f363dc3f329ffecdfcc4ab2", "question": "Which repositories did they collect from?", "answer": ["GitHub repositories", "Has at least one pull request or pull request review comment event between November 2017 and September 2019,\n\nHas 50 or more starts,\n\nHas a size between 1MB and 1GB, and\n\nHas a permissive license.", "Has at least one pull request or pull request review comment event between November 2017 and September 2019, ... 50 or more starts ... size between 1MB and 1GB ... permissive license"], "num_words": 3649, "gpt4_answer": "They collected from 43,462 eligible repositories on GitHub that met specific criteria such as size, license, and quality."}
{"id": "1911.12893.68aa460ad357b4228b16b31b2cbec986215813bf", "question": "Which three features do they use?", "answer": ["Ratio of the target perplexity over the source calculated by a language model ... Normalized edit distance between the source and the target ... Binary variable indicating whether the edit purely consists of changes in numbers", "mechanical, spell, and grammatical edits"], "num_words": 3649, "gpt4_answer": "The three features used are: 1) Ratio of the target perplexity over the source calculated by a language model, 2) Normalized edit distance between the source and the target, and 3) Binary variable indicating whether the edit purely consists of changes in numbers."}
{"id": "1911.12893.4542b162a5be00206fd14570898a7925cb267599", "question": "Which languages are covered in the corpus?", "answer": ["English,  Chinese (smpl.),  Japanese,  Russian,  French,  German,  Portuguese,  Spanish,  Korean , Hindi", "English, Chinese, Japanese, Russian, French, German, Portugese, Spanish, Korean, Hindi and Others", "the top 10 languages are English, simplified Chinese, Japanese, Russian, French, German, Portuguese, Spanish, Korean and Hindi"], "num_words": 3649, "gpt4_answer": "The GitHub Typo Corpus covers more than 15 languages, with the top three languages being English, Simplified Chinese, and Japanese."}
{"id": "1910.07973.a17fc7b96753f85aee1d2036e2627570f4b50c30", "question": "Do they report results only on English data?", "answer": ["Unanswerable"], "num_words": 1971, "gpt4_answer": "Yes, the results are reported only on English data."}
{"id": "1910.07973.c6170bb09ba2a416f8fa9b542f0ab05a64dbf2e4", "question": "What is the BM25 baseline?", "answer": ["Unanswerable"], "num_words": 1971, "gpt4_answer": "The BM25 baseline is a ranking function used in information retrieval systems to rank matching documents based on their relevance to a given search query."}
{"id": "1910.07973.fe080c6393f126b55ae456b81133bfc8ecbe85c2", "question": "Which BERT layers were combined to boost performance?", "answer": [" the top and bottom layer of the BERT fine-tuned on SNLI dataset", "Top and bottom layers", "combining the top and bottom layer embeddings"], "num_words": 1971, "gpt4_answer": "Combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance."}
{"id": "1910.07973.53a8c3cf22d6bf6477bc576a85a83d8447ee0484", "question": "Which NLI data was used to improve the quality of the embeddings?", "answer": ["MNLI ... SNLI", "MNLI BIBREF11 ... SNLI", "Two natural language inference datasets, MNLI BIBREF11 and SNLI"], "num_words": 1971, "gpt4_answer": "SNLI dataset was used to improve the quality of the embeddings."}
{"id": "1910.07973.3a33512d253005ac280ee9ca4f9dfa69aa38d48f", "question": "Which four QA datasets are examined?", "answer": ["(1) WikiPassageQA BIBREF13 ... (2) InsuranceQA (version 1.0) BIBREF14 ... (3) Quasar-t BIBREF15 ... (4) SearchQA BIBREF16", "WikiPassageQA ... InsuranceQA  ... Quasar-t  ... SearchQA", "WikiPassageQA ... InsuranceQA ... Quasar-t ... SearchQA"], "num_words": 1971, "gpt4_answer": "The four QA datasets examined are WikiPassageQA, InsuranceQA (version 1.0), Quasar-t, and SearchQA."}
{"id": "1910.07973.f7f2968feb28c2907266c892f051ae9f7d6286e6", "question": "Which two tasks from SentEval are the sentence embeddings evaluated against?", "answer": ["Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity ... probe sentence-level linguistic phenomena", "Unanswerable"], "num_words": 1971, "gpt4_answer": "The sentence embeddings are evaluated on downstream and probing tasks from SentEval."}
{"id": "1603.08868.38289bd9592db4d3670b65a0fef1fe8a309fee61", "question": "what classifiers did they train?", "answer": ["(1) a multinomial logistic regression model with ridge estimator ... (2) a multilayer perceptron ... (3) a support vector machine learner, Sequential Minimal Optimization (SMO) ... (4) a decision tree (J48)", "a multinomial logistic regression model with ridge estimator ... a multilayer perceptron ... a support vector machine learner ... Sequential Minimal Optimization (SMO) ... a decision tree (J48)", "multinomial logistic regression model with ridge estimator ... multilayer perceptron ... support vector machine learner, Sequential Minimal Optimization ... decision tree"], "num_words": 3324, "gpt4_answer": "They trained four classifiers: (1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner (Sequential Minimal Optimization), and (4) a decision tree (J48)."}
{"id": "1603.08868.cb7a00233502c4b7801d34bc95d6d22d79776ae8", "question": "what dataset did they use?", "answer": ["a subset of COCTAILL", "a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1)", "subset of COCTAILL"], "num_words": 3324, "gpt4_answer": "The dataset used is a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1)."}
{"id": "1603.08868.35d2eae3a7c9bed54196334a09344591f9cbb5c8", "question": "what combination of features helped improve the classification?", "answer": ["Using all the 61 features helped them improve the classification", "a combination of all features for the document level", "length-based, lexical, morphological, syntactic and semantic features"], "num_words": 3324, "gpt4_answer": "A combination of different features, compared to using lexical features alone, resulted in a 7% improvement in classification accuracy at the sentence level."}
{"id": "1603.08868.a70656fc61bf526dd21db7d2ec697b29a5a9c24e", "question": "what linguistics features did they apply?", "answer": ["Sentence length\nModal verbs to verbs\nAverage token length\nParticle IncSc\nExtra-long words\nSG pronoun IncSc\nNumber of characters\nPunctuation IncSc\nLIX\nSubjunction IncSc\nS-verb IncSc\nA1 lemma IncSc\nS-verbs to verbs\nA2 lemma IncSc\nAdjective IncSc\nB1 lemma IncSc\nAdjective variation\nB2 lemma IncSc\nAdverb IncSc\nC1 lemma IncSc\nAdverb variation\nC2 lemma IncSc\nNoun IncSc\nDifficult word IncSc\nNoun variation\nDifficult noun and verb IncSc\nVerb IncSc\nOut-of-Kelly IncSc\nVerb variation\nMissing lemma form IncSc\nNominal ratio\nAvg. Kelly log frequency\nNouns to verbs\nFunction word IncSc\nAverage dependency length\nLexical words to non-lexical words\nDependency arcs longer than\nLexical words to all tokens\nLongest dependency from root node\nNeuter gender noun IncSc\nRatio of right dependency arcs\nCon- and subjunction IncSc\nRatio of left dependency arcs\nPast participles to verbs\nModifier variation\nPresent participles to verbs\nPre-modifier IncSc\nPast verbs to verbs\nPost-modifier IncSc\nPresent verbs to verbs\nSubordinate IncSc\nSupine verbs to verbs\nRelative clause IncSc\nRelative structure IncSc\nPrepositional complement IncSc\nBilog type-token ratio\nSquare root type-token ratio\nAvg. nr. of senses per token\nPronouns to nouns\nNoun senses per noun\nPronouns to prepositions", "length-based ... lexical ... morphological ... syntactic ... semantic", "lexical, morphological, syntactic and semantic features"], "num_words": 3324, "gpt4_answer": "The linguistic features applied in this study can be divided into five subgroups: length-based, lexical, morphological, syntactic, and semantic features."}
{"id": "1603.08868.f381b0ef693243d67657f6c34bbce015f6b1fd07", "question": "what is the state of the art in English?", "answer": ["BIBREF9 , BIBREF12", "BIBREF9 ... BIBREF12"], "num_words": 3324, "gpt4_answer": "The state of the art in English for predicting linguistic complexity achieves comparable performance to the model presented in this paper, with an F-score of 0.8."}
{"id": "1910.01340.c176eb1ccaa0e50fb7512153f0716e60bf74aa53", "question": "Are results reported only on English data?", "answer": ["Yes"], "num_words": 3685, "gpt4_answer": "Yes, the results are reported only on English data."}
{"id": "1910.01340.e0b54906184a4ad87d127bed22194e62de38222b", "question": "What type of model were the features used in?", "answer": ["Logistic Regression classifier", "Random Selection ... Majority Class ... bag-of-words ... Tweet2vec BIBREF32", "character-based Bidirectional Gated Recurrent neural network"], "num_words": 3685, "gpt4_answer": "The features were used in a Logistic Regression model."}
{"id": "1910.01340.1f8044487af39244d723582b8a68f94750eed2cc", "question": "What unsupervised approach was used to deduce the thematic information?", "answer": [" Latent Dirichlet Allocation (LDA)", "Latent Dirichlet Allocation (LDA) topic modeling", "Latent Dirichlet Allocation (LDA) topic modeling algorithm BIBREF8"], "num_words": 3685, "gpt4_answer": "Latent Dirichlet Allocation (LDA) topic modeling algorithm was used to deduce the thematic information in an unsupervised way."}
{"id": "1910.01340.595fe416a100bc7247444f25b11baca6e08d9291", "question": "What profile features are used?", "answer": ["Part-of-speech tags (POS) ... syntactic dependency relations ... count of special characters ... consecutive characters and letters ... URLs ... hashtags ... users' mentions ... uppercase ratio ... tweet length", "bag of stopwords ... Part-of-speech tags ... syntactic dependency relations ... count of special characters ... consecutive characters and letters ... URLs ... hashtags ... users' mentions ... uppercase ratio ... tweet length", "bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL) ... count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions ... uppercase ratio and the tweet length"], "num_words": 3685, "gpt4_answer": "The profile features used in this paper include Native Language Identification (NLI) and Stylistic features. NLI features consist of bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL). Stylistic features include the count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions, uppercase ratio, and tweet length."}
{"id": "1910.01340.1f011fa772ce802e74eda89f706cdb1aa2833686", "question": "What textual features are used?", "answer": ["eight Plutchik's emotions ... positive and negative ... list of bad and sexual words from BIBREF10 ... belief, denial, doubt, fake, knowledge, negation, question, and report ... assertives verbs BIBREF14, bias BIBREF15, factive verbs BIBREF16, implicative verbs BIBREF17, hedges BIBREF18, report verbs BIBREF15 ... pronoun, anx, cogmech, insight, cause, discrep, tentat, certain, inhib, incl ... care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation", "emotion features, bad and sexual language features, stance towards some topics, bias cues, linguistic features from LIWC and morality cues"], "num_words": 3685, "gpt4_answer": "The textual features used include thematic information, emotions, sentiment, bad & sexual cues, stance cues, bias cues, LIWC, morality, native language identification (NLI), and stylistic features."}
{"id": "1709.09749.181027f398a6b79b1ba44d8d41cc1aba0d6f5212", "question": "what other representations do they compare with?", "answer": ["Paragraph Vector ... word2vec averagings", "Word2vec averaging (public release 300d), word2vec averaging (academic corpus), Paragraph Vector", "word2vec averaging ... Paragraph Vector"], "num_words": 2027, "gpt4_answer": "They compare KeyVec with Paragraph Vector, word2vec averaging (using public release and their own academic corpus-trained versions)."}
{"id": "1709.09749.ab097db03652b8b38edddc074f23e2adf9278cba", "question": "how many layers are in the neural network?", "answer": ["Unanswerable"], "num_words": 2027, "gpt4_answer": "The paper does not explicitly mention the number of layers in the neural network."}
{"id": "1709.09749.5d4190403eb800bb17eec71e979788e11cf74e67", "question": "what empirical evaluations performed?", "answer": [" we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.", "document retrieval ... document clustering"], "num_words": 2027, "gpt4_answer": "Empirical evaluations were performed on two text understanding tasks: document retrieval and document clustering."}
{"id": "1709.09749.56d41e0fcc288c1e65806ae77097d685c83e22db", "question": "which document understanding tasks did they evaluate on?", "answer": [" document retrieval ... document clustering", " document retrieval and document clustering", "document retrieval ... document clustering"], "num_words": 2027, "gpt4_answer": "They evaluated KeyVec on two text understanding tasks: document retrieval and document clustering."}
{"id": "1709.09749.1237b6fcc64b43901415f3ded17cc210a54ab698", "question": "what dataset was used?", "answer": ["669 academic papers published by IEEE", "669 academic papers published by IEEE ... 850 academic papers", "For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation."], "num_words": 2027, "gpt4_answer": "The dataset used in the experiments contained 669 academic papers published by IEEE."}
{"id": "2003.09244.31cba86bc45970337ba035ecf36d8954a9a5206a", "question": "What private companies are members of consortium?", "answer": ["Creditinfo, Grammatek, Mideind and Tiro", "Crediyinfo, Grammatek, \nMideind,\nTiro", "The rni Magnsson Instit. for Icelandic Studies, Reykjavik University (RU),  University of Iceland (UI),  RV, Creditinfo, The Association of the Visually Impaired, Grammatek, Mieind. Tiro"], "num_words": 5848, "gpt4_answer": "The paper does not provide specific names of the private companies that are members of the consortium."}
{"id": "2003.09244.3a25f82512d56d9e1ffba72f977f515ae3ba3cca", "question": "Does programme plans gathering and open sourcing some large dataset for Icelandic language?", "answer": ["Yes"], "num_words": 5848, "gpt4_answer": "Yes, the programme plans to gather and make freely available large datasets for the Icelandic language under open-source licensing."}
{"id": "2003.09244.b59f3a58939f7ac007d3263a459c56ebefc4b49a", "question": "What concrete software is planned to be developed by the end of the programme?", "answer": ["A lot of new software will be developed in all areas of the programme, some will be extensions of already available Greynir software.", "IceNLP ... Greynir  ... Nefnir  ... ABLTagger ... a flexible lexicon acquisition tool ... A punctuation system for Icelandic  ...  open source correction system ... a statistical phrase-based MT system  ...  a bidirectional LSTM model using the neural translation system OpenNMT ... a system based on an attention-based neural network ... An API and a web user interface"], "num_words": 5848, "gpt4_answer": "By the end of the programme, the plan is to develop and publish open-source software in the areas of speech recognition, speech synthesis, machine translation, and spell and grammar checking."}
{"id": "2003.09244.b4b7333805cb6fdde44907747887a971422dc298", "question": "What other national language technology programs are described in the paper?", "answer": ["Netherlands ... Spain ... Estonian", "STEVIN programme in the Netherlands ...  Plan for the Advancement of Language Technology in Spain ... Estonian LT programmes", "STEVIN programme in the Netherlands ... Plan for the Advancement of Language Technology in Spain ... Estonian LT programmes"], "num_words": 5848, "gpt4_answer": "The paper describes three national language technology programmes: the STEVIN programme in the Netherlands, the Plan for the Advancement of Language Technology in Spain, and the Estonian language technology programmes."}
{"id": "2003.09244.871f7661f5a3da366b0b5feaa36f54fd3dedae8e", "question": "When did language technology start in Iceland?", "answer": ["Around year 2000", "in the year 2000", "in the year 2000 ... couple of LT resources and products were developed in the years leading up to that"], "num_words": 5848, "gpt4_answer": "Language technology in Iceland is usually considered to have begun around the turn of the century."}
{"id": "1709.04491.3fafde90eebc1c00ba6c3fb4c5b984009393ce7f", "question": "what was their accuracy result?", "answer": ["Unanswerable"], "num_words": 2462, "gpt4_answer": "The paper does not provide a specific accuracy result."}
{"id": "1709.04491.e6bc11bd6cfd4b2138c29602b9b56fc5378a4293", "question": "what domain do the opinions fall under?", "answer": ["computers ... wireless routers ... speakers", "computers ... wireless routers ... speakers ", "computers, wireless routers, and speakers"], "num_words": 2462, "gpt4_answer": "The opinions in the dataset fall under three domains: computers, wireless routers, and speakers."}
{"id": "1709.04491.90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521", "question": "what was the baseline?", "answer": ["Unanswerable"], "num_words": 2462, "gpt4_answer": "Unanswerable"}
{"id": "1709.04491.4748a50c96acb1aa03f7efd1b43376c193b2450a", "question": "what dataset was used?", "answer": ["Bing Liu's dataset", "SNAP Amazon Dataset  ... Bing Liu's dataset", "SNAP Amazon Dataset BIBREF19 ... Bing Liu's dataset BIBREF20"], "num_words": 2462, "gpt4_answer": "Bing Liu's dataset was used for evaluation."}
{"id": "1908.07491.acac0606aab83cae5d13047863c7af542d58e54c", "question": "is this the first dataset with a grading scaling rather than binary?", "answer": ["Yes"], "num_words": 2159, "gpt4_answer": "Yes, this new dataset grades concepts on a 0-10 scale, rather than treating controversiality as a binary label."}
{"id": "1908.07491.2ee4ecf98ef7d02c9e4d103968098fe35f067bbb", "question": "what are the existing datasets for this task?", "answer": ["480 concepts previously analyzed in BIBREF1, BIBREF4", "Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives ... Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). ... Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. ", "Dataset I created and analyzed in BIBREF1, BIBREF4"], "num_words": 2159, "gpt4_answer": "The existing datasets for this task are Dataset I, Dataset II, and Dataset III."}
{"id": "1908.07491.82f8843b59668567bba09fc8f93963ca7d1fe107", "question": "what is the size of the introduced dataset?", "answer": ["608 controversial Wikipedia concepts ... 3561 concepts", "About 1216 in dataset II, 3561 in dataset III.", "Dataset I  -  480 concepts, 240  controversial examples, and  240 not-controversial examples.\nDataset II -  608 controversial concepts\nDataset III -  3561 controversial concepts"], "num_words": 2159, "gpt4_answer": "The introduced dataset (Dataset III) consists of 3561 concepts."}
{"id": "1908.07491.376e8ed6e039e07c892c77b7525778178d56acb7", "question": "did they crowdsource annotations?", "answer": ["Yes"], "num_words": 2159, "gpt4_answer": "Yes, Dataset III was crowd-annotated with 10 or more annotators per concept."}
{"id": "1908.07491.4de6bcddd46726bf58326304b0490fdb9e7e86ec", "question": "how was labeling done?", "answer": ["10 or more annotators marked whether a topic was controversial or not. The score was then normalized on an integer scale of 0-10.", "As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. ... For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random ... The annotation instructions were: Given a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10.", "The annotation instructions were: Given a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.. After that, annotations were normalized to controversiality scores on an integer scale of 0 - 10"], "num_words": 2159, "gpt4_answer": "Labeling was done through crowd-annotation, with 10 or more annotators per concept, following the instructions to mark if a topic is likely to be argued about. The annotations were then normalized to controversiality scores on an integer scale of 0 - 10."}
{"id": "1908.07491.e831ce6c406bf5d1c493162732e1b320abb71b6f", "question": "where does their dataset come from?", "answer": ["The topics from Wikipedia list of controversial issues that appear more than 50 times in Wikipedia, topics with their Wikipedia pages under edit protection.", "Wikipedia ", "Wikipedia list of controversial issues ... concepts whose Wikipedia pages are under edit protection"], "num_words": 2159, "gpt4_answer": "Their datasets come from Wikipedia concepts, with Dataset I consisting of 480 concepts previously analyzed, Dataset II based on a more recent version of the Wikipedia list of controversial issues, and Dataset III extracted from 3561 concepts whose Wikipedia pages are under edit protection."}
{"id": "1908.07491.634a071b13eb7139e77872ecfdc135a2eb2f89da", "question": "what are the baselines?", "answer": ["Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).", "Nearest neighbors (NN) Estimator ... Naive Bayes (NB) Estimator ... Recurrent neural network (RNN)"], "num_words": 2159, "gpt4_answer": "The baselines are the methods suggested in BIBREF1 and BIBREF4, which rely on complex meta-data and sophisticated algorithmic techniques for estimating controversiality."}
{"id": "1908.07491.8861138891669a45de3955c802c55a37be717977", "question": "what tools did they use?", "answer": ["Unanswerable", "nearest-neighbor estimator ... Naive Bayes model ... bidirectional RNN"], "num_words": 2159, "gpt4_answer": "The authors used pre-trained GloVe embeddings, a Nearest Neighbors (NN) Estimator, a Naive Bayes (NB) Estimator, and a Recurrent Neural Network (RNN) for their estimation schemes. They also included a BERT network for one experiment."}
{"id": "1805.11850.267d70d9f3339c56831ea150d2213643fbc5129b", "question": "What is the performance of NJM?", "answer": ["Captions generated by NJM were ranked \"funniest\" 22.59% of the time.", "It obtained a score of 22.59%", "NJM vas selected as the funniest caption among the three options 22.59% of the times, and NJM captions posted to Bokete averaged 3.23 stars"], "num_words": 1529, "gpt4_answer": "The NJM was ranked as generating the \"funniest\" captions 22.59% of the time, which is less funny than human-generated captions (67.99%) but much funnier than the baseline STAIR caption (9.41%)."}
{"id": "1805.11850.477da8d997ff87400c6aad19dcc74f8998bc89c3", "question": "How are the results evaluated?", "answer": ["The captions are ranked by humans in order of \"funniness\".", "With a questionnaire asking subjects to rank methods according to its \"funniness\". Also, by posting the captions to Bokete to evaluate them by received stars", "a questionnaire"], "num_words": 1529, "gpt4_answer": "The results are evaluated using a questionnaire where respondents rank the captions provided by humans, the Neural Joking Machine (NJM), and STAIR caption in order of \"funniness\". The questionnaire does not reveal the origins of the captions."}
{"id": "1805.11850.4485e32052741972877375667901f61e602ec4de", "question": "How big is the self-collected corpus?", "answer": [" 999,571 funny captions for 70,981 images", "999,571 funny captions for 70,981 images", "999571 captions for 70981 images."], "num_words": 1529, "gpt4_answer": "The self-collected BoketeDB contains 999,571 funny captions for 70,981 images."}
{"id": "1805.11850.df4895c6ae426006e75c511a304d56d37c42a1c7", "question": "How is the funny score calculated?", "answer": ["Based on the number of stars users assign funny captions, an LSTM calculates the loss value L as an average of each mini-batch and returns  L when the number of stars is less than 100, otherwise L-1.0", "The funny score is L if the caption has fewer than 100 stars and 1-L if the caption has 100 or more stars, where L is the average loss value calculated with the LSTM on the mini-batch."], "num_words": 1529, "gpt4_answer": "The Funny Score calculates a loss value INLINEFORM0 when #star is less than 100, and returns INLINEFORM1 when #star is over 100. The loss value INLINEFORM2 is calculated with the LSTM as an average of each mini-batch."}
{"id": "1710.06923.00e4c9aa87411dfc5455fc92f10e5c9266e7b95e", "question": "Which dataset do they use?", "answer": [" survey data and hand crafted a total of 293 textual questions BIBREF13", "Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013", "Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12", "U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013"], "num_words": 3482, "gpt4_answer": "They use the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013."}
{"id": "1710.06923.54b0d2df6ee27aaacdaf7f9c76c897b27e534667", "question": "Do they compare their proposed domain adaptation methods to some existing methods?", "answer": ["No", "Yes"], "num_words": 3482, "gpt4_answer": "The paper does not directly compare the proposed domain adaptation methods to existing methods."}
{"id": "1710.06923.b9a3836cff16af7454c7a8b0e5ff90206d0db1f5", "question": "Which of their proposed domain adaptation methods proves best overall?", "answer": ["Machine learning approach", "Unanswerable"], "num_words": 3482, "gpt4_answer": "The paper does not explicitly state which domain adaptation method is best overall."}
{"id": "1710.06923.99554d0c76fbaef90bce972700fa4c315f961c31", "question": "Do they use evolutionary-based optimization algorithms as one of their domain adaptation approaches?", "answer": ["Unanswerable", "Yes"], "num_words": 3482, "gpt4_answer": "Yes, they use an evolutionary development-based (Evo-Devo) mechanism as one of their domain adaptation approaches."}
{"id": "1811.04791.5370a0062aae7fa4e700ae47aa143be5c5fc6b22", "question": "With how many languages do they experiment in the multilingual setup?", "answer": ["16", "6", "ten languages"], "num_words": 6307, "gpt4_answer": "They experiment with 10 languages in the multilingual setup."}
{"id": "1811.04791.9a52a33d0ae5491c07f125454aea9a41b9babb82", "question": "How do they extract target language bottleneck features?", "answer": ["Extracting cae features requires three steps, as illustrated in Figure FIGREF6 . First, an utd system is applied to the target language to extract pairs of speech segments that are likely to be instances of the same word or phrase", "train a tdnn BIBREF36 with block softmax ... tdnn has six 625-dimensional hidden layers followed by a 39-dimensional bottleneck layer"], "num_words": 6307, "gpt4_answer": "They extract target language bottleneck features using a model trained on other languages."}
{"id": "1811.04791.8c46a26f9b0b41c656b5b55142d491600663defa", "question": "Which dataset do they use?", "answer": ["GlobalPhone\nCroatian\nHausa\nMandarin\nSpanish\nSwedish\nTurkish\nZRSC\nBuckeye\nXitsonga", "GlobalPhone corpus", "GlobalPhone corpus ... English wsj corpus ... Buckeye corpus ... NCHLT corpus"], "num_words": 6307, "gpt4_answer": "They use the GlobalPhone corpus of speech read from news articles, as well as the zrsc 2015 data sets (Buckeye corpus for English and NCHLT corpus for Xitsonga)."}
{"id": "1811.04791.e5f8d2fc1332e982a54ee4b4c1f7f55e900d0b86", "question": "Which intrisic measures do they use do evaluate obtained representations?", "answer": ["Precision and recall at a given threshold", "same-different ... ABX ", "same-different ... ABX evaluation measures"], "num_words": 6307, "gpt4_answer": "They use two intrinsic measures: the same-different task and the ABX phone discriminability task."}
{"id": "1906.01749.19578949108ef72603afe538059ee55b4dee0751", "question": "Do they use pretrained embeddings in their model?", "answer": ["No", "Unanswerable"], "num_words": 4114, "gpt4_answer": "No, the paper does not mention using pretrained embeddings in their model."}
{"id": "1906.01749.44435fbd4087fa711835d267036b6a1f82336a22", "question": "What results are obtained by their model?", "answer": ["Our model outperforms PG-MMR when trained and tested on the Multi-News dataset ... Transformer performs best in terms of R-1 while Hi-MAP outperforms it on R-2 and R-SU", "Their model ranked 2nd on R-1 metric and ranked 1st on R-2 and R-SU metrics"], "num_words": 4114, "gpt4_answer": "The Hi-MAP model outperforms PG-MMR when trained and tested on the Multi-News dataset, with improved performance in R-2 and R-SU metrics compared to the Transformer model."}
{"id": "1906.01749.86656aae3c27c6ea108f5600dd09ab7e421d876a", "question": "What sources do the news come from?", "answer": ["1500 news sites", "From a diverse set of news sources on site newser.com", "newser.com"], "num_words": 4114, "gpt4_answer": "The Multi-News dataset comes from a diverse set of news sources, with over 1,500 sites appearing as source documents 5 times or greater."}
{"id": "1906.01749.22488c8628b6db5fd708b6471c31a8eac31f66df", "question": "What is the size of Multi-news dataset?", "answer": ["56,216", "56216", "56216 "], "num_words": 4114, "gpt4_answer": "The Multi-News dataset contains 56,216 article-summary pairs."}
{"id": "2004.02334.1f2952cd1dc0c891232fa678b6c219f6b4d31958", "question": "Which vocabulary size was the better performer?", "answer": ["BPE 32k, 32k", "Source BPE vocabulary size is 32000; target BPE vocabulary size is 8000."], "num_words": 3992, "gpt4_answer": "The optimal BPE vocabulary size is not specified in the provided text, but it is mentioned that smaller vocabulary sizes tend to have better performance compared to larger ones, such as 32k or 64k."}
{"id": "2004.02334.23fe8431058f2a7b7588745766fc715f271aad07", "question": "Which languages are explored?", "answer": ["German (De) and English (En)", "German (De) and English (En) languages", "German ... English"], "num_words": 3992, "gpt4_answer": "The languages explored in the paper are German (De) and English (En)."}
{"id": "2004.02334.e5b2eb6a49c163872054333f8670dd3f9563046a", "question": "What datasets are used in the paper?", "answer": ["Europarl v9 ... NewsTest2013 ... NewsTest2014", "Europarl v9 parallel data set ... NewsTest2013 ... NewsTest2014", "Europarl v9 parallel data set ... NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track."], "num_words": 3992, "gpt4_answer": "The paper uses the Europarl v9 parallel data set for training German and English languages, and evaluates with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track."}
{"id": "2004.02334.73760a45b23b2ec0cab181f82953fb296bb6cd19", "question": "What vocabulary sizes are explored?", "answer": ["Word 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k", "Word 64k, Word 32k, BPE 64k,  BPE 32k,   BPE 16k,   BPE 8k,   BPE 4k,   BPE 2k,   BPE 1k.", "Word vocabulary sizes: 32000, 64000; BPE vocabulary sizes: 1000, 2000, 4000, 8000, 16000, 32000, 64000; Chars vocabulary sizes: 172, 176."], "num_words": 3992, "gpt4_answer": "The paper explores BPE vocabulary sizes such as 32k, 64k, 8k, and 1k."}
{"id": "2004.02334.ec990c16896793a819766bc3168c02556ef69971", "question": "What vocabulary size was the best performer?", "answer": ["Source BPE vocabulary size is 32000; target BPE vocabulary size is 8000.", "Unanswerable"], "num_words": 3992, "gpt4_answer": "The paper does not explicitly mention a specific best-performing vocabulary size."}
{"id": "2004.02334.11c4071d9d7efeede84f47892b1fa0c6a93667eb", "question": "What datasets do they look at?", "answer": ["Europarl v9 ... NewsTest2013  ... NewsTest2014", "Europarl v9 ... NewsTest2013 ... NewsTest2014", "Europarl v9 parallel data set ... NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track"], "num_words": 3992, "gpt4_answer": "The authors use the publicly available Europarl v9 parallel dataset for training German and English languages and evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track."}
{"id": "2004.02334.9aa751aebf6a449d95fb04ceec71688f2ed2cea2", "question": "Which vocab sizes did they analyze?", "answer": ["Word 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k", "Word 64k, Word 32k, BPE 64k,  BPE 32k,   BPE 16k,   BPE 8k,   BPE 4k,   BPE 2k,   BPE 1k.", "Word vocabulary sizes: 32000, 64000; BPE vocabulary sizes: 1000, 2000, 4000, 8000, 16000, 32000, 64000; Chars vocabulary sizes: 172, 176."], "num_words": 3992, "gpt4_answer": "The paper analyzed various vocabulary sizes, including 1k, 8k, 32k, and 64k BPE types."}
{"id": "1908.11046.2929e92f9b4939297b4d0f799d464d46e8d52063", "question": "Why is improvement on OntoNotes significantly smaller compared to improvement on WNUT 2017?", "answer": ["Ontonotes is less noisy than Wnut 2017", "The WNUT 2017 dataset had entities already seen in the training set filtered out while the OntoNotes dataset did not. Cross-context patterns thus provided more significant information for NER in WNUT 2017 because the possibility of memorizing entity forms was removed.", "suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities"], "num_words": 3489, "gpt4_answer": "The improvement on OntoNotes is smaller compared to WNUT 2017 because cross-context patterns are more crucial for emerging contexts and entities in WNUT 2017 than for familiar entities in OntoNotes, which might often be memorized by their surface forms."}
{"id": "1908.11046.1dcfcfa46dbcffc2fc7be92dd57df9620258097b", "question": "How is \"complexity\" and \"confusability\" of entity mentions defined in this work?", "answer": ["Complexity is defined by examples of a singular named entity (e.g. work-of-art and creative-work entities) being represented by multiple surface forms. Mapping all of these forms to a single NE requires a complex understanding of the variations, some of which are genre-specific. Confusability is defined by examples when it becomes more difficult to disambiguate named entities that share the same surface form, such as the \"language\" versus \"NORP\" distinction represented by the surface forms Dutch and English.", "disambiguating fine-grained entity types ... entities could in principle take any surface forms  unseen, the same as a person name, abbreviated, or written with unreliable capitalizations on social media"], "num_words": 3489, "gpt4_answer": "Complexity and confusability of entity mentions in this work refer to hard entity types, such as work-of-art, that can take any surface form, and fine-grained entity types, like language and NORP, that often share the same surface forms, making it challenging to disambiguate their types."}
{"id": "1908.11046.77bbe1698e001c5889217be3164982ea36e85752", "question": "What are the baseline models?", "answer": ["Baseline-BiLSTM-CNN", "BiLSTM-CNN", "BiLSTM-CNN proposed by BIBREF1"], "num_words": 3489, "gpt4_answer": "The baseline models are BiLSTM-CNN, CRF-BiLSTM(-BiLSTM), and CRF-IDCNN."}
{"id": "1906.01076.b537832bba2eb6d34702a9d71138e661c05a7c3a", "question": "What text classification tasks are considered?", "answer": [" AGNews (4 classes), Yelp (5 classes), DBPedia (14 classes), Amazon (5 classes), and Yahoo (10 classes)", "news classification ... sentiment analysis ... Wikipedia article classification", "news classification ... sentiment analysis ... Wikipedia article classification ... questions and answers categorization "], "num_words": 3296, "gpt4_answer": "The considered text classification tasks in the paper are news classification (AGNews), sentiment analysis (Yelp, Amazon), Wikipedia article classification (DBPedia), and questions and answers categorization (Yahoo)."}
{"id": "1906.01076.1002bd01372eba0f3078fb4a951505278ed45f2e", "question": "Do they compare against other models?", "answer": ["Yes"], "num_words": 3296, "gpt4_answer": "Yes, the paper compares the proposed model (MbPA++) against several baselines, including Enc-Dec, A-GEM, Replay, MbPA, MbPA K=0, and MTL, on text classification and question answering tasks."}
{"id": "1906.01076.3450723bf66956486de777f141bde5073e4a7694", "question": "What is episodic memory?", "answer": ["It is a memory that stores previously seen examples throughout its lifetime", "module that stores previously seen examples throughout its lifetime ... used for sparse experience replay and local adaptation to prevent catastrophic forgetting and encourage positive transfer"], "num_words": 3296, "gpt4_answer": "The episodic memory model introduced in the paper is designed for lifelong language learning and performs sparse experience replay and local adaptation to continuously learn and reuse previously acquired knowledge while mitigating catastrophic forgetting."}
{"id": "1908.09919.36cb7ebdd39e0b8a89ff946d3a3aef8a76a6bb43", "question": "Are LSA-reduced n-gram features considered hand-crafted features?", "answer": ["No", "Yes"], "num_words": 2208, "gpt4_answer": "Yes, LSA-reduced n-gram features are considered hand-crafted features."}
{"id": "1908.09919.28e50459da60ceda49fe1578c12f3f805b288bd0", "question": "What is the performance of the model on English, Spanish and Arabic?", "answer": ["Accuracy: English 82.31, Spanish 80.22, Arabic 80.50", "In terms of accuracy, 81.789% for English, 78.227% for Spanish and 78.5% for Arabic", "on PAN 2018 dataset, the accuracy is 82.31% for English, 80.22% for Spanish and 80.50% for Arabic"], "num_words": 2208, "gpt4_answer": "The improved model (RNNwA + n-gram) achieved state-of-the-art accuracy on English and had competitive performance on Spanish and Arabic."}
{"id": "1908.09919.e1f61500eb733f2b95692b6a9a53f8aaa6f1e1f6", "question": "How is this model different from a LSTM?", "answer": ["It's a recurrent neural network with n-gram model", "bidirectional RNN with GRU"], "num_words": 2208, "gpt4_answer": "This model uses a bidirectional RNN with GRU cells instead of LSTM cells."}
{"id": "1910.10670.da4d07645edaf7494a8cb5216150a00690da01f7", "question": "What does the cache consist of?", "answer": ["Unanswerable", "static public cache stores the most frequent states ... lifetime of a private cache actually can last for the entire dialog section for a specific user ... subsequent utterances faster as more states are composed and stored"], "num_words": 3030, "gpt4_answer": "The cache consists of two layers: the first layer is a public cache that stores most of the static part of the graph, shared globally among all users, and the second layer is a private cache that caches the graph representing the personalized language model, shared only by the utterances from a particular user."}
{"id": "1910.10670.c0cebef0e29b9d13c165b6f19f6ca8393348c671", "question": "What languages is the model tested on?", "answer": ["English", "Unanswerable"], "num_words": 3030, "gpt4_answer": "Unanswerable"}
{"id": "1910.10670.5695908a8c6beb0e3863a1458a1b93aab508fd34", "question": "What is a personalized language model?", "answer": [" contains the expected user-specific entities", "A model that contains the expected user-specific entities.", "language model which contains user-specific entities"], "num_words": 3030, "gpt4_answer": "A personalized language model (LM) is a language model that contains expected user-specific entities, such as names from a contact list, locations, or music titles, to improve speech recognition performance."}
{"id": "1902.06734.fa800a21469a70fa6490bfc67cabdcc8bf086fb5", "question": "Is the dataset used in other work?", "answer": ["Yes", "Yes, in Waseem and Hovy (2016)"], "num_words": 4384, "gpt4_answer": "Yes, the dataset used in this paper is from Waseem and Hovy's work, containing tweets manually annotated for hate speech."}
{"id": "1902.06734.6883767bbdf14e124c61df4f76335d3e91bfcb03", "question": "What is the drawback to methods that rely on textual cues?", "answer": ["They don't provide wider discourse information", "tweets that are part of a larger hateful discourse or contain links to hateful content while not explicitly having textual cues"], "num_words": 4384, "gpt4_answer": "The drawback to methods that rely on textual cues is that they disregard user and community information, which can be helpful in identifying hate speech and improving the performance of detection methods."}
{"id": "1902.06734.11679d1feba747c64bbbc62939a20fbb69ada0f3", "question": "What community-based profiling features are used?", "answer": ["The features are the output of running node2vec on a community graph where the nodes are users, and they are connected if one of them follows the other on Twitter.", "The features are the outputs from node2vec when run on a community graph where nodes are users and edges are connections if one user follows the other on Twitter."], "num_words": 4384, "gpt4_answer": "The community-based profiling features used in this paper are generated using an undirected unlabeled community graph, where nodes represent authors and edges represent connections between them. The node2vec framework is used to obtain a vector representation (embedding) for each author based on their positions and neighbors in the graph."}
{"id": "1907.08540.e0c80d31d590df46d33502169b1d32f0aa1ea6e3", "question": "what user traits are taken into account?", "answer": ["Family, Nature, Work-Ethic, Religion", "The hierarchical personal values lexicon with 50  sets of words and phrases that represent the user's value.", "personal values"], "num_words": 4995, "gpt4_answer": "The user traits taken into account include the user's history, profile, and attributes such as personal values."}
{"id": "1907.08540.7a8b24062a5bb63a8b4c729f6247a7fd2fec7f07", "question": "does incorporating user traits help the task?", "answer": ["No", "only in the 806-class task predicting <= 25 clusters"], "num_words": 4995, "gpt4_answer": "Yes, incorporating inferred user traits into the model helps with the prediction task."}
{"id": "1907.08540.cab082973e1648b0f0cc651ab4e0298a5ca012b5", "question": "how many activities are in the dataset?", "answer": ["29,494", "29537", "30,000"], "num_words": 4995, "gpt4_answer": "The dataset consists of posts from more than 200,000 users mentioning at least one of the nearly 30,000 everyday activities."}
{"id": "1907.08540.1cc394bdfdfd187fc0af28500ad47a0a764d5645", "question": "who annotated the datset?", "answer": ["1000 people", "Unanswerable"], "num_words": 4995, "gpt4_answer": "The paper does not provide specific information about who annotated the dataset."}
{"id": "1907.08540.16cc37e4f8e2db99eaf89337a3d9ada431170d5b", "question": "how were the data instances chosen?", "answer": [" query contains a first-person, past-tense verb within a phrase that describes a common activity that people do", "By querying Twitter Search API for the tweets containing a first-person and a past-tense verb that describes a common activity."], "num_words": 4995, "gpt4_answer": "The data instances were chosen by formulating a set of queries using the Twitter Search API to find instances of users tweeting about common human activities. The queries contained first-person, past-tense verbs within phrases describing common activities, and were built from two sources: the Event2Mind dataset and short activity surveys collected from 1,000 people across the United States."}
{"id": "1907.08540.cc78a08f5bfe233405c99cb3dac1f11f3a9268b1", "question": "what social media platform was the data collected from?", "answer": [" Twitter", "Twitter", "Twitter "], "num_words": 4995, "gpt4_answer": "The data was collected from Twitter."}
{"id": "1611.04887.101d7a355e8bf6d1860917876ee0b9971eae7a2f", "question": "Do they report results only for English data?", "answer": ["Unanswerable", "Yes"], "num_words": 2397, "gpt4_answer": "Unanswerable"}
{"id": "1611.04887.4288621e960ffbfce59ef1c740d30baac1588b9b", "question": "What conclusions do the authors draw from their experiments?", "answer": ["CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models ... Tweet length affects the task prediction accuracies, ... CNN, LSTM and BLSTM are extremely sensitive to word order", "Supervised models CNN, LSTM and BLSTM and unsupervised models BOW, DSSM, STV and T2V can  encapsulate most of the syntactic and social properties. Tweet length affects the task prediction accuracies for all models. LDA is insensitive to input word order, but, CNN, LSTM\nand BLSTM are not.", "among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models ... Tweet length affects the task prediction accuracies ... LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive"], "num_words": 2397, "gpt4_answer": "The authors conclude that among supervised models, CNN, LSTM, and BLSTM encapsulate most of the syntactic and social properties with great accuracy, while BOW, DSSM, STV, and T2V do that among the unsupervised models. They also found that tweet length affects task prediction accuracies, and models like LDA are insensitive to input word order, while CNN, LSTM, and BLSTM are sensitive to it."}
{"id": "1611.04887.c3befe7006ca81ce64397df654c31c11482dafbe", "question": "In what way does each classifier evaluate one of the syntactic or social properties which are salient for a tweet?", "answer": [" if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation", "Through 8 different property prediction tasks"], "num_words": 2397, "gpt4_answer": "Each classifier evaluates one of the syntactic or social properties by taking the tweet representation as input and predicting the specific property based on that representation."}
{"id": "1808.10006.5d0a3f8ca3882f87773cf8c2ef1b4f72b9cc241e", "question": "How is a per-word reward tuned with the perceptron algorithm?", "answer": ["Optimal per-word reward is found using SGD, which in this case is the same as the perceptron algorithm", "hen the stochastic gradient descent update is just the familiar perceptron rule: INLINEFORM1"], "num_words": 3790, "gpt4_answer": "The per-word reward is tuned using a perceptron-like tuning method, which involves updating the parameter with the familiar perceptron rule on a batch of sentences rather than a single sentence."}
{"id": "1808.10006.dce27c49b9bf1919ca545e04663507d83bb42dbe", "question": "What methods are used to correct the brevity problem?", "answer": [" tuned word reward ", "Length normalization ... Google's NMT ... constant word reward", "Length normalization; Googles NMT correction; constant word reward"], "num_words": 3790, "gpt4_answer": "There are three commonly used methods to correct the brevity problem: length normalization, Google's NMT system correction, and adding a constant word reward."}
{"id": "1808.10006.991ea04072b3412928be5e6e903cfa54eeac3951", "question": "Why does wider beam search hurt NMT?", "answer": ["Using a wider beam increases the probability of a shorter translation to remain in the top k variants and eventually score higher than any longer and more accurate translation variant", "brevity problem", "if a translation ending at time INLINEFORM2 scores higher than a longer translation, as long as it does not fall within the top INLINEFORM3 when compared with partial translations of length INLINEFORM4 (or complete translations of length at most INLINEFORM5 ), it will be pruned and unable to block the longer translation. But if we widen the beam ( INLINEFORM6 ), then translation accuracy will suffer. We call this problem (which is BIBREF0 's sixth challenge) the beam problem."], "num_words": 3790, "gpt4_answer": "Wider beam search hurts NMT because it exacerbates the brevity problem, which results from the locally-normalized structure of the model, leading to shorter translations and degraded performance."}
{"id": "1702.02584.a82a12a22a45d9507bc359635ffe9574f15e0810", "question": "What linguistic model does the conventional method use?", "answer": ["Random Forest BIBREF12", "Random Forest classifier using latent semantic structural features, semantic distance features and sentences' averaged Word2Vec representations", "Random Forest to perform humor recognition by using the following two groups of features:  latent semantic structural features and semantic distance features."], "num_words": 1923, "gpt4_answer": "The conventional method uses Random Forest with latent semantic structural features and semantic distance features."}
{"id": "1702.02584.355cf303ba61f84b580e2016fcb24e438abeafa7", "question": "What is novel about the newly emerging CNN method, in comparison to well-established conventional method?", "answer": ["Unanswerable", "one layer of convolution on top of word embedding vectors, achieves excellent results on multiple tasks"], "num_words": 1923, "gpt4_answer": "The novel aspect of the CNN method is its higher detection accuracy and the ability to learn essential features automatically, making it more efficient when facing new datasets."}
{"id": "1702.02584.88757bc49ccab76e587fba7521f0981d6a1af2f7", "question": "What lexical cues are used for humor recogition?", "answer": ["Incongruity ... Ambiguity ... Interpersonal Effect ... Phonetic Style", "alliteration ... antonymy ... adult slang"], "num_words": 1923, "gpt4_answer": "Lexical cues for humor recognition include incongruity, ambiguity, interpersonal effect, and phonetic pattern."}
{"id": "1702.02584.2f9a31f5a2b668acf3bce8958f5daa67ab8b2c83", "question": "Do they evaluate only on English data?", "answer": ["Unanswerable", "Yes"], "num_words": 1923, "gpt4_answer": "Yes, they evaluate only on English data."}
{"id": "1702.02584.4830459e3d1d204e431025ce7e596ef3f8d757d2", "question": "How many speakers are included in the dataset?", "answer": ["Unanswerable"], "num_words": 1923, "gpt4_answer": "The dataset contains about 100 times more speakers compared to the limited number of actors in existing corpora."}
{"id": "1702.02584.74ebfba06f37cc95dfe59c3790ebe6165e6be19c", "question": "How are the positive instances annotated? e.g. by annotators, or by laughter from the audience?", "answer": ["By laughter from the audience", "Laughter from the audience.", "by laughter"], "num_words": 1923, "gpt4_answer": "Positive instances are annotated by the presence of `(Laughter)' in the TED Talk transcripts, which represents where audiences laughed aloud during the talks."}
{"id": "1709.05453.3a01dc85ac983002fd631f1c28fc1cbe16094c24", "question": "How you incorporate commonsense into an LSTM?", "answer": ["by employing an external memory module containing commonsense knowledge", "using another LSTM for encoding all assertions $a$ in $A_x$ , as illustrated in Figure 3 . Each $a$ , originally in the form of $$ , is transformed into a sequence of tokens by chunking $c_1$ , $c_2$ , concepts which are potentially multi-word phrases, into $[c_{11},c_{12},c_{13}...]$ and $[c_{21},c_{22},c_{23}...]$ . Thus, $a=[c_{11},c_{12},c_{13}...,r,c_{21},c_{22},c_{23}...]$ .", "using another LSTM for encoding commonsense assertions"], "num_words": 3173, "gpt4_answer": "The Tri-LSTM model incorporates commonsense knowledge into an LSTM by using another LSTM for encoding all assertions in the commonsense knowledge base related to the message, and then calculating the match score between the commonsense assertion and the response."}
{"id": "1709.05453.00ffe2c59a3ba18d6d2b353d6ab062a152c88526", "question": "Which domain are the conversations in?", "answer": ["open-domain", "open-domain Twitter dialogues"], "num_words": 3173, "gpt4_answer": "The conversations are in the open-domain."}
{"id": "1709.05453.042800c3336ed5f4826203616a39747c61382ba6", "question": "Which commonsense knowledge base are they using?", "answer": ["ConceptNet"], "num_words": 3173, "gpt4_answer": "They are using ConceptNet as the commonsense knowledge base."}
{"id": "2002.06854.52868394eb2b3b37eb5f47f51c06ad53061f4495", "question": "How did they obtain the dataset?", "answer": ["The authors crawled all areas listed an TripAdvisor's SiteIndex and gathered all links related to hotels. Using Selenium, they put a time gap between opening each page, to mimic human behaviour and avoid having their scraper being detected. They discarded pages without a review and for pages with a review, they collected the review's profile, the overall rating, the summary, the written text and subratings, where given. ", "TripAdvisor hotel reviews", "hotel reviews from TripAdvisor"], "num_words": 2920, "gpt4_answer": "They obtained the dataset by crawling hotel reviews from TripAdvisor, collecting information such as user profile, hotel URL, overall rating, summary, user-written text, date, and multiple sub-ratings of aspects when provided. The crawling process was parallelized on approximately 100 cores for two months."}
{"id": "2002.06854.59dc6b1d3da74a2e67a6fb1ce940b28d9e3d8de0", "question": "Are the recommendations specific to a region?", "answer": ["No", "Unanswerable"], "num_words": 2920, "gpt4_answer": "Unanswerable"}
{"id": "2002.06854.713e1c7b0ab17759ba85d7cd2041e387831661df", "question": "Did they experiment on this dataset?", "answer": ["Yes"], "num_words": 2920, "gpt4_answer": "Yes, they experimented on the HotelRec dataset using different models for rating prediction and recommendation performance tasks."}
{"id": "1906.05506.00db191facf903cef18fb1727d1cab638c277e0a", "question": "What sized character n-grams do they use?", "answer": ["2, 3 and 4", "cahr3-MS-vec ... char4-MS-vec ... char2-MS-vec", "char3"], "num_words": 3045, "gpt4_answer": "They used character n-grams of sizes 2, 3, and 4 in their experiments."}
{"id": "1906.05506.1edfe390828f02a2db9a88454421c7f3d4cdd611", "question": "Do they experiment with fine-tuning their embeddings?", "answer": ["No"], "num_words": 3045, "gpt4_answer": "The paper does not mention experimenting with fine-tuning their embeddings."}
{"id": "1906.05506.3dad6b792044018bb968ac0d0fd4628653f9e4b7", "question": "Which word embeddings do they compare against?", "answer": ["Unanswerable"], "num_words": 3045, "gpt4_answer": "They compare against charCNN, char-Sum-vec, and char-SS-vec."}
{"id": "1906.05506.a28c73a6a8c46a43a1eec2b42b542dd7fde1e30e", "question": "Which dataset do they evaluate on for headline generation?", "answer": [" the annotated English Gigaword corpus", "English Gigaword corpus", "English Gigaword corpus BIBREF35"], "num_words": 3045, "gpt4_answer": "They used sentence-headline pairs extracted from the annotated English Gigaword corpus for headline generation."}
{"id": "1906.05506.5f1ffaa738fedd5b6668ec8b58a027ddea6867ce", "question": "What results do their embeddings obtain on machine translation?", "answer": ["BLEU score of 35.48 on En-Fr, 23.27 on En-De, 34.43 on Fr-En, 28.86 on De-En", "BLEU scores are: En-Fr(35.84), En-De(23.27), Fr-En(34.43) and De-En(28.86).", "Bleu on IWSLT16: En-FR 35.48, En-De 23.27, Fr-En 34.43, De-En 28.86"], "num_words": 3045, "gpt4_answer": "The proposed method, EncDec+char3-MS-vec, outperformed the base model EncDec in all test data for machine translation tasks, indicating that the method has a positive effect on the neural encoder-decoder model."}
{"id": "1906.05506.8e26c471ca0ee1b9779da04c0b81918fd310d0f3", "question": "How do they combine ordinary word embeddings and ones constructed from character n-grams?", "answer": ["They use a sum of charn-MS-vec and the standard word embedding as an input of an RNN", "Yes"], "num_words": 3045, "gpt4_answer": "The proposed method combines ordinary word embeddings and ones constructed from character n-grams by summing them up and using the sum as an input to the RNN."}
{"id": "1808.00957.a398c9b061f28543bc77c2951d0dfc5d1bee9e87", "question": "Which dataset do they use?", "answer": ["19538 tweets  from BIBREF4", "A crowdsourced twitter dataset containing 19358 tweets", "BIBREF4"], "num_words": 2450, "gpt4_answer": "They use a dataset of 19,538 crowdsourced annotated tweets curated by BIBREF4."}
{"id": "1808.00957.dae9caf8434ce43c9bc5913ebf062bc057a27cfe", "question": "By how much do they outperform previous state-of-the-art approaches?", "answer": ["BiLSTM for 0.02 F1,  Feature Engineering SotA  for 0.08 F1, and Concatenated NN Architecture for  0.24 F1.", "By more than 0.02 with F1 score and 0.21% with accuracy", "Proposed model had 0.63 F1 score and 83.49% accuracy compared to the 0.61 F1 and 83.28% accuracy of best compared method."], "num_words": 2450, "gpt4_answer": "The proposed model outperforms previous state-of-the-art approaches with an accuracy of 83.49%."}
{"id": "1808.00957.e9b6b14b8061b71d73a73d8138c8dab8eda4ba3f", "question": "Do they analyze attention outputs to determine which terms in general contribute to clickbait titles?", "answer": ["No"], "num_words": 2450, "gpt4_answer": "No, the paper does not specifically analyze attention outputs to determine which terms in general contribute to clickbait titles."}
{"id": "1808.07231.76e17e648a4d1f386eb6bf61b0c24f134af872be", "question": "What other scenarios can the bias mitigation methods be applied to?", "answer": ["developing bias mitigation methods that maintain (or even increase) the classification performance and reduce the bias at the same time", "other identity problems like racial ... sentiment analysis", "sentiment analysis  ... other identity problems like racial"], "num_words": 2493, "gpt4_answer": "The bias mitigation methods can be extended to other identity problems like racial biases and different tasks like sentiment analysis."}
{"id": "1808.07231.7572f6e68a2ed2c41b87c5088ba8680afa0c0a0b", "question": "Are the three bias mitigation methods combined in any model?", "answer": ["Debiased Word Embeddings ... Gender Swap ... Bias fine-tuning", "Yes"], "num_words": 2493, "gpt4_answer": "Yes, the three bias mitigation methods (debiased word embeddings, gender swap data augmentation, and fine-tuning with a larger corpus) are combined in a model, which improved both generated unbiased set performance and equality differences but had the largest decrease in the original performance."}
{"id": "1808.07231.5d2bbcc3aa769e639dc21893890bc36b76597a33", "question": "Which of the three bias mitigation methods is most effective?", "answer": ["Gender Swap", "most effective method was applying both debiased embedding and gender swap"], "num_words": 2493, "gpt4_answer": "The most effective bias mitigation method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance."}
{"id": "1808.07231.4ddc53afffaf1622d97695347dd1b3190d156dee", "question": "What model architectures are used?", "answer": ["Convolutional Neural Network (CNN) ... Gated Recurrent Unit (GRU) ... Bidirectional GRU with self-attention ( INLINEFORM0 -GRU)", "Convolutional Neural Network ... Gated Recurrent Unit ... Bidirectional GRU with self-attention"], "num_words": 2493, "gpt4_answer": "The model architectures used are Convolutional Neural Network (CNN), Gated Recurrent Unit (GRU), and Bidirectional GRU with self-attention (Bi-GRU)."}
{"id": "1808.07231.5d93245832d90b31aee42ea2bf1e7704c22ebeca", "question": "What pre-trained word embeddings are used?", "answer": ["word2vec ... FastText ... randomly initialized embeddings (random)", "word2vec BIBREF10 trained on Google News corpus ... FastText BIBREF16 ) trained on Wikipedia corpus,", "word2vec train on Google News corpus; FastText train on Wikipedia corpus; randomly initialized embeddings"], "num_words": 2493, "gpt4_answer": "The pre-trained word embeddings used are word2vec trained on Google News corpus and FastText trained on Wikipedia corpus."}
{"id": "1808.07231.c0dbf3f1957f3bff3ced5b48aff60097f3eac7bb", "question": "What metrics are used to measure gender biases?", "answer": ["AUC scores on the original test set  ... AUC scores on the unbiased generated test set ... the false positive/negative equality differences proposed in BIBREF1 which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate", "AUC scores on the original test set (Orig. AUC) ...  AUC scores on the unbiased generated test set (Gen. AUC) ... false positive/negative equality differences", "False Positive Equality Difference ... False Negative Equality Difference"], "num_words": 2493, "gpt4_answer": "The metrics used to measure gender biases are False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED)."}
{"id": "1712.02555.ed7ce13cd95f7664a5e4fc530dcf72dc3808dced", "question": "Do they ensure the that the architecture is differentiable everywhere after adding the Hungarian layer?", "answer": ["Yes"], "num_words": 3384, "gpt4_answer": "Yes, the authors modify the back-propagation algorithm in a dynamically graph-constructing manner to ensure that the architecture remains differentiable after adding the Hungarian layer."}
{"id": "1712.02555.26eceba0e6e4c0b6dfa94e5708dd74b63f701731", "question": "Which dataset(s) do they train on?", "answer": ["Quora Question Pairs", "the public benchmark dataset of Quora Question Pairs"], "num_words": 3384, "gpt4_answer": "They train on the famous public benchmark dataset of Quora Question Pairs."}
{"id": "1712.02555.ff69b363ca604f80b2aa7afdc6a32d2ffd2d1f85", "question": "By how much does their model outperform state-of-the-art baselines?", "answer": ["0.78% over the best state-of-the-art baseline", "The average improvement in accuracy of their model over baselines is 3.026 points.", "by more than  0.18"], "num_words": 3384, "gpt4_answer": "Our model outperforms state-of-the-art baselines substantially."}
{"id": "1909.02322.ee19fd54997f2eec7c87c7d4a2169026fe208285", "question": "Do they compare to previous work?", "answer": ["Unanswerable", "Yes"], "num_words": 3899, "gpt4_answer": "Yes, the authors compare their proposed Condense-Abstract framework to previous work, including both one-pass methods and Extract-Abstract-based methods. Their framework outperforms state-of-the-art models according to automatic metrics and human evaluation."}
{"id": "1909.02322.74fcb741d29892918903702dbb145fef372d1de3", "question": "What is the model trained?", "answer": ["BiLSTM autoencoder as the Condense model ... simple LSTM decoder enhanced with attention BIBREF14 and copy mechanisms BIBREF32", "Condense-Abstract Framework, consisting of BiLSTM autoencoder and LSTM decoder with attention."], "num_words": 3899, "gpt4_answer": "The model is trained using the Adam optimizer, maximum likelihood loss, and hinge loss on the Rotten Tomatoes dataset consisting of movie reviews and opinion summaries."}
{"id": "1909.02322.de0d135b94ba3b3a4f4a0fb03df38a84f9dc9da4", "question": "How large is the dataset used?", "answer": ["3,731 movies; for each movie we are given a large set of reviews (99.8 on average)", "3731", "3731 movies containing around 372353 reviews"], "num_words": 3899, "gpt4_answer": "The Rotten Tomatoes dataset used in this paper contains 3,731 movies with an average of 99.8 reviews per movie."}
{"id": "1805.04579.6a20a3220c4edad758b912e2d3e5b99b0b295d96", "question": "How exactly do they weigh between different statistical models?", "answer": ["They define cWeight as weight obtained for each sentence using all the models where the sentences is in the summary of predicted by each model.", "after training on corpus, we assign weights among the different techniques", "by training on field-specific corpora"], "num_words": 2974, "gpt4_answer": "The paper uses a machine learning-based approach to fine-tune weights for different models, taking the F1-score/F-measure as a factor. They train on domain-specific documents with gold standard/human-composed summaries and assign weights to each model using the F1-score for that model in each document."}
{"id": "1805.04579.c2745e44ebe7dd57126b784ac065f0b7fc2630f1", "question": "Do they compare against state-of-the-art summarization approaches?", "answer": ["No", "Yes"], "num_words": 2974, "gpt4_answer": "The paper does not explicitly compare their approach against state-of-the-art summarization methods."}
{"id": "1805.04579.d5dcc89a08924bed9772bc431090cbb52fb7836f", "question": "What showed to be the best performing combination of semantic and statistical model on the summarization task in terms of ROUGE score?", "answer": ["Best result was obtained by using combination of: Jaccard/Cosine Similarity Matrix, TextRank and InferSent Based Model", "Combination of Jaccard/Cosine Similarity Matrix, TextRank and  InferSent Based Model", "Jaccard/Cosine Similarity Matrix+TextRank\n+InferSent Based Model"], "num_words": 2974, "gpt4_answer": "The best performing combination of semantic and statistical model on the summarization task in terms of ROUGE score is not explicitly mentioned in the provided text."}
{"id": "1908.10149.d418bf6595b1b51a114f28ac8a6909c278838aeb", "question": "What QA system was used in this work?", "answer": ["Rasa natural language understanding framework", "We implement our question answering system using state-of-the-art open source components. "], "num_words": 3192, "gpt4_answer": "The QA system used in this work is based on the Rasa natural language understanding (NLU) framework with two standard pipelines: spacy_sklearn and tensorflow_embedding."}
{"id": "1908.10149.6d6b0628d8a942c57d7af1447a563021be79bc64", "question": "Is the re-ranking approach described in this paper a transductive learning technique?", "answer": ["Unanswerable", "Yes"], "num_words": 3192, "gpt4_answer": "No, the re-ranking approach described in this paper is not a transductive learning technique."}
{"id": "1908.10149.b21245212244ad7adf7d321420f2239a0f0fe56b", "question": "How big is the test set used for evaluating the proposed re-ranking approach?", "answer": ["3084", "3084 real user requests  assigned to suitable answers from the training corpus.", "3084 real user requests from a chat-log of T-Mobile Austria"], "num_words": 3192, "gpt4_answer": "The test set used for evaluating the proposed re-ranking approach consists of 3084 real user requests from a chat-log of T-Mobile Austria."}
{"id": "1803.07828.4a201b8b9cc566b56aedb5ab45335f202bc41845", "question": "What is the new metric?", "answer": ["Neighbour Similarity Test (NST) and Type and Category Test (TCT)", "Neighbour Similarity Test; Type and Category Test", "They propose two new metrics. One, which they call the Neighbour Similarity Test, calculates how many shared characteristics there are between entities whose representations are neighbors in the embedding space. The second, which they call the Type and Category Test, is the same as the  Neighbour Similarity Test, except it uses entity types and categories in the place of individual entity characteristics."], "num_words": 3089, "gpt4_answer": "The new metric introduced in the paper is the Neighbour Similarity Test (NST), which is based on the Jaccard index between the sets of characteristics of two entities in a knowledge graph."}
{"id": "1803.07828.6a90135bd001be69a888076aff1b149b78adf443", "question": "How long do other state-of-the-art models take to process the same amount of data?", "answer": ["RDF2Vec takes 123 minutes to generate random walks and an estimated 96 hours to train word2vec. KGloVe takes an estimated 12 hours to train GloVe. fastText takes an estimated 72 hours to train", "RDF2Vec: 123 minutes runtime with >96 hours training, FastText: 5 minutes with >72 hours training", "between 12 hours and 96 hours"], "num_words": 3089, "gpt4_answer": "Unanswerable"}
{"id": "1803.07828.1f40adc719d8ccda81e7e90525b577f5698b5aad", "question": "What context is used when computing the embedding for an entity?", "answer": ["a subject, a predicate, and an object in a knowledge base", "context window of 2"], "num_words": 3089, "gpt4_answer": "The context used when computing the embedding for an entity is a sequence of three URIs, consisting of a subject, a predicate, and an object in a triple from the knowledge base."}
{"id": "1803.08419.f92c344e9b1a986754277fd0f08a47dc3e5f9feb", "question": "What are the limitations of the currently used quantitative metrics? e.g. why are they not 'good'?", "answer": ["As there can be multiple correct output utterances for a given input utterance there is no quantitative way to evaluate how well a model is performing. ... The current evaluation methods like perplexity and BLEU score are not good enough and correlate very weakly with human judgments.  ... According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses.", "The metrics correlate very weakly with human judgements, word-overlap metrics require too many ground-truth reposnses and embedding-based metrics are insufficiently complex for modeling sentence-level compositionality in dialogue", "perplexity and BLEU score are not good enough and correlate very weakly with human judgments ... word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses ... metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality"], "num_words": 6400, "gpt4_answer": "The limitations of the currently used quantitative metrics, like BLEU and perplexity, include their inability to capture the actual objective achieved through human communication, such as longer-term information exchange, and their inadequacy in evaluating multiple correct output utterances for a given input utterance."}
{"id": "1803.08419.b10388e343868ca8e5c7c601ebb903f52e756e61", "question": "What metrics are typically used to compare models?", "answer": [" perplexity and BLEU score", "BLeU  ... perplexity ", "BLeU ... perplexity"], "num_words": 6400, "gpt4_answer": "There is no perfect quantitative method to compare conversational agents. The field relies on qualitative measures or metrics like BLEU and perplexity borrowed from machine translation."}
{"id": "1803.08419.e8cdeb3a081d51cc143c7090a54c82d393f1a2ca", "question": "Is there a benchmark to compare the different approaches?", "answer": ["No", "Unanswerable"], "num_words": 6400, "gpt4_answer": "Unanswerable"}
{"id": "1803.08419.833d3ae7613500f2867ed8b33d233d71781014e7", "question": "What GAN and RL approaches are used?", "answer": ["adversarial training for open-domain dialogue generation  ... trust region actor-critic with experience replay  ... episodic natural actor-critic with experience replay ... multi-turn dialogue agent ... on-policy Monte Carlo method ", "authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated ... task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones", "the authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. ... The task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones ... The generative model defines the policy that generates a response given the dialog history and the discriminative model is a binary classifier that takes a sequence of dialog utterances as inputs and outputs whether the input is generated by the humans or machines."], "num_words": 6400, "gpt4_answer": "The paper discusses reinforcement learning (RL) approaches for dialogue policy optimization, such as the KB-InfoBot, which uses an RNN and the REINFORCE algorithm, and the Actor-Critic Algorithm, which includes trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER). However, it does not specifically mention any generative adversarial network (GAN) approaches."}
{"id": "1803.08419.a1a0365bf6968cbdfd1072cf3923c26250bc955c", "question": "What type of neural models are used?", "answer": ["Sequence to Sequence approaches ... Language Model ", "Sequence to Sequence approaches ... Language Model based approaches", "Sequence to Sequence approaches for dialogue modelling ... Language Model based approaches for dialogue modelling"], "num_words": 6400, "gpt4_answer": "The paper discusses various neural models, including sequence-to-sequence models, hierarchical recurrent encoder-decoder (HRED) architecture, language models, and knowledge-augmented models."}
{"id": "1803.08419.64f7337970e8d1989b2e1f7106d86f73c4a3d0af", "question": "What type of statistical models were used initially?", "answer": ["The speech recognition was done using n-gram statistical model ... The grammar rules used to identify bus stops were generated automatically from the schedule database ... they trained a statistical language model on the artificial corpus", "semi-continuous HMMs"], "num_words": 6400, "gpt4_answer": "Initially, n-gram statistical models were used for speech recognition in conversational agents."}
{"id": "1803.08419.8fdb4f521d3ba4179f8ccc4c28ba399aab6c3550", "question": "What was the proposed use of conversational agents in pioneering work?", "answer": ["allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries ... conversational assistant called Persona which was one of the first true personal assistant similar to what we have in recent times (like Siri, etc) ... Lets Go project (Raux et al, 2003 BIBREF8 ) that was designed to provide Pittsburgh area bus information", "spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries."], "num_words": 6400, "gpt4_answer": "The proposed use of conversational agents in pioneering work was to build machines that can converse with humans, as seen in early AI literature and popular culture."}
{"id": "1803.08419.a0d45b71feb74774cfdc0d5c6e23cd41bc6bc1f2", "question": "What work pioneered the field of conversational agents?", "answer": [" ESPRIT SUNDIAL project", "ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 )"], "num_words": 6400, "gpt4_answer": "The work that pioneered the field of conversational agents is ELIZA, developed at MIT in 1964."}
{"id": "1908.08917.89414ef7fcb2709c47827f30a556f543b9a9e6e0", "question": "How does this research compare to research going on in the US and USSR at this time?", "answer": ["Author of this research noted the USA prototype effort from 1954 and research papers in 1955as well as USSR effort from 1955. ", "It is worthwhile to note that both the USA and the USSR had access to state-of-the-art computers, and the political support for the production of such systems meant that computers were made available to researchers in machine translation. However, the results were poor in the late 1950s, and a working system was yet to be shown. All work was therefore theoretical work implemented on a computer, which proved to be sub-optimal.", "lagging only a couple of years behind the research of the superpowers"], "num_words": 4397, "gpt4_answer": "The Croatian research group was lagging only a couple of years behind the heavily funded research of the US and USSR, but they had to manage without an actual computer, and their results remained mainly theoretical."}
{"id": "1908.08917.faffcc6ef27c1441e6528f924e320368430d8da3", "question": "What is the reason this research was not adopted in the 1960s?", "answer": [" poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers", "the lack of federal funding ... Laszlos group had to manage without an actual computer", "the lack of funding"], "num_words": 4397, "gpt4_answer": "The Croatian research group's work on machine translation was not adopted in the 1960s due to a lack of federal funding, the unavailability of a computer for building a prototype, and underestimation of the complexity of certain tasks like sentential alignment."}
{"id": "1908.08917.afad388a0141bdda5ca9586803ac53d5f10f41f6", "question": "What is included in the cybernetic methods mentioned?", "answer": ["Separation of the dictionary from the MT algorithm ... Separation of the understanding and generation modules of the MT algorithms ... All words need to be lemmatized ... The word lemma should be the key of the dictionary, ... Use context to determine the meaning of polysemous words.", "compile a dictionary of words sorted from the end of the word to the beginning ... make a word frequency table ... create a good thesaurus"], "num_words": 4397, "gpt4_answer": "The cybernetic methods mentioned in the paper refer to approaches that study analogies between machines and living organisms, focusing on modeling analogical processes and the functioning of the human nervous system. The paper suggests that the Croatian group might have considered using artificial neural networks as a part of their cybernetic approach."}
{"id": "1908.08917.baaa6ad7148b785429a20f38786cd03ab9a2646e", "question": "What were the usual logical approaches of the time period?", "answer": ["The idea was to have a logical intermediate language", "They evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype. ... Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian. ... Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards. ...  Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7.", "to have a logical intermediate language, under the working name Interlingua, which was the connector of both natural languages"], "num_words": 4397, "gpt4_answer": "The usual logical approaches of the time period involved using an intermediary language, often called \"Interlingua,\" to model common-sense human knowledge and connect both natural languages, as well as employing rule-based methods and algorithms for translation."}
{"id": "1908.08917.de346decb1fbca8746b72c78ea9d1208902f5e0a", "question": "What language was this research published in?", "answer": ["Unanswerable"], "num_words": 4397, "gpt4_answer": "The research was published in Croatian."}
{"id": "1804.07445.0bde3ecfdd7c4a9af23f53da2cda6cd7a8398220", "question": "what language was the data in?", "answer": ["English", "English "], "num_words": 2336, "gpt4_answer": "The data was in English."}
{"id": "1804.07445.f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b", "question": "what was the baseline?", "answer": ["Dress ...  Dress-Ls ... Pbmt-R ... Hybrid ... Sbmt-Sari", "Dress ... Dress-Ls ... Pbmt-R ... Hybrid ...  Sbmt-Sari"], "num_words": 2336, "gpt4_answer": "The baseline systems used for comparison include Dress, Dress-Ls, Pbmt-R, Hybrid, and Sbmt-Sari."}
{"id": "1804.07445.051034cc94f2c02d3041575c53f969b3311c9ea1", "question": "which automatic metrics were used in evaluation?", "answer": ["BLEU  ... SARI ", "BLEU ... SARI"], "num_words": 2336, "gpt4_answer": "BLEU and SARI were used as automatic metrics for evaluation."}
{"id": "1804.07445.511e46b5aa8e1ee9e7dc890f47fa15ef94d4a0af", "question": "how do humans judge the simplified sentences?", "answer": ["By fluency, adequacy, and simplicity using a five point Likert scale.", "Rate simplifications with respect to Fluency, Adequacy, and Simplicity, using a five point Likert scale.", "We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale."], "num_words": 2336, "gpt4_answer": "Humans judge simplified sentences based on Fluency, Adequacy, and Simplicity using a five-point Likert scale."}
{"id": "1804.07445.6b4006a90aeaaff8914052d72d28851a9c0c0146", "question": "what datasets were used?", "answer": ["Newsela ... WikiSmall ... WikiLarge", "Newsela BIBREF22 ... WikiSmall BIBREF10 ... WikiLarge BIBREF15"], "num_words": 2336, "gpt4_answer": "The paper used three simplification datasets: (1) Newsela, (2) WikiSmall, and (3) WikiLarge."}
{"id": "1910.03355.eccbbe3684d0cf6b794cb4eef379bb1c8bcc33bf", "question": "What previous approaches are presented for comparison?", "answer": ["Baseline system corresponds to considering the original document as the modernized version. They used two approaches SMT and NMT and compared to the baseline, SMT showed best results.", "prefix-based "], "num_words": 2476, "gpt4_answer": "The paper compares Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) approaches for prefix-based Interactive Machine Translation."}
{"id": "1910.03355.a3705b53c6710b41154c65327b7bbec175bdfae7", "question": "What kind of data is used to train the model?", "answer": ["Dutch Bible ... El Quijote", "Dutch Bible BIBREF1 ... El Quijote BIBREF2 ...  El Conde Lucanor BIBREF2", "Modern and historical versions of literature like the Bible and a Spanish novel."], "num_words": 2476, "gpt4_answer": "The model is trained using historical documents, such as the Dutch Bible from different time periods, El Quijote, and El Conde Lucanor, along with their modernized versions."}
{"id": "1910.03355.b62b7ec5128219f04be41854247d5af992797937", "question": "Does proposed approach use neural networks?", "answer": ["Yes"], "num_words": 2476, "gpt4_answer": "Yes, the proposed approach uses neural networks."}
{"id": "1910.03355.e8fa4303b36a47a5c87f862458442941bbdff7d9", "question": "What machine learning techniques are used in the model architecture?", "answer": ["Classical IMT approaches ... Prefix-based IMT  ... Neural Machine Translation ... Prefix-based Interactive Neural Machine Translation", "NMT systems using NMT-Keras ... SMT systems were trained with Moses ... Statistical IMT systems", "classification for SMT and neural methods for NMT"], "num_words": 2476, "gpt4_answer": "The model architecture uses statistical machine translation (SMT) and neural machine translation (NMT) approaches."}
{"id": "1910.03355.51e9f446d987219bc069222731dfc1081957ce1f", "question": "What language(s) is the model tested on?", "answer": ["Dutch ... Spanish", "Dutch and Spanish"], "num_words": 2476, "gpt4_answer": "The model is tested on Dutch and Spanish historical documents."}
{"id": "1603.09381.13fb28e8b7f34fe600b29fb842deef75608c1478", "question": "By how much did their model outperform baselines?", "answer": ["Answer with content missing: (Table 3) Best proposed result has F1 score of 0.844, 0.813, 0.870, 0.842, 0.844 compared to 0.855, 0.789, 0.852, 0.792, 0.833 on span, modality, degree, polarity and type respectively.", "Their average F1 score is higher than that of baseline by 0.0234 ", "on event expression tasks average by 2.3% with respect to F1; on phase 2 subtask by 11.3% with respect to recall"], "num_words": 1881, "gpt4_answer": "The paper does not provide a specific value or percentage by which their model outperforms the baselines."}
{"id": "1603.09381.d5bce5da746a075421c80abe10c97ad11a96c6cd", "question": "Which baselines did they compare against?", "answer": ["memorization", "memorization baseline", "memorization, median report, max report"], "num_words": 1881, "gpt4_answer": "The paper compared their initial submits RUN 4 and 5 against the memorization baseline on every metric on every task."}
{"id": "1603.09381.930733efb3b97e1634b4dcd77123d4d5731e8807", "question": "What was their performance on this task?", "answer": ["Answer with content missing: (Table 3) Best proposed result has F1 score of 0.844, 0.813, 0.870, 0.842, 0.844 on span, modality, degree, polarity and type respectively.", "Their average F1 score was 0.874 on span detection; 08115 on contextual modality detection; 0.8695 on degree detection; 0.839 on polarity detection; 0.844 on type detection"], "num_words": 1881, "gpt4_answer": "The paper's initial submits (RUN 4 and 5) outperformed the memorization baseline on every metric on every task."}
{"id": "1603.09381.11f9c207476af75a9272105e646df02594059c3f", "question": "What dataset did they use to evaluate?", "answer": ["Clinical TempEval corpus"], "num_words": 1881, "gpt4_answer": "The authors used the Clinical TempEval corpus as the evaluation dataset."}
{"id": "1603.09381.b32de10d84b808886d7a91ab0c423d4fc751384c", "question": "How did they obtain part-of-speech tags?", "answer": ["Answer with content missing: (We then use PerceptronTagger as our part-ofspeech tagger due to its fast tagging speed) PerceptronTagger.", "Using NLTK POS tagger"], "num_words": 1881, "gpt4_answer": "They obtained part-of-speech tags by leveraging basic natural language processing modules such as tokenization and part-of-speech tagging."}
{"id": "1708.05482.9ea3669528c2b295f21770cb7f70d0c4b4389223", "question": "what was their system's f1 score?", "answer": ["0.6955", "69.55"], "num_words": 4237, "gpt4_answer": "The ConvMS-Memnet, their proposed system, achieved an F1-score of 0.5948."}
{"id": "1708.05482.9863f5765ba70f7ff336a580346ef70205abbbd8", "question": "what were the baselines?", "answer": ["RB (Rule based method) ... CB (Common-sense based method) ... RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base) ... SVM ... Word2vec ... Multi-kernel ... CNN ... Memnet", "RB (Rule based method) ... CB (Common-sense based method) ... RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base) ... SVM classifier using the unigram, bigram and trigram features ... SVM classifier using word representations learned by Word2vec ... multi-kernel method BIBREF31 ...  convolutional neural network for sentence classification BIBREF5", "RB (Rule based method) ... CB (Common-sense based method) ... RB+CB+ML ... SVM ... Word2vec ... Multi-kernel ... CNN"], "num_words": 4237, "gpt4_answer": "The baselines were RB (Rule based method), CB (Common-sense based method), RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base), SVM, Word2vec, Multi-kernel, and CNN."}
{"id": "1708.05482.ced63053eb631c78a4ddd8c85ec0f3323a631a54", "question": "what emotion cause dataset was used?", "answer": ["Chinese emotion cause corpus", "a simplified Chinese emotion cause corpus BIBREF31", "simplified Chinese emotion cause corpus BIBREF31"], "num_words": 4237, "gpt4_answer": "The emotion cause dataset used is a simplified Chinese emotion cause corpus from a previous study (BIBREF31)."}
{"id": "1708.05482.f13a5b6a67a9b10fde68e8b33792879b8146102c", "question": "what lexical features are extracted?", "answer": ["Unanswerable", "the distance between a clause and an emotion words"], "num_words": 4237, "gpt4_answer": "The paper does not explicitly mention specific lexical features extracted."}
{"id": "1708.05482.67c16ba64fe27838b1034d15194c07a9c98cdebe", "question": "what word level sequences features are extracted?", "answer": ["Concatenation of three prediction output vectors", "concatenation of three output vectors"], "num_words": 4237, "gpt4_answer": "The ConvMS-Memnet model captures context information for clauses and identifies the most important keyword relating to the emotion cause."}
{"id": "1707.05589.58a3cfbbf209174fcffe44ce99840c758b448364", "question": "what are the recent models they compare with?", "answer": ["BIBREF1 ... Neural Cache BIBREF6 ... BIBREF0", "Recurrent Highway Networks ... NAS ", "Recurrent Highway Networks ... NAS ... BIBREF5"], "num_words": 3176, "gpt4_answer": "The recent models they compare with are LSTMs, Recurrent Highway Networks (RHNs), and Neural Architecture Search (NAS) cells."}
{"id": "1707.05589.6c6e06f7bfb6d30003fd3801fdaf34649ef1b8f4", "question": "what were their results on the hutter prize dataset?", "answer": ["1.30 BPC is their best result", "1.30 and 1.31", "slightly off the state of the art"], "num_words": 3176, "gpt4_answer": "On the Hutter Prize dataset (Enwik8), the authors' models were slightly off the state of the art, with NAS lagging the other models by a surprising margin at this task."}
{"id": "1707.05589.b6e97d1b1565732b1b3f1d74e6d2800dd21be37a", "question": "what was their newly established state of the art results?", "answer": ["58.3 perplexity in PTB, and 65.9 perplexity in Wikitext-2", "At 24M, all depths obtain very similar results, reaching exp(4.065) [fixed,zerofill,precision=1] at depth 4 ... our best result, exp(4.188)"], "num_words": 3176, "gpt4_answer": "The new state of the art results were established on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset."}
{"id": "1707.05589.4f8b078b9f60be30520fd32a3d8601ab3babb5c0", "question": "what regularisation methods did they look at?", "answer": ["Unanswerable", "dropout ... variational dropout ... recurrent dropout"], "num_words": 3176, "gpt4_answer": "The paper does not provide specific details about the regularization methods they looked at."}
{"id": "1707.05589.54517cded8267ea6c9a3f3cf9c37a8d24b3f7c2c", "question": "what architectures were reevaluated?", "answer": ["Answer with content missing: (Architecture section missing) The Long Short-Term Memory, Recurrent Highway Network and NAS", "LSTM, RHN and NAS.", "LSTMs ... Recurrent Highway Networks ... NAS"], "num_words": 3176, "gpt4_answer": "The paper reevaluated LSTM, Recurrent Highway Networks (RHN), and NAS architectures."}
{"id": "1910.14589.803babb71e1bdaf507847d6c712585f4128e9f47", "question": "what baseline models are trained?", "answer": [" Transformer Big BIBREF26", "Transformer Big"], "num_words": 3659, "gpt4_answer": "The baseline models are trained using the Transformer Big architecture, building upon the latest techniques for MT robustness and domain adaptation."}
{"id": "1910.14589.5fd112980d0dd7f7ce30e6273fe6e7b230b13225", "question": "what dataset was used?", "answer": ["11.5k French reviews from Foursquare", "WMT 2014 ...  UGC (User Generated Content)", "WMT 2014 ... UGC (User Generated Content)"], "num_words": 3659, "gpt4_answer": "The dataset used is a French-English parallel corpus of Foursquare restaurant reviews."}
{"id": "1910.14589.eaae11ffd4ff955de2cd6389b888f5fd2c660a32", "question": "what are the human evaluation metrics?", "answer": [" translation quality.", "The outputs are ranked by human evaluators, the wins, ties and losses are counted, then the Wilcoxon signed-rank test is applied.", "number of wins, ties and losses, and apply the Wilcoxon signed-rank test"], "num_words": 3659, "gpt4_answer": "The human evaluation metrics involve bilingual judges ranking the output of 4 models by their translation quality, given the French source sentence in context. The inter-judge Kappa coefficient is used to measure the agreement between judges."}
{"id": "1910.14589.290ebf0d1c49b67a6d1858366be751d89086a78b", "question": "what automatic evaluation is performed?", "answer": ["BLEU BIBREF28 ... indirect evaluation on an Aspect-Based Sentiment Analysis (ABSA) task ...  task-related evaluation based on polysemous words", "BLEU, accuracy"], "num_words": 3659, "gpt4_answer": "The automatic evaluation performed is BLEU (Bilingual Evaluation Understudy) on newstest2014 and Foursquare-test."}
{"id": "1910.14589.806fefe0e331ddb3c17245d6a9fa7433798e367f", "question": "what are the existing online systems?", "answer": ["Existing online systems compared in this work are Google Translate (Feb 2019) and DeepL (Feb 2019).", "Google Translate", "Google Translate, DeepL"], "num_words": 3659, "gpt4_answer": "The paper does not specifically mention the names of the existing online systems."}
{"id": "1801.05617.458e5ed506883bfec6623102ec9f43c071f0616f", "question": "What are their baselines?", "answer": ["Linear-kernel SVM based on word n-grams, vocabulary-based classifier.", "an unoptimised linear-kernel SVM ... a keyword-based system that marks posts as positive for cyberbullying if they contain a word from existing vocabulary lists composed by aggressive language and profanity terms", "unoptimised linear-kernel SVM ... keyword-based system"], "num_words": 6522, "gpt4_answer": "The baselines for the experiments are: (i) an unoptimized linear-kernel SVM (configured with default parameter settings) based on word unigrams only, and (ii) a keyword-based system that marks posts as positive for cyberbullying if they contain a word from existing vocabulary lists composed of aggressive language and profanity terms."}
{"id": "1801.05617.85ab5f773b297bcf48a274634d402a35e1d57446", "question": "Do they report the annotation agreement?", "answer": ["Yes"], "num_words": 6522, "gpt4_answer": "Yes, the inter-annotator agreement scores were calculated using Kappa on a subset of each corpus. The Kappa scores for the identification of cyberbullying are 0.69 (Dutch) and 0.59 (English)."}
{"id": "1801.05617.5154f63c50729b8ac04939588c2f5ffeb916e3df", "question": "How long is the test dataset for Dutch?", "answer": ["78387", "Random 10 percent out of 78381 posts.", "sample ( INLINEFORM2 ) of all data"], "num_words": 6522, "gpt4_answer": "The test dataset for Dutch is not explicitly mentioned in the paper."}
{"id": "1801.05617.2aeabec8a734a6e8ca9e7a308dd8c9a1011b3d6e", "question": "How long is the training dataset for English?", "answer": ["113698", "Random 90 percent out of 113698 posts."], "num_words": 6522, "gpt4_answer": "The English training dataset contains 113,698 posts."}
{"id": "1801.05617.f2b8a2ed5916d75cf568a931829a5a3cde2fc345", "question": "What features are used?", "answer": ["Topic model features ... Subjectivity lexicon features ... Term lists ... Character INLINEFORM0 -gram bag-of-words ... Word INLINEFORM0 -gram bag-of-words", "Word INLINEFORM0 -gram bag-of-words ... Character INLINEFORM0 -gram bag-of-words ... Term lists ... Subjectivity lexicon features ... Topic model features"], "num_words": 6522, "gpt4_answer": "Word n-gram bag-of-words, character n-gram bag-of-words, term lists, and subjectivity lexicon features are used in the experiments."}
{"id": "1801.05617.c0af44ebd7cd81270d9b5b54d4a40feed162fa54", "question": "What is the source of the data?", "answer": [" social networking site ASKfm", "social networking site ASKfm"], "num_words": 6522, "gpt4_answer": "The data for the study was collected from the social networking site ASKfm, where users can create profiles and ask or answer questions, with the option of doing so anonymously."}
{"id": "1905.08067.a4a9971799c8860b50f219c93f050ebf6a627b3d", "question": "What languages feature in the dataset?", "answer": ["English"], "num_words": 4407, "gpt4_answer": "The paper does not explicitly mention the languages in the dataset."}
{"id": "1905.08067.778c6a27182349dc5275282c3e9577bda2555c3d", "question": "What textual, psychological and behavioural patterns are observed in radical users?", "answer": ["They use a lot of \"us\" and \"them\" in their vocabulary. They use a lot of mentions, and they tend to be \"central\" in their network. They use a lot of violent words. "], "num_words": 4407, "gpt4_answer": "Radical users exhibit distinguishable textual, psychological, and behavioural properties. Psychological properties are among the most distinguishing features, and textual models using vector embedding features significantly improve detection over TF-IDF features."}
{"id": "1905.08067.42dcf1bb19b8470993c05e55413eed487b0f2559", "question": "Where is the propaganda material sourced from?", "answer": [" online English magazine called Dabiq", "Dabiq", "English magazine called Dabiq"], "num_words": 4407, "gpt4_answer": "The propaganda material is sourced from Dabiq magazine, an online magazine published by ISIS terrorist groups for recruiting people and promoting their propaganda and ideology."}
{"id": "1905.08067.2ecd12069388fd58ad5f8f4ae7ac1bb4f56497b9", "question": "Which behavioural features are used?", "answer": ["frequency of tweets posted ...  followers/following ratio ... using hashtags ... using mention action", "frequency of tweets posted ... followers/following ratio ... degree of influence each user has over their network", "frequency of tweets posted ... followers/following ratio ... users' interactions with others through using hashtags ... engagement in discussions using mention action"], "num_words": 4407, "gpt4_answer": "The behavioral features used include the number of mentions a single user makes, their HITS hub and authority rank among their interaction network, and how active the user is in interacting with other users and how much attention they receive from their community."}
{"id": "1905.08067.824629b36a75753b1500d9dcaee0fc3c758297b1", "question": "Which psychological features are used?", "answer": ["Analytically thinking ... Clout ... Tone ... Authentic ... Openness ... Conscientiousness ... Extraversion ... Agreeableness ... Neuroticism ... positive emotions ... negative emotions ... personal drives, namely power, reward, risk, achievement, and affiliation ... number of 1st, 2nd, and 3rd personal pronouns used.", "Openness ... Conscientiousness ... Extraversion ... Agreeableness ... Neuroticism", "summary variable - analytically thinking, clout, tone, authentic, Big five variable - openness, conscientiousness, extraversion, agreeableness, neuroticism, Emotional variables - positive emotions in the text, negative emotions in the text, personal drives - power, reward, risk, achievement, affiliation, personal pronouns -  counts the number of 1st, 2nd, and 3rd personal pronouns used, Minkowski distance between each profile and average values of these features created from the ISIS magazines"], "num_words": 4407, "gpt4_answer": "The psychological features used include summary variables (analytical thinking, clout, tone, authenticity), Big Five personality traits (openness, conscientiousness, extraversion, agreeableness, neuroticism), emotional analysis (positive emotions, negative emotions, anger, sadness, anxiety), personal drives (power, reward, risk, achievement, affiliation), and personal pronouns (1st, 2nd, and 3rd personal pronouns)."}
{"id": "1905.08067.31894361833b3e329a1fb9ebf85a78841cff229f", "question": "Which textual features are used?", "answer": ["N-grams ... word2vec", "ratio of violent words in the tweet, ratio of curse words in the tweet, frequency of words with all capital letters, 200 dimension sized vector for the tweet calculated using word embedding, tf-idf scores for top scoring uni-grams, bi-grams and tri-grams", "uni-grams ... bi-grams ... tri-grams"], "num_words": 4407, "gpt4_answer": "The textual features used are Term-Frequency Inverse-Document-Frequency (TF-IDF) scores of uni-, bi-, and tri-grams, and Word embeddings generated from a word2vec model."}
{"id": "1910.03943.cef3a26d8b46cd057bcc2abd3d648dc15336a2bf", "question": "what is the cold-start problem?", "answer": ["Dealing with hotels/items  that  appear infrequently or never in historical data and choosing appropriate weights for them is referred to as the \"cold start problem.\"", "Unanswerable", "hotels/items appear infrequently or never in historical data ... Recommender systems typically have difficulty handling these items effectively due to the lack of relevant training data"], "num_words": 4149, "gpt4_answer": "The cold-start problem refers to the difficulty recommender systems face when handling items with little or no historical data, making it challenging to provide effective recommendations for these items."}
{"id": "1910.03943.636ac549cf4917c5922cd09a655abf278924c930", "question": "how was the experiment evaluated?", "answer": ["Hits@k for hotel context prediction ... Comparison using cosine similarity ... Average intra/inter market embedding similarities ... Visualization of embeddings ... Most similar hotels ... Algebraic operations on hotel embeddings", "the average number of times the correct selection appears in the top k predictions"], "num_words": 4149, "gpt4_answer": "The experiment was evaluated using hits@k metric in various scenarios, such as raw evaluation, filtered evaluation, and cold start problem evaluation. Additionally, the performance of the embeddings was also analyzed using cosine similarity and visualization of embeddings."}
{"id": "1910.03943.c61c0b25f9de4a7ca2013d2e4aba8a5047e14ce4", "question": "what other applications did they experiment in?", "answer": ["None", "Unanswerable"], "num_words": 4149, "gpt4_answer": "The paper focuses on hotel recommendation systems and does not mention experiments in other applications."}
{"id": "1910.03943.1d047286ac63e5dca1ab811172b89d7d125679e5", "question": "what dataset was used for training?", "answer": [" dataset contains more than 40M user click sessions, which includes more than 1.1 million unique hotels", "A dataset containing 40M user click sessions with more than 1.1M unique hotels.", "Our dataset contains more than 40M user click sessions"], "num_words": 4149, "gpt4_answer": "The dataset used for training contains more than 40 million user click sessions, which includes more than 1.1 million unique hotels."}
{"id": "1911.12722.6d17dc00f7e5331128b6b585e78cac0b9082e13d", "question": "Was the entire annotation process done manually?", "answer": ["Yes"], "num_words": 4252, "gpt4_answer": "Yes, the annotation process was performed manually by several student assistants with a background in linguistics and Norwegian as their native language."}
{"id": "1911.12722.de0154affd86c608c457bf83d888bbd1f879df93", "question": "What were the results of their experiment?", "answer": [" .41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\text{F}_1$)", ".41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively"], "num_words": 4252, "gpt4_answer": "The baseline model achieved modest results with 0.41, 0.31, and 0.31 Proportional F1 scores on Holders, Targets, and Polarity Expressions, respectively (0.41, 0.36, 0.56 Binary F1 scores)."}
{"id": "1911.12722.9887ca3d25e2109f41d1da80eeea05c465053fbc", "question": "How big is the dataset?", "answer": ["7451 sentences", "7451 sentences ... 6949 polar expressions ... 5289 targets ... 635 holders", "total of 7451 sentences "], "num_words": 4252, "gpt4_answer": "The dataset comprises 7451 annotated sentences."}
{"id": "1911.12722.87b65b538d79e1218fa19aaac71e32e9b49208df", "question": "What are all the domains the corpus came from?", "answer": [" a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc.", "Unanswerable", "professionally authored reviews from multiple news-sources and across a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc"], "num_words": 4252, "gpt4_answer": "The corpus comes from multiple domains, including literature, video games, music, products, movies, TV-series, stage performance, and restaurants."}
{"id": "1909.00786.075d6ab5dd132666e85d0b6ad238118271dfc147", "question": "How big is benefit in experiments of this editing approach compared to generating entire SQL from scratch?", "answer": ["improvement of 7% question match accuracy and 11% interaction match accuracy", "our model delivers an improvement of 7% question match accuracy and 11% interaction match accuracy over the previous state-of-the-art"], "num_words": 3765, "gpt4_answer": "The editing approach delivers consistent improvements of 7% increase in question matching accuracy and 11% increase in interaction matching accuracy compared to generating the entire SQL from scratch."}
{"id": "1909.00786.f2b1e87f61c65aaa99bcf9825de11ae237260270", "question": "What are state-of-the-art baselines?", "answer": ["For SParC, context-dependent seq2seq and syntaxSQL-con. For Spider, a recursive decoding procedure, graph neural networks, and intermediate representation models.", "SQLNet, SyntaxSQLNet,\nSyntxSQLNet + data augmentation,\nRecursive Decodoing Procedure Lee(2019),\nGNN,\nIRNet and IRNet(BERT)", "guo2019towards who achieve state-of-the-art performance"], "num_words": 3765, "gpt4_answer": "State-of-the-art baselines for the SParC dataset include Context-dependent Seq2Seq (CD-Seq2Seq) and SyntaxSQL-con."}
{"id": "1909.03087.78c7318b2218b906a67d8854f3e511034075f79a", "question": "Which dialogue data do they use to evaluate on?", "answer": ["Datasets from PersonaChat and Wizard of Wikipedia tasks.", "PersonaChat ... Wizard of Wikipedia", "PersonaChat BIBREF5 ... Wizard of Wikipedia BIBREF7"], "num_words": 3950, "gpt4_answer": "They use dialogue data from two tasks, PersonaChat and Wizard of Wikipedia, to evaluate the models."}
{"id": "1909.03087.697c5d2ba7e019ddb91a1de5031a90fe741f2468", "question": "How much faster are pairwise annotations than other annotations?", "answer": ["Unanswerable", "by 5 times"], "num_words": 3950, "gpt4_answer": "The paper does not provide a direct comparison of speed between pairwise annotations and other annotations."}
{"id": "2004.02105.e25b73f700e8c958b64951f14a71bc60d225125c", "question": "How much improvement is there in the BLEU score?", "answer": ["Average SacreBLEU score accross all domains is improved from 40.88 to 41.26.", "On average the three selection methods had better BLEU scores than Random and Oracle methods. \nThe proposed method Domain-Finetune-Top-500k had better BLEU score than random by 4.34, better than Moore-Lewis by 0.38, better than Oracle by 0.92, and better than All method by 1.4"], "num_words": 4933, "gpt4_answer": "The paper does not provide a specific overall improvement in BLEU score, as the improvements vary across different domains and methods. However, the Domain-Finetune method generally performs best, showing better performance than other data selection methods and oracle in-domain training across all five domains in the benchmark."}
{"id": "2004.02105.908ba58d26d15c14600623498d4e86c9b73b14b2", "question": "What is the established approach used for comparison?", "answer": ["established method for data selection was proposed by BIBREF4", "method of BIBREF4"], "num_words": 4933, "gpt4_answer": "The established approach used for comparison is the method proposed by Moore and Lewis (BIBREF4), which involves comparing the cross-entropy according to domain-specific and non-domain-specific language models for each candidate sentence for selection."}
{"id": "2004.02105.3e0fd1a3944e207edbbe7c7108239dbaf3bccd4f", "question": "What are the five domains?", "answer": ["subtitles ... medical ... legal ... Koran ... IT", "subtitles ... medical text ... legal text ... translations of the Koran ... IT-related text", "subtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software)"], "num_words": 4933, "gpt4_answer": "The five domains are subtitles, medical text, legal text, translations of the Koran, and IT-related text."}
{"id": "2004.02105.c0847af3958d791beaa14c4040ada2d364251c4d", "question": "Which pre-trained language models are used?", "answer": ["BERT ... DistilBERT ... RoBERTa", "BERT ... DistilBERT ... RoBERTa ... GPT-2 ... XLNet"], "num_words": 4933, "gpt4_answer": "The pre-trained language models used in this paper include BERT, DistilBERT, RoBERTa, GPT-2, and XLNet."}
{"id": "1909.01720.2f142cd11731d29d0c3fa426e26ef80d997862e0", "question": "Do they report results only on English data?", "answer": ["Unanswerable", "Yes"], "num_words": 4006, "gpt4_answer": "Yes, the experiments are performed on two public datasets for fake news detection and stance detection, RumourEval and PHEME, which contain English Twitter conversation threads."}
{"id": "1909.01720.ce23849e9e9a22626965f1ca8ca948a5c87280e9", "question": "What are the hyperparameter setting of the MTL model?", "answer": ["Size of word embeddings is 200, size of position embeddings is 100, 6 attention heads and 2 blocks in encoder, dropout in multi-head attention is 0.7, minibatch size is 64, initial learning rate is 0.001, dropout rate is 0.3, lambda is 0.6.", "The sizes of word embeddings and position embeddings are set to 200 and 100, attention heads and blocks are set to 6 and 2 respectively, and the dropout of multi-head attention is set to 0.7,  the minibatch size is 64, the initial learning rate is set to 0.001, the dropout rate to 0.3, and $\\lambda $ to 0.6 for fake news detection.", "size of word embeddings is 200, size of position embedding is 100, the number of attention heads in transformer block is 6, the number of attention block Is 2, dropout of multi-head attention is 0.7, minibatch size is 64, the initiall learning rate is .001. In fake news detection, the dropout rate is 0.3 and lambda is 0.6."], "num_words": 4006, "gpt4_answer": "The hyperparameter settings of the MTL model include a minibatch size of 64, an initial learning rate of 0.001, a dropout rate of 0.3, and a weight of 0.6 for fake news detection."}
{"id": "1909.01720.d9a45fea8539aac01dec01f29b7d04b44b9c2ca6", "question": "What architecture does the rest of the multi-task learning setup use?", "answer": ["shared features in the shared layer are equally sent to their respective tasks without filtering", "transformer"], "num_words": 4006, "gpt4_answer": "The multi-task learning setup uses a shared-private model with a selected sharing layer that includes a gated sharing cell for filtering useless features and an attention sharing cell for focusing on valuable shared features for specific tasks."}
{"id": "1909.01720.246e924017c48fa1f069361c44133fdf4f0386e1", "question": "How is the selected sharing layer trained?", "answer": ["By jointly training the tasks of stance and fake news detection.", "The selected sharing layer is trained jointly on the tasks of stance detection and fake news detection"], "num_words": 4006, "gpt4_answer": "The selected sharing layer is trained by relying on the gate mechanism and attention mechanism, which selectively captures valuable shared features between the tasks of fake news detection and stance detection for respective tasks."}
{"id": "1908.10090.96459b02efa82993a0b413530ed0b517c6633eea", "question": "what were the length constraints they set?", "answer": ["They set translation length longer than minimum 0.25 times the source sentence length", "search to translations longer than 0.25 times the source sentence length ... search to either the length of the best Beam-10 hypothesis or the reference length"], "num_words": 2042, "gpt4_answer": "The length constraints they set were translations longer than 0.25 times the source sentence length and constraining exact search to either the length of the best Beam-10 hypothesis or the reference length."}
{"id": "1908.10090.6c1614991647705265fb348d28ba60dd3b63b799", "question": "what is the test set size?", "answer": ["2,169 sentences"], "num_words": 2042, "gpt4_answer": "The test set size is 2,169 sentences."}
{"id": "1905.05644.b948bb86855b2c0bfc8fad88ff1e29cd94bb6ada", "question": "what are the evaluation metrics used?", "answer": ["BLEU-4 ... slot error rate", "BLEU-4 ... slot error rate ... informativeness ... naturalness", "informativeness  ... naturalness "], "num_words": 3657, "gpt4_answer": "The evaluation metrics used are BLEU-4 score and slot error rate (ERR)."}
{"id": "1905.05644.157284acedf13377cbc6d58c8f3648d3a62f5db5", "question": "what other training procedures were explored?", "answer": ["Scratch-NLG ... MTL-NLG ... Zero-NLG ... Supervised-NLG"], "num_words": 3657, "gpt4_answer": "Other training procedures explored include Scratch-NLG, MTL-NLG, Zero-NLG, and Supervised-NLG."}
{"id": "1910.07154.e4ea0569b637d5f56f63e933b8f269695fe1a926", "question": "What baseline did they use?", "answer": ["HexaF", "HexaF - UCL ", "we compare the label accuracy of SUPPORTS label against a supervised approach  HexaF"], "num_words": 2090, "gpt4_answer": "The authors used a supervised approach, HexaF, as a baseline for comparison."}
{"id": "1910.07154.e3c44964eb6ddc554901244eb6595f26a9bae47e", "question": "What is the threshold?", "answer": ["0.76 ... 0.67", "0.76 suggests that at least 3 out of the 4 questions have to be answered correctly ... 0.67 suggests that at least 2 out of the 3 questions has to be answered correctly"], "num_words": 2090, "gpt4_answer": "The classification threshold ($\\phi$) is derived empirically based on the precision-recall curve. In the paper, two different thresholds, 0.76 and 0.67, are compared."}
{"id": "1910.07154.905a8d775973882227549e960c7028e4a3561752", "question": "How was the masking done?", "answer": ["The named entities are then used to generate the questions by masking the entities for the subsequent stage.", "Unanswerable", "similar to a Cloze-task or masked language modeling task where the named entities are masked with a blank"], "num_words": 2090, "gpt4_answer": "The masking was done by using Stanford CoreNLP NER tagger to convert the claim into a Cloze-task by masking the named entities."}
{"id": "1910.07154.76f90c88926256e7f90d2104a88acfdd7fc5475e", "question": "How large is the FEVER dataset?", "answer": ["185k claims", "around 185k claims from the corpus of 5.4M Wikipedia articles"], "num_words": 2090, "gpt4_answer": "The FEVER dataset contains around 185k claims."}
{"id": "1901.09501.182eb91090017a7c8ea38a88b219b641842664e4", "question": "How do they obtain structured data?", "answer": ["The structured data is obtained from the box-score tables.", "split each game report into individual sentences, and, for each sentence, find its corresponding data in the box-score table as the content record ... we found some simple rules are sufficient to obtain high-quality results"], "num_words": 3791, "gpt4_answer": "To obtain structured data, the authors split each game report into individual sentences and, for each sentence, find its corresponding data in the box-score table as the content record. They use simple rules to obtain high-quality results in this step."}
{"id": "1901.09501.0ef114d24a7a32821967e912dff23c016c4eab41", "question": "Which prior approaches for style transfer do they test with?", "answer": ["AttnCopy-S2S ... Rule-based Method ... Multi-Attribute Style Transfer (MAST) BIBREF11 ... Adversarial Style Transfer (AdvST) BIBREF12", "Multi-Attribute Style Transfer ... Adversarial Style Transfer", "Multi-Attribute Style Transfer ... Adversarial Style Transfer "], "num_words": 3791, "gpt4_answer": "They test with Multi-Attribute Style Transfer (MAST) and Adversarial Style Transfer (AdvST)."}
{"id": "1901.09501.67672648e7ebcbef18921006e2c8787966f8cdf2", "question": "Which competing objectives for their unsupevised method do they use?", "answer": ["A combination of Content Objective and Style Objective", "Reconstructing the auxiliary sentence and reconstructing the reference sentence."], "num_words": 3791, "gpt4_answer": "The unsupervised method uses two competing objectives: reconstructing the auxiliary sentence (for content fidelity) and reconstructing the reference sentence (for style preservation)."}
{"id": "1901.09501.c32fc488f0527f330273263fa8956788bd071efc", "question": "Which content coverage constraints do they design?", "answer": ["Answer with content missing: (Content Coverage Constraint section) We thus devise an additional learning constraint based on the nature of content descriptioneach data tuple in the content record should usually be mentioned exactly once in the generated sentence.\nThe copy mechanism over content record x enables a simple yet effective way to encourage the behavior. Intuitively, we want each tuple to be copied once and only once on average.", "Unanswerable"], "num_words": 3791, "gpt4_answer": "The authors design an explicit content coverage constraint that encourages the model to precisely and fully convey the structured content."}
{"id": "1705.02023.8908d1b865137bc309dde10a93735ec76037e5f9", "question": "what were the evaluation metrics?", "answer": ["Unanswerable", "macro-average recall"], "num_words": 1441, "gpt4_answer": "The evaluation metric was macro-average recall."}
{"id": "1705.02023.d207f78beb6cd754268881bf575c8f98000667ea", "question": "how many sentiment labels do they explore?", "answer": ["3"], "num_words": 1441, "gpt4_answer": "They explore three sentiment labels: positive, negative, and neutral."}
{"id": "1912.06203.35c01dc0b50b73ee5ca7491d7d373f6e853933d2", "question": "Which dataset do they use for text altering attributes matching to image parts?", "answer": ["CUB bird ... COCO", "Unanswerable"], "num_words": 4237, "gpt4_answer": "The authors use the CUB bird dataset and the COCO dataset for text-guided image manipulation."}
{"id": "1912.06203.c077519ea42c9649fb78da34485de2262a0df779", "question": "Is it possible for the DCM module to correct text-relevant content?", "answer": ["No", "Unanswerable", "Yes"], "num_words": 4237, "gpt4_answer": "Yes, the Detail Correction Module (DCM) can correct inappropriate attributes, including text-relevant content, and reconstruct text-irrelevant contents."}
{"id": "1809.05807.a51c680a63ee393792d885f66de75484dc6bc9bc", "question": "Is an ablation test performed?", "answer": ["No"], "num_words": 3837, "gpt4_answer": "No, an ablation test is not mentioned in the paper."}
{"id": "1809.05807.e752dc4d721a2cf081108b6bd71e3d10b4644354", "question": "What statistical test is performed?", "answer": ["Unanswerable", "t-test"], "num_words": 3837, "gpt4_answer": "The t-test is performed."}
{"id": "1904.02954.c79f168503a60d1b08bb2c9aac124199d210b06d", "question": "Which downstream tasks are used for evaluation in this paper?", "answer": ["Argument component detection ... ACE Entities/Events ... POS ... Chunking ... WNUT16 ... CoNLL 2003 shared task on named entity recognition ... GENIA NER", "Various sequence tagging tasks: Argument detection, ACE entity and event detection, part-of-speech tagging, CoNLL chunking, CoNLL named entity recognition, GENIA bio-entity recognition, WNUT named entity recognition. They also evaluate on Stanford Sentiment Treebank, Penn TreeBank constituency parsing, and Stanford Natural Language Inference."], "num_words": 2400, "gpt4_answer": "The downstream tasks used for evaluation in this paper include argument component detection, ACE Entities/Events, part-of-speech tagging, chunking, named entity recognition (NER), GENIA NER, and WNUT16 NER over Twitter."}
{"id": "1904.02954.9dd8ce48a2a59a63ae6366ab8b2b8828e5ae7f35", "question": "Which datasets are used for evaluation?", "answer": ["Argument detection, ACE 2005, Universal Dependencies part-of-speech tags, CoNLL 2000 chunking shared task, CoNLL 2003 named entity recognition shared task, GENIA NER Bio-Entity Recognition, WNUT16 Twitter named entity recognition shared task, Stanford Sentiment Treebank, Penn TreeBank constituency parsing, Stanford Natural Language Inference corpus", "Arguments ... ACE 2005 dataset ... part-of-speech tags from Universal Dependencies v. 1.3 for English ... CoNLL 2000 shared task dataset on chunking ... CoNLL 2003 shared task on named entity recognition ... GENIA NER ... WNUT16", "For the first experiment, the datasets used were: argument component detection persuasive essays, ACE 2005 dataset of entities/essays, POS tags from Universal Dependencies, CoNLL 2000 shared task on chunking, CoNLL 2003\nshared task on named entity recognition, the Bio-Entity Recognition Task dataset, WNUT 16 dataset on NER over tweets. For the second experiment, they used the CoNLL 2003 NER\ndataset, the Stanford Sentiment Treebank (SST5) dataset, the constituency parsing model for the\nPenn TreeBank as dataset, and the Stanford Natural Language Inference Corpus (SNLI) dataset."], "num_words": 2400, "gpt4_answer": "The datasets used for evaluation are Arguments, ACE Entities/Events, POS, Chunking, NER, GENIA NER, and WNUT16."}
{"id": "1804.03839.5cc5e2db82f5d40a5244224dad94da50b4f673db", "question": "What does the human-in-the-loop do to help their system?", "answer": ["appropriately modify the text to create an unbiased version", "identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it", "modify the text to create an unbiased version"], "num_words": 2076, "gpt4_answer": "The human-in-the-loop takes the text, analyzes it, and uses the meaningful clues or pieces of evidence provided by the system to appropriately modify the text to create an unbiased version."}
{"id": "1804.03839.ab975efc916c34f55e1144b1d28e7dfdc257e371", "question": "Which dataset do they use to train their model?", "answer": ["1) Occupation Data ... 2) Names Data", "A dataset they created that contains occupation and names data."], "num_words": 2076, "gpt4_answer": "The paper does not mention using a specific dataset to train a model. Instead, it focuses on using occupation and names datasets for tagging and checking biases in the text."}
{"id": "1804.03839.e7ce612f53e9be705cdb8daa775eae51778825ef", "question": "Can their approach be extended to eliminate racial or ethnic biases?", "answer": ["Unanswerable"], "num_words": 2076, "gpt4_answer": "Yes, their approach can potentially be extended to eliminate racial or ethnic biases by adapting the methodology to focus on those dimensions of biases."}
{"id": "1804.03839.6c5a64b5150305c584326882d37af5b0e58de2fd", "question": "How do they evaluate their de-biasing approach?", "answer": ["Unanswerable"], "num_words": 2076, "gpt4_answer": "The paper does not provide a specific evaluation method for their de-biasing approach."}
{"id": "2004.02214.f7a27de3eb6447377eb48ef6d2201205ff943751", "question": "Is there a metric that also rewards good stylistic response?", "answer": ["Yes"], "num_words": 3406, "gpt4_answer": "Yes, the \"Style Expression\" metric measures how well the generated responses express the desired style, with scores ranging from 1 to 5."}
{"id": "2004.02214.2df3cd12937591481e85cf78c96a24190ad69e50", "question": "What are existing baseline models on these benchmark datasets?", "answer": ["Generative Approaches ::: Seq2seq ... Generative Approaches ::: GPT2-FT: ... Generative Approaches ::: Speaker: ... Generative Approaches ::: ECM: ... Retrieval-Based Approaches ::: Skeleton-to-Response (SR) ... Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST) ... Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST) ... Retrieval-Based Approaches ::: Retrieval + Reranking (RRe)", "Seq2seq ... GPT2-FT ... Speaker ... ECM ... Skeleton-to-Response (SR) ... Retrieval + Style Transfer (RST) ... Retrieval + Reranking (RRe)"], "num_words": 3406, "gpt4_answer": "Existing baseline models on these benchmark datasets include Seq2seq, GPT2-FT, Speaker, ECM, Skeleton-to-Response (SR), Retrieval + Style Transfer (RST), and Retrieval + Reranking (RRe)."}
{"id": "2004.02214.fcb0ac1934e2fd9f58f4b459e6853999a27844f9", "question": "On what two languages is experimented on?", "answer": ["Chinese ... English", "Chinese and English", "English and Chinese"], "num_words": 3406, "gpt4_answer": "Two languages experimented on are Chinese and English."}
{"id": "2004.02214.fc9aa04de4018b7d55e19a39663a2e9837328de7", "question": "What three benchmark datasets are used?", "answer": ["Gender-Specific Dialogue Dataset ... Emotion-Specific Dialogue Dataset ... Sentiment-Specific Dialogue Dataset", "gender-specific (Chinese) dataset ... emotion-specific (Chinese) dataset ... sentiment-specific (English) dataset"], "num_words": 3406, "gpt4_answer": "The three benchmark datasets used are gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, and sentiment-specific (English) dataset."}
{"id": "1911.05652.044cb5ef850c0a2073682bb31d919d504667f907", "question": "What IS versification?", "answer": ["Unanswerable", "the ratios of iambic lines ending with a stressed syllable (The view of earthly glory: men might say) to lines ending with an extra unstressed one (Till this time pomp was single, but now married)", "the ratios of iambic lines ending with a stressed syllable (The view of earthly glory: men might say) to lines ending with an extra unstressed one (Till this time pomp was single, but now married), pointing out that the distribution of values across scenes is strongly bimodal."], "num_words": 2388, "gpt4_answer": "Versification is the art or practice of creating metrical patterns in poetry, including the arrangement of rhythmic structures, rhyme schemes, and the use of various poetic forms."}
{"id": "1911.05652.c845110efee2f633d47f5682573bc6091e8f5023", "question": "How confident is the conclusion about Shakespeare vs Flectcher?", "answer": ["high reliability", "very"], "num_words": 2388, "gpt4_answer": "The conclusion about Shakespeare and Fletcher's collaboration in H8 is highly confident, as the combined versification-based and word-based models yield a high accuracy of authorship recognition."}
{"id": "1911.05652.2301424672cb79297cf7ad95f23b58515e4acce8", "question": "Is Henry VIII reflective of Shakespeare in general?", "answer": ["No", "Yes"], "num_words": 2388, "gpt4_answer": "Henry VIII is not reflective of Shakespeare's work in general, as it is a collaborative work involving other authors, such as John Fletcher, in addition to Shakespeare."}
{"id": "1911.05652.6c05376cd0f011e00d1ada0254f6db808f33c3b7", "question": "Is vocabulary or versification more important for the analysis?", "answer": ["Unanswerable", "Yes"], "num_words": 2388, "gpt4_answer": "Both vocabulary and versification are important for the analysis, as combined models provide a reliable discriminator between Shakespeare's, Fletcher's, and Massinger's styles."}
{"id": "1911.05652.9925e7d8757e8fd7411bcb5250bc08158a244fb3", "question": "What are the modifications by Thomas Merriam?", "answer": ["Thomas Merriam proposed a modification to Speddings original attribution concerning re-attribution of several parts of supposedly Fletchers scenes back to Shakespeare and vice versa.", "a modification to Speddings original attribution concerning re-attribution of several parts of supposedly Fletchers scenes back to Shakespeare and vice versa.", "measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeares and Fletchers plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. "], "num_words": 2388, "gpt4_answer": "Thomas Merriam proposed modifications to Spedding's original attribution concerning re-attribution of several parts of supposedly Fletcher's scenes back to Shakespeare and vice versa, based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare's and Fletcher's plays, controversial CUSUM technique, and principal component analysis of 64 most frequent words."}
{"id": "1911.05652.fa468c31dd0f9095d7cec010f2262eeed565a7d2", "question": "What are stop words in Shakespeare?", "answer": ["Unanswerable"], "num_words": 2388, "gpt4_answer": "Unanswerable"}
{"id": "1703.10090.8c89f1d1b3c2a45c0254c4c8d6e700ab9a4b4ffb", "question": "What sources of less sensitive data are available?", "answer": ["MIMICII(I), THYME, results from i2b2 and ShARe/CLEF shared task, MiPACQ, Blulab, EMC Dutch Clinical Corpus, 2010 i2b2/VA, VetCompass", "deceased persons ... surrogate data ... derived data ... veterinary texts", "personal health information of deceased persons ... surrogate data ... derived data. Data that can not be used to reconstruct the original text ... veterinary texts"], "num_words": 3148, "gpt4_answer": "Sources of less sensitive data include health-related texts from social media, patient-generated data available online, derived data that cannot reconstruct the original text, randomized subsets of clinical notes, and veterinary texts."}
{"id": "1703.10090.f5bc07df5c61dcb589a848bd36f4ce9c22abd46a", "question": "Other than privacy, what are the other major ethical challenges in clinical data?", "answer": ["Texts produced in the clinical settings do not always tell a complete or accurate patient story (e.g. due to time constraints or due to patient treatment in different hospitals), yet important decisions can be based on them. As language is situated, a lot of information may be implicit, such as the circumstances in which treatment decisions are made ... discrimination can occur when individuals or groups receive unfair treatment as a result of automated processing, which might be a result of biases in the data that were used to train models. ... Clinical texts may include bias coming from both patient's and clinician's reporting. ... prejudices held by healthcare practitioners which may impact patients' perceptions ... communication difficulties in the case of ethnic differences ... Observational bias Although variance in health outcome is affected by social, environmental and behavioral factors, these are rarely noted in clinical reports ... Dual use", "sampling bias, unfair treatment due to biased data,  incomplete clinical stories, and reflection of health disparities."], "num_words": 3148, "gpt4_answer": "Other major ethical challenges in clinical data include social impact, biases (such as sampling bias, data quality, reporting bias, and observational bias), and dual use of data for both beneficial and harmful purposes."}
{"id": "1905.10039.8126c6b8a0cab3e22661d3d71d96aa57360da65c", "question": "what evaluation metrics were used?", "answer": ["EM INLINEFORM0  ... EM INLINEFORM0 ... Rouge INLINEFORM0", "EM-outline, EM-sec, Rouge", "EMoutline, EMsec, Rougehead"], "num_words": 6043, "gpt4_answer": "The evaluation metrics used were EM-Outline, EM-Boundary, and Rouge-1."}
{"id": "1905.10039.2f01d3e5120d1fef4b01028536cb5fe0abad1968", "question": "what state of the art models did they compare with?", "answer": ["HiStGen_P, HiStGen_S, HiStGen_H, HiStGen_R, HiStGen_PSHR, IGCRF+TextRank, IGCRF+TopicRank, IGCRF+Hier, IGCRF+GHD, IGGPD+TextRank, IGGPD+TopicRank, IGGPD+Hier, IGGPD+GHD, GATextRank, GATopicRank, GAHier, GAGHD", "IG CRF+GHD"], "num_words": 6043, "gpt4_answer": "The paper does not explicitly mention specific state-of-the-art models they compared with. Instead, they compared their HiStGen model with several degraded versions of HiStGen and other baseline methods, such as First-Identify-then-Generate (IG) and First-Generate-then-Aggregate (GA) approaches using TextRank, TopicRank, Hier, and GHD methods."}
{"id": "1704.06851.b78bb6fe817c2d4bc69236df998f546e94c3ee21", "question": "Is the performance improvement (with and without affect attributes) statistically significant?", "answer": ["Yes"], "num_words": 4538, "gpt4_answer": "The average perplexity improvement is 1.44 (relative improvement 1.94%) for the model trained on Fisher, while it is 0.79 (1.31%) for the adapted models, indicating that the performance improvement with affect attributes is statistically significant."}
{"id": "1704.06851.1a419468d255d40ae82ed7777618072a48f0091b", "question": "How to extract affect attributes from the sentence?", "answer": ["A sentence is represented by five features that each mark presence or absence of an emotion: positive emotion, angry, sad, anxious, and negative emotion.", "Using a dictionary of emotional words, LIWC, they perform keyword spotting.", "either (1) inferred from the context using LIWC (this occurs when we provide sentence beginnings which are emotionally colored themselves), or (2) set to an input emotion descriptor $\\mathbf {e}$"], "num_words": 4538, "gpt4_answer": "The affect attributes are extracted from the sentence using the Linguistic Inquiry and Word Count (LIWC) text analysis program for feature extraction through keyword spotting."}
{"id": "1911.06815.52f5249a9a2cb7210eeb8e52cb29d18912f6c3aa", "question": "How many layers does the neural network have?", "answer": ["Unanswerable"], "num_words": 1897, "gpt4_answer": "The paper does not provide an exact number of layers for the neural network."}
{"id": "1911.06815.baad4b6f834d5944f61bd12f30908e3cf3739dcd", "question": "Which BERT-based baselines do they compare to?", "answer": ["BERT ... BERT-Joint ... BERT-Granularity", "BERT with one separately trained linear layer for each of the two tasks, BERT-Joint, which trains a layer for both tasks jointly, BERT-Granularity,  a modification of BERT-Joint which transfers information from the less granular task to the more granular task. ", "BERT. We add a linear layer on top of BERT and we fine-tune it ... BERT-Joint. We use the layers for both tasks in the BERT baseline, $L_{g_1}$ and $L_{g_2}$, and we train for both FLC and SLC jointly (cf. Figure FIGREF7-b). ... BERT-Granularity. We modify BERT-Joint to transfer information from SLC directly to FLC"], "num_words": 1897, "gpt4_answer": "They compare to three BERT-based baselines: BERT, BERT-Joint, and BERT-Granularity."}
{"id": "1911.06815.37b972a3afae04193411dc569f672d802c16ad71", "question": "What are the propaganda types?", "answer": ["Although not all of the 18 types are listed, they include using loaded language or appeal to authority and slogans, using logical fallacies such as strawmen, hidden ad-hominen fallacies ad red herrings. ", "annotated according to eighteen persuasion techniques BIBREF4"], "num_words": 1897, "gpt4_answer": "The paper does not provide an exhaustive list of propaganda types, but it mentions some examples such as loaded language, appeal to authority, slogans, straw men, hidden ad-hominem fallacies, and red herrings."}
{"id": "1911.06815.a01af34c7f630ba0e79e0a0120d2e1c92d022df5", "question": "Do they look at various languages?", "answer": ["No", "Unanswerable"], "num_words": 1897, "gpt4_answer": "In future work, they plan to include more media sources, especially from non-English-speaking media and regions."}
{"id": "1911.06815.0c4e419fe57bf01d58a44f3e263777c22cdd90dc", "question": "What datasets did they use in their experiment?", "answer": ["451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques BIBREF4", "A dataset of news articles from different news outlets collected by the authors.", "retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques"], "num_words": 1897, "gpt4_answer": "They used a dataset of 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which were annotated by professional annotators with eighteen persuasion techniques."}
{"id": "1704.08390.7b76b8b69246525a48c0a8ca0c42db3319cd10a5", "question": "What size ngram models performed best? e.g. bigram, trigram, etc.", "answer": ["advantage of bigrams on Subtask A was very slight", "bigram ", "the trigram language model performed better on Subtask B ... the bigram language model performed better on Subtask A"], "num_words": 1909, "gpt4_answer": "Bigram and trigram language models performed best in this study."}
{"id": "1704.08390.8b1af67e3905244653b4cf66ba0acec8d6bff81f", "question": "How were the ngram models used to generate predictions on the data?", "answer": ["The n-gram models were used to calculate the logarithm of the probability for each tweet", "The system sorts all the tweets for each hashtag and orders them based on their log probability score, where the funniest tweet should be listed first", "system sorts all the tweets for each hashtag and orders them based on their log probability score"], "num_words": 1909, "gpt4_answer": "The N-gram models were used to assign a logarithm of probability to each tweet based on the trained language model, and then the tweets were sorted based on their log probability scores to predict which tweet is funnier."}
{"id": "1704.08390.9a7aeecbecf5e30ffa595c233fca31719c9b429f", "question": "What package was used to build the ngram language models?", "answer": ["KenLM Toolkit"], "num_words": 1909, "gpt4_answer": "KenLM was used to build the N-gram language models."}
{"id": "1704.08390.3605ea281e72e9085a0ac0a7270cef25fc23063f", "question": "What rank did the language model system achieve in the task evaluation?", "answer": ["4th place on SubtaskA; 1st place on Subtask B", "Unanswerable"], "num_words": 1909, "gpt4_answer": "The paper does not provide a specific rank for the language model system in the task evaluation."}
{"id": "1704.08390.21f6cb3819c85312364dd17dd4091df946591ef0", "question": "What were subtasks A and B?", "answer": ["For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets. ... For Subtask B, the system outputs all the tweet_ids for a hashtag file starting from the funniest."], "num_words": 1909, "gpt4_answer": "Subtask A involved comparing pairs of tweets and determining which one was funnier, while Subtask B required semi-ranking tweets in order of their humor from funniest to least funny."}
{"id": "1911.03642.fd8a8eb69f07c584a76633f8802c2746f7236d64", "question": "Do the authors report only on English", "answer": ["Yes"], "num_words": 3559, "gpt4_answer": "Yes, the authors evaluate gender bias in English language predictions."}
{"id": "1911.03642.452e978bd597411b65be757bf47dc6a78f3c67c9", "question": "How does counterfactual data augmentation affect gender bias in predictions and performance?", "answer": ["Unanswerable", "mitigates these contextual biases"], "num_words": 3559, "gpt4_answer": "Counterfactual Data Augmentation (CDA) effectively mitigates gender bias in the model's predictions and improves the model's performance by equalizing the contexts for feminine and masculine words."}
{"id": "1911.03642.159025c44c0115ab4cdc253885384f72e592e83a", "question": "How does hard debiasing affect gender bias in prediction and performance?", "answer": ["Hard-Debiased Word Embeddings was also extremely effective at mitigating the difference in F1 scores for all relations", "Unanswerable", "mitigating the difference in F1 scores for all relations ... debiased embeddings increases absolute score"], "num_words": 3559, "gpt4_answer": "Hard debiasing effectively mitigates gender bias in predictions and slightly increases the absolute F1 scores for both male and female sentences for all relations."}
{"id": "1911.03642.6590055fb033cb32826f2afecb3d7f607dd97d57", "question": "How does name anonymization affect gender bias in predictions and performance?", "answer": ["Name Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations.", "substantially increases F1 score gap for the hypernym relation ... slightly decreases F1 score gap for all other relations"], "num_words": 3559, "gpt4_answer": "Name Anonymization slightly decreases the F1 score gap for all relations except for the hypernym relation, where it substantially increases the gap. It is effective at debiasing all relations aside from hypernym but not as effective as gender-swapping or using debiased embeddings."}
{"id": "1911.03642.3435e365adf7866e45670c865dc33bb7d2a6a0c6", "question": "How are the sentences in WikiGenderBias curated?", "answer": ["Distant Supervision assumption ... any sentence from an article written about one of those entities that mentions the other entity expresses the relation", "for a given relation between two entities, assume that any sentence from an article written about one of those entities that mentions the other entity expresses the relation"], "num_words": 3559, "gpt4_answer": "To generate WikiGenderBias, a variant of the Distant Supervision assumption is used: for a given relation between two entities, it is assumed that any sentence from an article written about one of those entities that mentions the other entity expresses the relation. The dataset is created using Wikipedia articles and relation information from DBPedia."}
{"id": "1911.01214.cd82bdaa0c94330f8cccfb1c59b4e6761a5a4f4d", "question": "what crowdsourcing platform did they use?", "answer": [" Amazon Mechanical Turk", "Amazon Mechanical Turk"], "num_words": 5358, "gpt4_answer": "The authors used Amazon Mechanical Turk as the crowdsourcing platform for their annotations."}
{"id": "1911.01214.753a187c1dd8d96353187fbb193b5f86293a796c", "question": "did they crowdsource annotations?", "answer": ["Yes"], "num_words": 5358, "gpt4_answer": "Yes, they crowdsourced annotations using Amazon Mechanical Turk for tasks like stance annotation and fine-grained evidence (FGE) annotation."}
{"id": "1911.01214.29794bda61665a1fbe736111e107fd181eacba1b", "question": "where does their data come from?", "answer": ["Snopes", "Snopes ", "Snopes fact-checking website"], "num_words": 5358, "gpt4_answer": "The data for their corpus comes from the Snopes fact-checking website."}
{"id": "1911.01214.dd80a38e578443496d3720d883ad194ce82c5f39", "question": "which existing corpora do they compare with?", "answer": ["FEVER shared task BIBREF7 and the Fake News Challenge BIBREF8", "PolitiFact14 ... Emergent16 ... PolitiFact17 ... RumourEval17 ... Snopes17 ... CLEF-2018 ... FEVER18"], "num_words": 5358, "gpt4_answer": "The authors compare their corpus with existing corpora such as PolitiFact14, Emergent16, PolitiFact17, RumourEval17, Snopes17, CLEF-2018, and FEVER18."}
{"id": "1911.01214.9a9774eacb8f75bcfa07a4e60ed5eb02646467e3", "question": "what is the size of their corpus?", "answer": ["6,422", "Corpus has 6422 claims, 16509 ETSs, 8291 FGE sets and 14296 ODCs."], "num_words": 5358, "gpt4_answer": "The corpus consists of 6,422 validated claims."}
{"id": "1911.01214.4ed58d828cd6bb9beca1471a9fa9f5e77488b1d1", "question": "which architectures did they experiment with?", "answer": ["AtheneMLP ... DecompAttent BIBREF20 ... USE+Attent", "For stance detection they used MLP, for evidence extraction they used Tf-idf and BiLSTM, for claim validation they used MLP, BiLSTM and SVM"], "num_words": 5358, "gpt4_answer": "They experimented with AtheneMLP, DecompAttent, USE+Attent, rankingESIM, BiLSTM, Tf-Idf, BertEmb, extendedESIM, and USE+MLP architectures."}
{"id": "1911.01214.de580e43614ee38d2d9fc6263ff96e6ca2b54eb5", "question": "what domains are present in the corpus?", "answer": ["corpus covers multiple domains, including discussion blogs, news, and social media", "discussion blogs ... news ... social media"], "num_words": 5358, "gpt4_answer": "The corpus covers multiple domains, including discussion blogs, news, and social media."}
{"id": "1911.01214.ae89eed483c11ccd70a34795e9fe416af8a35da2", "question": "what was the inter-annotator agreement?", "answer": ["Cohen's Kappa of $\\kappa = 0.7$ BIBREF14 ... $\\kappa = 0.55$ Cohen's Kappa", "For stance annotation the inter-annotator agreement was 0.7, for FGE annotation inter-annotator agreement was 0.55"], "num_words": 5358, "gpt4_answer": "The inter-annotator agreement for stance annotation was substantial with a Cohen's Kappa of  = 0.7, while for FGE annotation, it was moderate with a Cohen's Kappa of  = 0.55."}
{"id": "1701.03578.fc62549a8f0922c09996a119b2b6a8b5e829e989", "question": "Which metrics are used for quantitative analysis?", "answer": ["Cross entropy between the trained model and models trained on different corpora.", "a measure that calculates the cross entropy between the word distribution of the model output and that of the target data", "perplexity ... cross entropy"], "num_words": 3733, "gpt4_answer": "The metrics used for quantitative analysis are perplexity and cross entropy."}
{"id": "1701.03578.e2a507749a4a3201edd6413c77ad0d4c23e9c6ce", "question": "Is their data open sourced?", "answer": ["No", "Unanswerable"], "num_words": 3733, "gpt4_answer": "Unanswerable"}
{"id": "1701.03578.a3a867f7b3557c168d05c517c468ff6c7337bff9", "question": "What dataset did they use?", "answer": ["WMT'14, English bible corpus, Drama corpus, and main character corpora", "Workshop on Statistical Machine Translation (WMT) data ... script of the drama, Friends,\" ... English bible data"], "num_words": 3733, "gpt4_answer": "They used the WMT'14 corpus and the script of the drama \"Friends\" for their experiments."}
{"id": "1701.03578.8bb2280483af8013a32e0d294e97d44444f08ab0", "question": "What metric did they use for qualitative evaluation?", "answer": ["Sample model output", "length and style of sample output"], "num_words": 3733, "gpt4_answer": "The authors used cross entropy as a metric for qualitative evaluation."}
{"id": "1701.03578.a68acd8364764d5601dc12e4b31d9102fb7d5f7e", "question": "What metric did they use for quantitative evaluation?", "answer": ["Cross entropy between word distribution of model output and word distribution of target data.", "perplexity"], "num_words": 3733, "gpt4_answer": "They used cross entropy as the metric for quantitative evaluation."}
{"id": "1701.03578.6d55e377335815b7ad134d1a2977d231ad34a25b", "question": "Which similarity metrics are used for quantitative analysis?", "answer": ["Cross entropy between word distribution of model output and word distribution of target data.", "cross entropy"], "num_words": 3733, "gpt4_answer": "The cross entropy metric is used for quantitative analysis."}
{"id": "1802.09233.0035b351df63971ec57e36d4bfc6f7594bed41ae", "question": "How is the data labeled?", "answer": ["Unanswerable"], "num_words": 2012, "gpt4_answer": "The data is labeled with manually annotated sentiment polarity."}
{"id": "1802.09233.2b021e1486343d503bab26c2282f56cfdab67248", "question": "What is the best performing model?", "answer": ["An ensemble of N-Channels ConvNet and XGboost regressor model", "Ensemble Model"], "num_words": 2012, "gpt4_answer": "Unanswerable"}
{"id": "1802.09233.e801b6a6048175d3b1f3440852386adb220bcb36", "question": "How long is the dataset?", "answer": ["Unanswerable"], "num_words": 2012, "gpt4_answer": "The dataset used for training consists of 12,284 English-language tweets and 6,100 Arabic-language tweets."}
{"id": "1902.08830.3699927c6c1146f5057576034d226a99946d52cb", "question": "what languages did they evaluate on?", "answer": ["Arabic, Chinese, English, French, and German", "Arabic, Chinese, French, English, and German"], "num_words": 9472, "gpt4_answer": "The authors evaluated their model on five languages: Arabic, Chinese, English, French, and German."}
{"id": "1902.08830.6606160e210d05b94f7cbd9c5ff91947339f9d02", "question": "were these categories human evaluated?", "answer": ["Yes"], "num_words": 9472, "gpt4_answer": "Yes, the induced categories were evaluated by comparison against a human-created reference categorization, and judgments on the coherence of learnt feature types and their relevance to their associated categories were collected from large crowds of native speakers."}
{"id": "1902.08830.0dc9050c832a6091bc9db3f7fa7be72139f51177", "question": "do language share categories? ", "answer": ["Yes"], "num_words": 9472, "gpt4_answer": "Yes, mental categories exist across communities and cultures, although their exact manifestations may differ."}
{"id": "1909.08357.4beb50ba020f624446ff1ef5bf4adca5ed318b98", "question": "What languages are evaluated?", "answer": ["Unanswerable"], "num_words": 1887, "gpt4_answer": "Unanswerable"}
{"id": "1909.08357.9bf60073fbb69fbf860196513fc6fd2f466535f6", "question": "Does the training of ESuLMo take longer compared to ELMo?", "answer": ["Unanswerable"], "num_words": 1887, "gpt4_answer": "Yes, the training of ESuLMo takes longer compared to ELMo."}
{"id": "1909.08357.7d503b3d4d415cf3e91ab08bd5a1a2474dd1047b", "question": "How long is the vocabulary of subwords?", "answer": ["500"], "num_words": 1887, "gpt4_answer": "The best vocabulary size for both segmentation algorithms (BPE and ULM) is 500 subwords."}
{"id": "1801.04433.1c8958ec50976a9b1088c51e8f73a767fb3973fa", "question": "what rnn classifiers were used?", "answer": ["LSTM classifier with no additional features, Neutral & Sexism, Neutral & Racism, Racism & Sexism and Neutral, Racism & Sexism.", "Long Short-Term Memory Network (LSTM)", "experiment with combining various LSTM models enhanced with a number of novel features (O No additional features,  NS Neutral & Sexism,  NR Neutral & Racism, RS Racism & Sexism,  NRS Neutral, Racism & Sexism) in an ensemble."], "num_words": 4354, "gpt4_answer": "The paper used Long Short-Term Memory (LSTM) based classifiers in their ensemble approach."}
{"id": "1801.04433.363d0cb0cd5c9a0b0364d61d95f2eff7347d5a36", "question": "what results did their system obtain?", "answer": ["Best authors' system achieved 0.9320 F1 score.", "The best model achieved a 0.9320 F-score", "The best performing single classifier produces F1 0.9265. The best ensemble classifier  (O+NS+RS+NR+NRS) produce  F1 0.9320."], "num_words": 4354, "gpt4_answer": "The system achieved an F-score of 0.93, outperforming the current state of the art in detecting hate speech in short text like tweets."}
{"id": "1801.04433.cf0b7d8a2449d04078f69ec9717a547adfb67d17", "question": "what are the existing approaches?", "answer": ["BIBREF12 , in which character n-grams and gender information were used as features ... BIBREF5 investigated the impact of the experience of the annotator in the performance ... two step classification BIBREF16 ... BIBREF13 , which focuses on various classes of Sexism ... CNN in, BIBREF16"], "num_words": 4354, "gpt4_answer": "Existing approaches for automated detection of offensive text include Natural Language Processing (NLP) approaches, supervised learning classification methods, and unsupervised learning models that apply concepts from NLP or use AI-solutions and bag-of-words based text representations."}
{"id": "1908.07888.8de0e1fdcca81b49615a6839076f8d42226bf1fe", "question": "Which dataset do they use?", "answer": ["500 rescored intent annotations found in the lattices in cancellations and refunds domain", "dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain"], "num_words": 4380, "gpt4_answer": "The authors use a dataset of over 70,000 US English phone conversations with an average duration of 11 minutes, spanning across several domains such as account information inquiries, refund requests, and service cancellations."}
{"id": "1908.07888.909ecf675f874421eecc926a9f7486475aa1423c", "question": "How do they use extracted intent to rescore?", "answer": [" the rescoring was judged by two annotators, who labeled 250 examples each", "providing a library of intent examples"], "num_words": 4380, "gpt4_answer": "The extracted intents are used to guide the selection of the \"best\" variant in the annotated lattice by prioritizing paths with recognized intents rather than just word probabilities, thus improving the understanding of spontaneous conversations."}
{"id": "1908.07888.29477c8e28a703cacb716a272055b49e2439a695", "question": "Do they evaluate by how much does ASR improve compared to state-of-the-art just by using their FST?", "answer": ["No"], "num_words": 4380, "gpt4_answer": "The paper demonstrates an increase in intent recognition by 25.1% compared to the baseline algorithm, but it does not explicitly compare ASR improvement to state-of-the-art using their FST."}
{"id": "2003.03131.9186b2c5b7000ab7f15a46a47da73ea45544bace", "question": "How is the model evaluated against the original recursive training algorithm?", "answer": ["Morfessor EM+Prune configuration significantly outperforms Morfessor Baseline w.r.t. the F-score for all languages except North Smi. Morfessor EM+Prune is less responsive to tuning than Morfessor Baseline.", "The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\alpha $-weighted sum.\n\nThe closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21.", "We perform an error analysis, with the purpose of gaining more insight into the ability of the methods to model particular aspects of morphology.", "boundary precision ... boundary recall ...  boundary $F_{1}$-score"], "num_words": 4005, "gpt4_answer": "The model is evaluated against the original recursive training algorithm by comparing the Morfessor cost of the segmented training data and the test set Boundary Precision, Recall, and F1-score for English, Finnish, Turkish, and North Smi languages."}
{"id": "2003.03131.d30b2fb5b29faf05cf5e04d0c587a7310a908d8c", "question": "What is the improvement in performance compared to the linguistic gold standard?", "answer": [" For English and North Smi, EM+Prune results in less under-segmentation but worse over-segmentation.  For Finnish these results are reversed.", "Proposed approach is best in:\n- Recall English: +3.47 (70.84 compared to next best 67.37)\n- Precision Finnish: +6.16 (68.18 compared to 62.02)\n- Recall NorthSami: +1.44 (62.84 compared to 61.40)"], "num_words": 4005, "gpt4_answer": "The improved optimization of Morfessor EM+Prune leads to higher morphological segmentation accuracy when compared to a linguistic gold standard."}
{"id": "2003.03131.526dc757a686a1fe41e77f7e3848e3507940bfc4", "question": "What is the improvement in performance brought by lexicon pruning on a simple EM algorithm?", "answer": ["Unanswerable"], "num_words": 4005, "gpt4_answer": "The paper does not provide a direct quantitative measure for the improvement in performance brought by lexicon pruning on a simple EM algorithm. However, it states that the proposed Morfessor EM+Prune leads to lower Morfessor costs and improved accuracy when compared to linguistic morphological segmentation."}
{"id": "1903.01411.2d91554c3f320a4bcfeb00aa466309074a206712", "question": "Which metrics do they use to evaluate results?", "answer": ["BLEU  ... METEOR  ... chrF", "BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics ... precision, recall and F-measure", "BLEU BIBREF35 ... METEOR BIBREF36 ... chrF BIBREF37 ... precision ... recall  ... F-measure"], "num_words": 4218, "gpt4_answer": "The authors use BLEU, METEOR, and chrF metrics to evaluate the results."}
{"id": "1903.01411.53362c2870cf76b7981c27b3520a71eb1e3e7965", "question": "Does the performance increase with the number of used languages?", "answer": ["Unanswerable", "Yes"], "num_words": 4218, "gpt4_answer": "Yes, the performance increases with the number of used languages, as engaging a large number of different languages helps identify relevant context and improve the quality of translations."}
{"id": "1903.01411.5138121b9e9bd56962e69bfe49d5df5301cb7745", "question": "By how much do they outperform translating without contextual information?", "answer": ["Unanswerable"], "num_words": 4218, "gpt4_answer": "The experiment shows a significant improvement over translation without any contextual information, but the paper does not provide a specific numerical value for the improvement."}
{"id": "2004.03090.25e6ba07285155266c3154d3e2ca1ae05c2f7f2d", "question": "Which baselines did they compare to?", "answer": ["Fine tuned DIaloGPT and GPT2 on Interview without speaker information.", "finetune (FT) DialoGPT and GPT2 on Interview without speaker information", "two models (GPT2 and DialoGPT) on two datasets (DailyDialog and CALLHOME)"], "num_words": 2362, "gpt4_answer": "They compared the performance of state-of-the-art language models fine-tuned on Interview and other popular conversational datasets, including DailyDialog and CALLHOME. They also investigated DialoGPT, a model pre-trained on Reddit threads."}
{"id": "2004.03090.d68cc9aaf0466b97354600a5646c3be4512fc096", "question": "What dialog tasks was it experimented on?", "answer": ["1) generating appropriate responses for a specific role given a conversation history (speaker role modeling) ... 2) predicting whether a new speaker will interject on the next sentence of a conversation", "role modeling in media dialog  ... role change detection ", "role modeling in media dialog and role change detection on Interview"], "num_words": 2362, "gpt4_answer": "The paper experimented on two tasks: 1) generating appropriate responses for a specific role given a conversation history (speaker role modeling); and 2) predicting whether a new speaker will interject on the next sentence of a conversation (role change detection)."}
{"id": "2004.03090.d038e5d2a6f85e68422caaf8b96cb046db6599fa", "question": "How was annotation done?", "answer": ["Unanswerable", "annotations for each turn"], "num_words": 2362, "gpt4_answer": "Annotation was not explicitly mentioned for the Interview dataset, as it was collected from news interview transcripts where speaker roles were already identified (hosts and guests)."}
{"id": "2004.03090.c66e0aa86b59bbf9e6a1dc725fb9785473bfa137", "question": "Which news outlets did they focus on?", "answer": ["NPR", "Unanswerable", "natural dialog"], "num_words": 2362, "gpt4_answer": "The paper focuses on 7 programs from National Public Radio (NPR) over 20 years (19992019), which amounts to a total of 10k hours of interview transcripts."}
{"id": "2004.03090.369d7bc5351409910c7a5e05c0cbb5abab8e50ec", "question": "Do the interviews fall under a specific news category? ", "answer": ["No"], "num_words": 2362, "gpt4_answer": "The interviews in the dataset are collected from National Public Radio (NPR) programs and cover a wide range of topics, making them open-domain."}
{"id": "2004.03090.b9d9803ba24127f91ba4d7cff4da11492da20f09", "question": "Which baselines did they compare to?", "answer": ["Fine-tuned DialGPT and GPT2  on Interview without speaker information.", "We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information"], "num_words": 2362, "gpt4_answer": "They compared to speaker-agnostic baselines, GPT2, DialoGPT, and their fine-tuned versions on the Interview dataset."}
{"id": "2004.03090.7625068cc22a095109580b83eff48616387167c2", "question": "Which dialog tasks did they experiment on?", "answer": [" role modeling in media dialog and role change detection on Interview", "1) generating appropriate responses for a specific role given a conversation history (speaker role modeling) ... 2) predicting whether a new speaker will interject on the next sentence of a conversation"], "num_words": 2362, "gpt4_answer": "They experimented on two tasks: 1) generating appropriate responses for a specific role given a conversation history (speaker role modeling); and 2) predicting whether a new speaker will interject on the next sentence of a conversation (role change detection)."}
{"id": "2004.03090.be0b438952048fe6bb91c61ba48e529d784bdcea", "question": "Did they use crowdsourcing for annotations?", "answer": ["Unanswerable"], "num_words": 2362, "gpt4_answer": "Unanswerable"}
{"id": "2004.03090.a97137318025a6642ed0634f7159255270ba3d4f", "question": "Were annotations done manually?", "answer": ["Unanswerable"], "num_words": 2362, "gpt4_answer": "No, the paper does not mention manual annotations for the dataset."}
{"id": "2004.03090.a24b2269b292fd0ee81d50303d1315383c594382", "question": "Which news sources do the transcripts come from?", "answer": [" 7 programs on National Public Radio (NPR)", "7 programs on National Public Radio (NPR) over 20 years"], "num_words": 2362, "gpt4_answer": "The transcripts come from 7 programs on National Public Radio (NPR) over 20 years (19992019)."}
{"id": "1709.06365.7d8cd7d6c86349ef0bd4fdbd84c8dc49c7678f46", "question": "Which real world datasets do they experiment on?", "answer": ["Reuters ... 20 Newsgroup ... New York Times ... Web Snippet ... Tag My News ... ABC News", "Reuters ... 20NG, 20 Newsgroup ... NYT, New York Times ... WS, Web Snippet ... TMN, Tag My News ... AN, ABC News", "Reuters-21578 dataset ... 20 Newsgroup ... New York Times ... Tag My News ... ABC News ... Web Snippet"], "num_words": 5923, "gpt4_answer": "The authors experimented on six real-world datasets, including Reuters, 20NG (20 Newsgroup), NYT (New York Times), WS (Web Snippet), TMN (Tag My News), and AN (ABC News)."}
{"id": "1709.06365.0fee37ebe0a010cf8bd665fa566306d8e7d12631", "question": "Which other models that incorporate meta information do they compare against?", "answer": ["LDA BIBREF0 ... LLDA, Labelled LDA BIBREF11 and PLLDA, Partially Labelled LDA BIBREF9 ... DMR, LDA with Dirichlet Multinomial Regression BIBREF8 ... WF-LDA, Word Feature LDA BIBREF16 ... LF-LDA, Latent Feature LDA BIBREF5 ... GPU-DMM, Generalized Plya Urn DMM BIBREF7 ... PTM, Pseudo document based Topic Model BIBREF18", "LLDA ... PLLDA ... DMR ... WF-LDA ... LF-LDA ... GPU-DMM"], "num_words": 5923, "gpt4_answer": "They compare against Supervised LDA (sLDA), Labelled LDA (LLDA), Partially Labelled LDA (PLLDA), Dirichlet Multinomial Regression (DMR), Word Feature LDA (WF-LDA), Latent Feature LDA (LF-LDA), Generalized Plya Urn DMM (GPU-DMM), and Pseudo document based Topic Model (PTM)."}
{"id": "1709.06365.f8bba20d1781ce2b14fad28d6eff024e5a6c2c02", "question": "How do they measure topic quality?", "answer": ["NPMI scores", "Normalised Pointwise Mutual Information"], "num_words": 5923, "gpt4_answer": "Topic quality is measured using semantic coherence of the words in a topic."}
{"id": "1709.06365.252599e53f52b3375b26d4e8e8b66322a42d2563", "question": "Which data augmentation techniques do they use?", "answer": ["augmented with a set of Beta random variables ... auxiliary variable", "the introduction of auxiliary variables"], "num_words": 5923, "gpt4_answer": "MetaLDA uses two data augmentation techniques: the introduction of auxiliary variables and the local conjugacy of the model."}
{"id": "1901.10619.e12166fa9d6f63c4e92252c95c6a7bc96977ebf4", "question": "Is this an English language corpus?", "answer": ["No", "Yes"], "num_words": 4155, "gpt4_answer": "Yes, this is an English language corpus."}
{"id": "1901.10619.d4cb704e93086a2246a8caa5c1035e8297b8f4c0", "question": "The authors point out a relevant constraint on the previous corpora of workplace, do they authors mention any relevant constrains on this corpus?", "answer": ["No"], "num_words": 4155, "gpt4_answer": "The authors do not specifically mention any constraints on the Twitter Job/Employment Corpus in the provided text."}
{"id": "1901.10619.a11b5eb928a6db9a0e3bb290ace468ff1685d253", "question": "What type of annotation is performed?", "answer": ["human annotation and automatic predictions over multiple rounds to label automatically data as job-related or not job-related", "multiple iterations of human annotations and automatic machine learning predictions, followed by some linguistic heuristics"], "num_words": 4155, "gpt4_answer": "Crowdsourced annotation is performed in this study."}
{"id": "1901.10619.275b2c22b6a733d2840324d61b5b101f2bbc5653", "question": "How are the tweets selected?", "answer": [" multiple iterations of human annotations and automatic machine learning predictions, followed by some linguistic heuristics, to extract job-related tweets from personal and business accounts", "They collected tweets from US and then applied some filtering rules based on Lexicons"], "num_words": 4155, "gpt4_answer": "The tweets are selected using a humans-in-the-loop framework involving multiple iterations of human annotations and automatic machine learning predictions, followed by some linguistic heuristics to extract job-related tweets from personal and business accounts."}
{"id": "1612.09535.f1f7a040545c9501215d3391e267c7874f9a6004", "question": "what dataset was used?", "answer": ["CoNLL_X bosque data, News data by Lusa agency, Sports news data", "News ... Sports news"], "num_words": 4298, "gpt4_answer": "The datasets used for evaluation are a set of 227 texts published by the Lusa agency (referred to as 'News') and a set of 881 sports news (referred to as 'Sports news')."}
{"id": "1612.09535.b6f4fd6bc76bfcbc15724a546445908afa6d922c", "question": "by how much did their model improve over current alternatives?", "answer": ["On average, it had better Recall by 0.481 in case of news dataset and by 0.372 in case of sports news dataset. \nOn average, it had better Precision by 0.086 in case of news dataset and by 0.37 in case of sports news dataset. \nOn average, it had better F1 by 0.381 in case of news dataset and by 0.616 in case of sports news dataset. ", "Pampo had F1 score of 0.932 and 0.971 compared to best alternative result of 0.608 and 0.794 on News and Sport news dataset respectively."], "num_words": 4298, "gpt4_answer": "PAMPO significantly obtained higher recall and F1 measures than existing tools, with an increase in F1 measure by at least 0.25."}
{"id": "1612.09535.3614c1f1435b7c1fd1f7f0041219eebf5bcff473", "question": "did they experiment with other languages besides portuguese?", "answer": ["No"], "num_words": 4298, "gpt4_answer": "The PAMPO method was developed for Portuguese but is potentially adaptable to other languages; however, the paper does not mention experiments with other languages."}
{"id": "1612.09535.c316d7d0c80b8f720ff90a8bb84a8b879a3ef7ea", "question": "how many rules did they use?", "answer": ["Unanswerable"], "num_words": 4298, "gpt4_answer": "PAMPO uses four pattern bases, which include various rules, but the exact number of rules is not specified in the paper."}
{"id": "1910.02001.a786cceba4372f6041187c426432853eda03dca6", "question": "What is the state-of-the-art?", "answer": ["BIBREF2", "BIBREF2, who predict the roles of the Russian trolls on Twitter by leveraging social theory and Actor-Network Theory approaches"], "num_words": 5051, "gpt4_answer": "The state-of-the-art in predicting the political role of Internet trolls is a novel approach that leverages community structure and the text of messages in the online social network of trolls represented as a graph, extracting several types of embeddings for the trolls. The approach improves over previous work, reaching an accuracy of 89.6% in the fully supervised case and 78.5% in the distant supervised case."}
{"id": "1910.02001.a837dcbd339e27a974e28944178c790a5b0b37c0", "question": "How large is the dataset?", "answer": ["2973371 tweets", "2973371 tweets by 2848 Twitter users"], "num_words": 5051, "gpt4_answer": "The dataset contains 2,973,371 tweets by 2,848 Twitter users."}
{"id": "1910.02001.c135e1f8ecaf7965f6a6d3e30b537eb37ad74230", "question": "How are labels for trolls obtained?", "answer": ["We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves without any need for manual labeling.", "manual labeling ... more realistic scenario assumes that labels for troll accounts are not available", "the ground truth labels for the troll users are available ... we leverage more persistent entities and their labels: news media. We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves"], "num_words": 5051, "gpt4_answer": "Labels for trolls are obtained through manual labeling by analyzing their behavior patterns, actions, and content they post on social media platforms."}
{"id": "1910.02001.16a10c1681dc5a399b6d34b4eed7bb1fef816dd0", "question": "Do they only look at tweets?", "answer": ["No", "Yes"], "num_words": 5051, "gpt4_answer": "Yes, the paper focuses on analyzing the behavior patterns of political trolls using features from their tweets on Twitter."}
{"id": "1603.01547.2ca3ca39d59f448e30be6798514709be7e3c62d8", "question": "Which datasets did they use to train the model?", "answer": ["CNN  ... Daily Mail ... CBT CN and NE", "CNN ... Daily Mail ... Children's Book Test"], "num_words": 3792, "gpt4_answer": "The model was trained on the CNN, Daily Mail, and Children's Book Test (CBT) datasets."}
{"id": "1603.01547.df7fb8e6e44c9c5af3f19dde762c75cbf2f8452f", "question": "What is the performance of their model?", "answer": ["CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5% ... In named entity prediction our best single model with accuracy of 68.6%", "The different AS Reader models had average test accuracy of 71,35% and AS Reader (avg ensemble) had the highest test accuracy between all tested models with 75.4%\n\nIn case of Daily Mail average was  75.55% and greedy assemble had the highest value with 77.7%\nCBT NE average was 69.65% and greedy ensemble had the highest value of 71% \n\nCBT CN had average of 65.5% and avg assemble had the highest value of 68.9%\n"], "num_words": 3792, "gpt4_answer": "The ensemble of their models achieved state-of-the-art results on all evaluated datasets (CNN, Daily Mail, and CBT)."}
{"id": "1603.01547.20e2b517fddb0350f5099c39b16c2ca66186d09b", "question": "What baseline do they compare against?", "answer": ["Attentive and Impatient Readers  ... Chen et al. 2016\n ... MenNN ... Dynamic Entity Representation  ... LSTM ", "Unanswerable"], "num_words": 3792, "gpt4_answer": "They compare against several recent deep neural network architectures such as Attentive and Impatient Readers, Chen et al. 2016, Memory Networks, Dynamic Entity Representation, and Pointer Networks."}
{"id": "1603.01547.70512cc9dcd45157e40c8d1f85e82d21ade7645b", "question": "What datasets is the model evaluated on?", "answer": ["CNN  ... Daily Mail ...  CBT CN and NE", "CNN ... Daily Mail ... Children's Book Test", "CNN, Daily Mail and CBT"], "num_words": 3792, "gpt4_answer": "The model is evaluated on the CNN, Daily Mail, and Children's Book Test (CBT) datasets."}
{"id": "1912.00239.fd556a038c36abc88a800d9d4f2cfa0aef6f5aba", "question": "What is the percentage of human judgment agreement on the set?", "answer": ["Unanswerable"], "num_words": 3978, "gpt4_answer": "Unanswerable"}
{"id": "1912.00239.9119fbfba84d298014d1b74e0e3d30330320002c", "question": "Are the orders of case assignment biases motivated by frequency considerations?", "answer": ["Yes"], "num_words": 3978, "gpt4_answer": "Yes, the orders of case assignment biases are motivated by frequency considerations."}
{"id": "1912.00239.058b6e3fdbb607fa7dbfc688628b3e13e130c35a", "question": "Does the paper list other heuristic biases in the LSTMs?", "answer": ["Yes"], "num_words": 3978, "gpt4_answer": "Yes, the paper mentions that LSTMs are contaminated by heuristics not found in humans, such as a preference toward nominative noun phrases."}
{"id": "1912.00239.5b95665d44666a1dc9e568d2471e5edf8614859f", "question": "What are the performances of LSTMs and humans on the task?", "answer": ["LSTM obtains an overall score of 0.56 while humans' score is 0.58", "LTSM 0.56 AUC, humans 0.58 AUC", "mean AUC of 0.56 for the LTSM and of 0.58 for humans"], "num_words": 3978, "gpt4_answer": "The performances are rather modest, with a mean AUC of 0.56 for the LSTM and 0.58 for humans, compared to the chance score of 0.5 for the unigram and bigram models."}
{"id": "2004.00809.b9686a168366aafbab1737df426e031ad74a6284", "question": "Do they authors offer a hypothesis for why Twitter data makes better predictions about the inventory of languages used in each country?", "answer": ["Twitter data has fewer missing languages than what census-based data contains because it matches populations better when they are weighting by GDP", "Yes"], "num_words": 4150, "gpt4_answer": "The authors do not provide a specific hypothesis for why Twitter data makes better predictions about the inventory of languages used in each country."}
{"id": "2004.00809.740cc392c0c8bfadfe6b3a60c0be635c03e17f2a", "question": "What social media platforms are represented?", "answer": ["Twitter", "Twitter "], "num_words": 4150, "gpt4_answer": "The paper focuses on data from Twitter as the social media platform."}
{"id": "2004.00809.845bdcd900c0f96b2ae091d086fb1ab8bb1063f0", "question": "Which websites were used in the web crawl?", "answer": ["81.5 billion web pages covered in Common Crawl dataset", "web-crawled data from the Common Crawl"], "num_words": 4150, "gpt4_answer": "The paper does not specify individual websites used in the web crawl. It used the Common Crawl dataset, which covers a wide range of web pages from 2014 through the end of 2017."}
{"id": "2004.00809.8d1b6c88f06ee195d75af32ede85dbd6477c8497", "question": "What countries and languages are represented in the datasets?", "answer": ["English, Spanish, Russian, Serbo-Croatian, Mandarin, German, French, Slovenian, Portuguese, Finnish, Bulgarian, Arabic, Indonesian, Latvian, Estonian, Slovak, Azerbaijani, Romanina, Icelandic, Italian, among others.", "Unanswerable"], "num_words": 4150, "gpt4_answer": "The datasets represent 199 countries in total, with the web-crawled data covering 166 countries and the Twitter data covering 169 countries. The top twenty languages in the web corpus include English, Spanish, Russian, French, German, and others, making up 87.9% of the web data and 80.4% of the social media data."}
{"id": "1903.10318.bc05503eef25c732f1785e29d59b6022f12ba094", "question": "What other evaluation metrics did they use other than ROUGE-L??", "answer": ["ROUGE-1 and ROUGE-2", "Rouge-1, Rouge-2, Rouge Recall, Rouge F1", "they also use ROUGE-1 and ROUGE-2"], "num_words": 1791, "gpt4_answer": "They used ROUGE-1 and ROUGE-2 evaluation metrics in addition to ROUGE-L."}
{"id": "1903.10318.a6603305f4fd3dd0010ac31243c40999a116537e", "question": "Do they encode sentences separately or together?", "answer": ["Together", "Yes"], "num_words": 1791, "gpt4_answer": "They encode sentences together, modifying the input sequence and embeddings of BERT to make it possible for extracting summaries."}
{"id": "1903.10318.2ba4477d597b1fd123d14be07a7780ccb5c4819b", "question": "How do they use BERT to encode the whole text?", "answer": ["insert a [CLS] token before each sentence and a [SEP] token after each sentence ... use interval segment embeddings to distinguish multiple sentences within a document", "interval segment embeddings to distinguish multiple sentences within a document"], "num_words": 1791, "gpt4_answer": "To use BERT for extractive summarization, they modify the input sequence and embeddings of BERT, inserting a [CLS] token before each sentence and a [SEP] token after each sentence. They also use interval segment embeddings to distinguish multiple sentences within a document."}
{"id": "1903.10318.027814f3a879a6c7852e033f9d99519b8729e444", "question": "What is the ROUGE-L score of baseline method?", "answer": ["37.17", "37.17 for the baseline model using a non-pretrained Transformer"], "num_words": 1791, "gpt4_answer": "The paper does not provide a specific ROUGE-L score for the baseline method."}
{"id": "1903.10318.00df1ff914956d4d23299d02fd44e4c985bb61fa", "question": "Which is the baseline method?", "answer": ["non-pretrained Transformer baseline "], "num_words": 1791, "gpt4_answer": "The baseline method is a non-pretrained Transformer with the same architecture as BERT but with smaller parameters, randomly initialized and only trained on the summarization task."}
{"id": "1706.02427.b57ad10468e1ba2a7a34396688dbb10a575d89f5", "question": "What loss function is used?", "answer": ["negative log-likelihood"], "num_words": 4868, "gpt4_answer": "The loss function used is negative log-likelihood."}
{"id": "1706.02427.9d6d17120c42a834b2b5d96f2120d646218ed4bb", "question": "Do they use the unstructured text on the webpage that was the source of the table?", "answer": ["No"], "num_words": 4868, "gpt4_answer": "No, they focus on exploring the content of the table itself rather than using unstructured text on the webpage."}
{"id": "1706.02427.965e0ce975a0b8612a30cfc31bbfd4b8a57aa138", "question": "Does their method rely on the column headings of the table?", "answer": ["Yes"], "num_words": 4868, "gpt4_answer": "Yes, their method relies on the column headings (headers) of the table."}
{"id": "1706.02427.8dfdd1ed805bb23c774fbb032ef1d97c6802e07c", "question": "Are all the tables in the dataset from the same website?", "answer": ["No, they come from the top ranked web pages relevant to a query and from Wikipedia ", "Yes"], "num_words": 4868, "gpt4_answer": "No, the dataset contains tables from various websites, including 273,816 web tables from Wikipedia."}
{"id": "1706.02427.c21675d8a90bda624d27e5535d1c10f08fcbc16b", "question": "How are the tables extracted from the HTML?", "answer": ["Unanswerable"], "num_words": 4868, "gpt4_answer": "The paper does not provide specific details on how tables are extracted from HTML."}
{"id": "1911.02747.da077b385d619305033785af5b204696d6145bd8", "question": "Does the query-bag matching model use a neural network?", "answer": ["Yes"], "num_words": 2714, "gpt4_answer": "Yes, the query-bag matching model uses a deep neural network to model the matching relationship."}
{"id": "1911.02747.6d8a51e2790043497ed2637a1abc36bdffb39b71", "question": "What datasets are used for experiments?", "answer": ["AliMe  ... Quora", "AliMe and Quora", "the AliMe and Quora dataset"], "num_words": 2714, "gpt4_answer": "The experiments are conducted on two datasets: AliMe and Quora."}
{"id": "1911.02747.de4cc9e7fa5d700f5046d60789770f47911b3dd7", "question": "Which natural language(s) is/are studied?", "answer": ["Chinese and English", "Unanswerable"], "num_words": 2714, "gpt4_answer": "Unanswerable"}
{"id": "1911.02747.8ad5ebca2f69023b60ccfa3aac0ed426234437ac", "question": "Is model compared to some baseline?", "answer": ["Yes"], "num_words": 2714, "gpt4_answer": "Yes, the model is compared to two baselines: the Q-Q matching based baseline and the query-bag matching based baseline."}
{"id": "1911.02747.4afd4cfcb30433714b135b977baff346323af1e3", "question": "What datasets are used in experiments?", "answer": [" the AliMe and Quora dataset "], "num_words": 2714, "gpt4_answer": "The experiments are conducted on two datasets: AliMe and Quora."}
{"id": "1908.10461.b2dc0c813da92cf13d86528bd32c12286ec9b9cd", "question": "How many lexical features are considered?", "answer": ["3: In addition to word embedding, there is a POS tag embedding and a dependcy relation embedding. ", "No", "Unanswerable"], "num_words": 2035, "gpt4_answer": "Three lexical features are considered: cross-lingual word embeddings, universal PoS tags, and universal dependencies."}
{"id": "1908.10461.c4c06f36454fbfdc5d218fb84ce74eaf7f78fc98", "question": "What is the performance for the three languages tested?", "answer": ["Best authors achieved (different models) in terms of F1 score is:\nGerman - 0.6446\nItalian - 0.6999\nDutch - 0.6057", "Max-F Scores for German .6446, Italian .6999. Dutch .6057 compared to 0.8748 for English"], "num_words": 2035, "gpt4_answer": "The performance of the cross-lingual models in German, Italian, and Dutch is not explicitly stated in the given text."}
{"id": "1908.10461.347dc2fd6427b39cf2358d43864750044437dff8", "question": "How many Universal Dependency features are considered?", "answer": ["Unanswerable"], "num_words": 2035, "gpt4_answer": "The paper does not specify an exact number of Universal Dependency features considered."}
{"id": "1908.10461.6911e8724dfdb178fa81bf58019947b71ef8fbe7", "question": "Do they evaluate any non-zero-shot parsers on the three languages?", "answer": ["No", "Unanswerable"], "num_words": 2035, "gpt4_answer": "No, the paper focuses on zero-shot cross-lingual semantic parsing for the three languages."}
{"id": "1908.10461.b012df09fa2a3d6b581032d68991768cf4bc9d7b", "question": "How big is the Parallel Meaning Bank?", "answer": ["4405 English sentences, 1173 German sentences, 633 Italian sentences and 583 Dutch sentences", "6794 sentences"], "num_words": 2035, "gpt4_answer": "The Parallel Meaning Bank (PMB) consists of 4405 English sentences, 1173 German sentences, 633 Italian sentences, and 583 Dutch sentences."}
{"id": "1908.10461.62edffd051d056cf60e17deafcc55a8c9af398cb", "question": "What is the source of the crosslingual word embeddings?", "answer": ["MUSE BIBREF17"], "num_words": 2035, "gpt4_answer": "The source of the cross-lingual word embeddings is MUSE (BIBREF17)."}
{"id": "1612.05202.d5c393df758dec6ea6827ae5b887eb6c303a4f4d", "question": "Do they compare against manually-created lexicons?", "answer": ["Yes"], "num_words": 2017, "gpt4_answer": "No, they do not compare against manually-created lexicons."}
{"id": "1612.05202.11a3af3f056e0fb5559fe5cbff1640e022732735", "question": "Do they compare to non-lexicon methods?", "answer": ["Yes"], "num_words": 2017, "gpt4_answer": "Yes, they compare their method with the \"No Sentiment Lexicon\" system, which does not have any lexicon feature."}
{"id": "1612.05202.07a214748a69b31400585aef7aba6af3e3d9cce2", "question": "What language pairs are considered?", "answer": ["English-French, English-Italian, English-Spanish, English-German.", "French, Italian, Spanish and German ... Existing English sentiment lexicons are translated to the target languages"], "num_words": 2017, "gpt4_answer": "The language pairs considered are English to French, Italian, Spanish, and German."}
{"id": "1911.12237.44bf3047ff7e5c6b727b2aaa0805dd66c907dcd6", "question": "How many abstractive summarizations exist for each dialogue?", "answer": ["Each dialogue contains only one reference summary.", "Unanswerable"], "num_words": 3692, "gpt4_answer": "Each dialogue contains only one reference summary."}
{"id": "1911.12237.c6f2598b85dc74123fe879bf23aafc7213853f5b", "question": "How is human evaluators' judgement measured, what was the criteria?", "answer": ["$-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all ... 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary ... 1  it is understandable and gives a brief overview of the text", "We asked two linguists to mark the quality of every summary on the scale of $-1$, 0, 1, where $-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 1  it is understandable and gives a brief overview of the text, and 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary."], "num_words": 3692, "gpt4_answer": "Human evaluators' judgement was measured on a scale of -1, 0, and 1, where -1 means the summarization is poor, 0 means it extracts only a part of relevant information or makes some mistakes, and 1 means it is understandable and gives a brief overview of the text."}
{"id": "1911.12237.bdae851d4cf1d05506cf3e8359786031ac4f756f", "question": "What models have been evaluated?", "answer": ["MIDDLE-n, LONGEST-n, LONGER-THAN-n and MOST-ACTIVE-PERSON are the baselines, and experiments also carried out on Pointer generator networks, Transformers, Fast Abs RL, Fast Abs RL Enhanced, LightConv and DynamicConv ", "Pointer generator network ... Transformer ... Fast Abs RL ... Fast Abs RL Enhanced ... LightConv and DynamicConv"], "num_words": 3692, "gpt4_answer": "The evaluated models include Pointer Generator Network, Transformer, Fast Abs RL, Fast Abs RL Enhanced, LightConv, and DynamicConv."}
{"id": "1911.12237.894bbb1e42540894deb31c04cba0e6cfb10ea912", "question": "Do authors propose some better metric than ROUGE for measurement of abstractive dialogue summarization?", "answer": ["No"], "num_words": 3692, "gpt4_answer": "The authors do not propose a specific better metric than ROUGE for abstractive dialogue summarization but suggest that a new metric should be designed to measure the quality of abstractive dialogue summaries."}
{"id": "1911.12237.75b3e2d2caec56e5c8fbf6532070b98d70774b95", "question": "How big is SAMSum Corpus?", "answer": ["16369 conversations", "contains over 16k chat dialogues with manually annotated summaries"], "num_words": 3692, "gpt4_answer": "The SAMSum Corpus contains over 16k chat dialogues with manually annotated summaries."}
{"id": "1909.07873.573b8b1ad919d3fd0ef7df84e55e5bfd165b3e84", "question": "Do they manually check all adversarial examples that fooled some model for potential valid examples?", "answer": ["No", "Only 100 successfully adversarial examples were manually checked, not all of them."], "num_words": 4956, "gpt4_answer": "No, they do not manually check all adversarial examples. Instead, they conducted a human evaluation experiment with 100 randomly sampled adversarial examples generated by their model, which were successful in fooling the target classifier."}
{"id": "1909.07873.07d98dfa88944abd12acd45e98fb7d3719986aeb", "question": "Are all generated examples semantics-preserving perturbations to the original text?", "answer": ["No", "While the models aim to generate examples which preserve the semantics of the text with minimal perturbations, the Random model randomly replaces a character, which may not preserve the semantics. "], "num_words": 4956, "gpt4_answer": "Yes, the adversarial examples generated by the Adversarial Examples Generator (AEG) model are semantics-preserving perturbations to the original text."}
{"id": "1909.07873.3a40559e5a3c2a87c7b9031c89e762b828249c05", "question": "What is success rate of fooling tested models in experiments?", "answer": ["Authors best attacking model resulted in dip in the accuracy of CNN-Word (IMDB) by 79.43% and CNN-Char (AG's News) model by 72.16%", "Our complete AEG model outperforms all the other models with significant drop in accuracy. For the CNN-Word, DeepWordBug decreases the accuracy from 89.95% to 28.13% while AEG model further reduces it to 18.5%."], "num_words": 4956, "gpt4_answer": "The paper does not provide a specific success rate of fooling the tested models in the experiments. However, it mentions that their AEG model significantly reduces the accuracy of the target classifiers, with the CNN-Word model's accuracy dropping from 89.95% to 18.5%."}
{"id": "1909.07873.5db47bbb97282983e10414240db78154ea7ac75f", "question": "What models are able to be fooled for AG's news corpus news categorization task by this approach?", "answer": [" character-based convolutional model (CNN-Char)", "A word-based convolutional model (CNN-Word) and a character-based convolutional model (CNN-Char)"], "num_words": 4956, "gpt4_answer": "The approach is able to fool character-based convolutional models (CNN-Char) trained on AG's news corpus for news categorization tasks."}
{"id": "1909.07873.c589d83565f528b87e355b9280c1e7143a42401d", "question": "What models are able to be fooled for IMDB sentiment classification task by this approach?", "answer": ["A word-based convolutional neural network (CNN-Word)", "word-based convolutional model (CNN-Word)"], "num_words": 4956, "gpt4_answer": "The models that can be fooled for the IMDB sentiment classification task by this approach include word-based convolutional model (CNN-Word) and character-based convolutional model (CNN-Char)."}
{"id": "1909.07873.7f90e9390ad58b22b362a57330fff1c7c2da7985", "question": "Do they use already trained model on some task in their reinforcement learning approach?", "answer": ["Yes"], "num_words": 4956, "gpt4_answer": "Yes, they use a pretrained model in their reinforcement learning approach."}
{"id": "1909.07873.3e3e45094f952704f1f679701470c3dbd845999e", "question": "How does proposed reinforcement learning based approach generate adversarial examples in black-box settings?", "answer": ["Training ::: Training with Reinforcement learning\nWe fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm.\n\nTraining ::: Training with Reinforcement learning ::: Self-critical sequence training (SCST)\nIn SCST approach, the model learns to gather more rewards from its sampled sequences that bring higher rewards than its best greedy counterparts. First, we compute two sequences: (a) $y^{\\prime }$ sampled from the model's distribution $p(y^{\\prime }_j|y^{\\prime }_{", "able to generate both character and word level perturbations as necessary ... modifying the standard decoder BIBREF29, BIBREF30 to have two-level decoder GRUs: word-GRU and character-GRU"], "num_words": 4956, "gpt4_answer": "The proposed reinforcement learning based approach generates adversarial examples in black-box settings by using an Adversarial Example Generator (AEG) model. This model employs a hybrid encoder-decoder architecture that operates at both word and character levels to generate adversarial examples. The model is pretrained on paraphrase datasets and character transformations, then fine-tuned using a self-critical approach to maximize rewards based on fooling the target classifier, minimizing perturbations, and preserving the semantics of the text."}
{"id": "1906.01502.475ef4ad32a8589dae9d97048166d732ae5d7beb", "question": "Which languages with different script do they look at?", "answer": ["Urdu ... Hindi ... English ... Japanese ... Bulgarian"], "num_words": 2234, "gpt4_answer": "The paper looks at languages such as Urdu (written in Arabic script) and Hindi (written in Devanagari script) as an example of languages with different scripts."}
{"id": "1906.01502.3fd8eab282569b1c18b82f20d579b335ae70e79f", "question": "What languages do they experiment with?", "answer": ["Answer with content missing: (subscripts 2 and 3)\nNER task: Arabic, Bengali, Czech, German, English, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Russian, Turkish, and Chinese.\nPOS task: Arabic, Bulgarian, Catalan, Czech, Danish, German, Greek, English, Spanish, Estonian, Basque, Persian, Finnish, French, Galician, Hebrew, Hindi, Croatian, Hungarian, Indonesian, Italian, Japanese, Korean, Latvian, Marathi, Dutch, Norwegian (Bokmaal and Nynorsk), Polish, Portuguese (European and Brazilian), Romanian, Russian, Slovak, Slovenian, Swedish, Tamil, Telugu, Turkish, Urdu, and Chinese.", "Dutch ... Spanish ... English ... German"], "num_words": 2234, "gpt4_answer": "The paper experiments with 104 languages, including Dutch, Spanish, English, German, Urdu, and Hindi, among others."}
{"id": "1906.01502.8e9561541f2e928eb239860c2455a254b5aceaeb", "question": "What language pairs are affected?", "answer": ["Language pairs that are typologically different", "Unanswerable"], "num_words": 2234, "gpt4_answer": "M-BERT's representations exhibit systematic deficiencies affecting certain language pairs, but the paper does not specify which exact pairs are affected."}
{"id": "1906.01502.50c1bf8b928069f3ffc7f0cb00aa056a163ef336", "question": "What evaluation metrics are used?", "answer": ["accuracy", "ner F1 score ... pos zero-shot accuracy"], "num_words": 2234, "gpt4_answer": "The evaluation metrics used are F1 score, accuracy, and nearest neighbor accuracy."}
{"id": "1906.01502.2ddfb40a9e73f382a2eb641c8e22bbb80cef017b", "question": "What datasets did they use?", "answer": ["CoNLL-2002 and -2003  ... Universal Dependencies ... WMT16 ", "CoNLL-2002 and -2003 sets ... an in-house dataset with 16 languages ... Universal Dependencies (UD) BIBREF7"], "num_words": 2234, "gpt4_answer": "They used CoNLL-2002 and -2003 sets for NER experiments and Universal Dependencies (UD) data for 41 languages for POS experiments."}
{"id": "1611.00440.65b39676db60f914f29f74b7c1264422ee42ad5c", "question": "what are the other methods they compare to?", "answer": ["BIBREF1", "Naive Bayes Classifier"], "num_words": 2336, "gpt4_answer": "The paper compares its Naive-Bayesian predictive model to previous researches that used sentiment analysis and Naive Bayesian models, such as those predicting the outcome of the 2008 and 2012 U.S. Presidential Elections using Twitter data."}
{"id": "1611.00440.a2baa8e266318f23f43321c4b2b9cf467718c94a", "question": "what preprocessing method is introduced?", "answer": ["(1) removing URLs and pictures ... (2) by filtering tweets which have candidates' name", "Tweets without candidate names are removed, URLs and pictures are removed from the tweets that remain."], "num_words": 2336, "gpt4_answer": "The introduced preprocessing method involves removing URLs and pictures from the tweets and filtering tweets that have candidates' names. Hashtags, mentions, and retweets are not removed to maintain the original meaning of a tweet."}
{"id": "1702.02367.97ff88c31dac9a3e8041a77fa7e34ce54eef5a76", "question": "How well does their model perform on the recommendation task?", "answer": ["Proposed model achieves HITS@100 of 30.0 compared to best baseline model result of 29.2 on recommendation task.", "Their model achieves 30.0 HITS@100 on the recommendation task, more than any other baseline"], "num_words": 3837, "gpt4_answer": "The model outperforms all other systems on the recommendation task, except for the QA SYSTEM on the QA task."}
{"id": "1702.02367.272defe245d1c5c091d3bc51399181da2da5e5f0", "question": "Which knowledge base do they use to retrieve facts?", "answer": ["Unanswerable", "bAbI Movie Dialog dataset"], "num_words": 3837, "gpt4_answer": "The authors use the bAbI Movie Dialog dataset to retrieve facts."}
{"id": "1702.02367.860257956b83099cccf1359e5d960289d7d50265", "question": "Which neural network architecture do they use?", "answer": ["Gated Recurrent Units", "bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU) ... additional recurrent neural network with GRU units"], "num_words": 3837, "gpt4_answer": "They use a novel model based on Artificial Neural Networks with an iterative attention mechanism for multiple answer prediction."}
{"id": "1608.07836.1b1849ad0bdd79c6645572849fe7873ec7bd7e6d", "question": "Are reddit and twitter datasets, which are fairly prevalent, not effective in addressing these problems?", "answer": ["Unanswerable", "Yes"], "num_words": 2172, "gpt4_answer": "Reddit and Twitter datasets can be helpful in addressing the problems of non-canonical language in NLP, but the paper does not specifically mention their effectiveness."}
{"id": "1607.07514.deb0c3524a3b3707e8b20abd27f54ad6188d6e4e", "question": "did they experiment with other languages?", "answer": ["No"], "num_words": 2622, "gpt4_answer": "The paper focuses on English-language tweets and does not mention experimenting with other languages."}
{"id": "1607.07514.d7e43a3db8616a106304ac04ba729c1fee78761d", "question": "by how much did their system outperform previous tasks?", "answer": ["On paraphrase and semantic similarity proposed model has F1 score of 0.677 compared to best previous model result of 0.674, while on sentiment classification it has 0.656 compared to 0.648 of best previous result.", "Sentiment classification task by 0,008  F1, and semantic similarity task by 0,003 F1."], "num_words": 2622, "gpt4_answer": "Tweet2Vec outperformed the top four models in the SemEval 2015 competition for both tasks, but the paper does not provide specific numerical values for the performance difference."}
{"id": "1607.07514.0ba8f04c3fd64ee543b9b4c022310310bc5d3c23", "question": "what are the previous state of the art for sentiment categorization?", "answer": ["INESC-ID,  lsislif, unitn and  Webis.", "INESC-ID, lsislif, unitn and Webis."], "num_words": 2622, "gpt4_answer": "The paper does not specifically mention the previous state-of-the-art models for sentiment categorization."}
{"id": "1607.07514.b7d02f12baab5db46ea9403d8932e1cd1b022f79", "question": "what are the previous state of the art for tweet semantic similarity?", "answer": ["nnfeats, ikr, linearsvm and svckernel."], "num_words": 2622, "gpt4_answer": "The paper does not specifically mention the previous state-of-the-art for tweet semantic similarity."}
{"id": "1809.03680.ff2b58c90784eda6dddd8a92028e6432442c1093", "question": "By how much do they outperform baselines?", "answer": ["On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests.", "On r=2 SEM-HMM Approx. is 2.2% better, on r=5 SEM-HMM is 3.9% better and on r=10 SEM-HMM is 3.9% better than the best baseline"], "num_words": 4669, "gpt4_answer": "On average, SEM-HMM significantly outperforms all the baselines, with the average improvement in accuracy across OMICS tasks being statistically significant at a .01 level."}
{"id": "1809.03680.5e4eac0b0a73d465d74568c21819acaec557b700", "question": "Which baselines do they use?", "answer": ["The \"frequency\" baseline, the \"conditional\" baseline, the \"BMM\" baseline and the \"BMM+EM\" baseline", "Frequency baseline ... Conditional baseline ... BMM ... BMM + EM"], "num_words": 4669, "gpt4_answer": "The four baselines used are: \"Frequency,\" \"Conditional,\" \"BMM,\" and \"BMM + EM.\""}
{"id": "1809.03680.bc6ad5964f444cf414b661a4b942dafb7640c564", "question": "Which datasets do they evaluate on?", "answer": ["Open Minds Indoor Common Sense (OMICS) corpus", "The Open Minds Indoor Common Sense (OMICS) corpus "], "num_words": 4669, "gpt4_answer": "They evaluate on the Open Minds Indoor Common Sense (OMICS) corpus."}
{"id": "1706.00188.380e71848d4b0d1e983d504b1249119612f00bcb", "question": "What deep learning methods do they look at?", "answer": ["CNN ... LSTM ... FastText", "FastText ... Convolutional Neural Networks (CNNs) ... Long Short-Term Memory Networks (LSTMs)"], "num_words": 1313, "gpt4_answer": "The paper investigates three deep learning methods: FastText, Convolutional Neural Networks (CNNs), and Long Short-Term Memory Networks (LSTMs)."}
{"id": "1706.00188.21c89ee0281f093b209533453196306b9699b552", "question": "What is their baseline?", "answer": ["Char n-grams ... TF-IDF ... BoWV", "char n-grams ... TF-IDF vectors ... Bag of Words vectors (BoWV)"], "num_words": 1313, "gpt4_answer": "The baseline methods used in the paper are: (1) Char n-grams, (2) TF-IDF, and (3) BoWV (Bag of Words Vector) approach using the average of the word (GloVe) embeddings to represent a sentence."}
{"id": "1706.00188.5096aaea2d0f4bea4c12e14f4f7735e1aea1bfa6", "question": "Which architectures do they experiment with?", "answer": ["CNN ... LSTM ... FastText", "FastText ... Convolutional Neural Networks (CNNs) ... Long Short-Term Memory Networks (LSTMs)"], "num_words": 1313, "gpt4_answer": "The authors experiment with FastText, Convolutional Neural Networks (CNNs), and Long Short-Term Memory Networks (LSTMs) architectures."}
{"id": "1706.00188.452e2d7d7d9e1bb4914903479cd7caff9f6fae42", "question": "Are pretrained embeddings used?", "answer": ["GloVe", "Yes"], "num_words": 1313, "gpt4_answer": "Yes, pretrained GloVe embeddings were used in the experiments."}
{"id": "1609.02075.cdb211be0340bb18ba5a9ee988e9df0e2ba8b793", "question": "Does the paper discuss limitations of considering only data from Twitter?", "answer": ["No"], "num_words": 5766, "gpt4_answer": "The paper does not explicitly discuss the limitations of considering only data from Twitter."}
{"id": "1609.02075.4cb2e80da73ae36de372190b4c1c490b72977ef8", "question": "Did they represent tie strength only as number of social ties in a networks? ", "answer": ["No", "Yes", "Yes, a normalized mutual friends metric"], "num_words": 5766, "gpt4_answer": "No, they quantified tie strength in terms of embeddedness, using the normalized mutual friends metric introduced by Adamic and Adar."}
{"id": "1609.02075.a064337bafca8cf01e222950ea97ebc184c47bc0", "question": "What sociolinguistic variables (phonetic spellings) did they analyze? ", "answer": ["phonetic spelling ... abbreviation ... lexical words", "variation and change in the use of words characteristic from eight US cities that have non-standard spellings"], "num_words": 5766, "gpt4_answer": "The authors analyzed phonetic spellings such as ain (phonetic spelling of ain't), dese (phonetic spelling of these), yeen (phonetic spelling of you ain't); ard (phonetic spelling of alright), inna (phonetic spelling of in a and in the), and asl (phonetic spelling of as hell, typically used as an intensifier on Twitter)."}
{"id": "1609.02075.993d5bef2bf1c0cd537342ef76d4b952f0588b83", "question": "What older dialect markers did they explore?", "answer": ["Unanswerable"], "num_words": 5766, "gpt4_answer": "The paper does not provide specific examples of older dialect markers they explored."}
{"id": "1708.09025.a8e5e10d13b3f21dd11e8eb58e30cc25efc56e93", "question": "How many domains do they create ontologies for?", "answer": ["4", "four domains"], "num_words": 4651, "gpt4_answer": "They create ontologies for four domains."}
{"id": "1708.09025.949a2bc34176e47a4d895bcc3223f2a960f15a81", "question": "Do they separately extract topic relations and topic hierarchies in their model?", "answer": ["No"], "num_words": 4651, "gpt4_answer": "Yes, they divide a terminological ontology into two components: topic hierarchies and topic relations, and extract them separately."}
{"id": "1708.09025.70abb108c3170e81f8725ddc1a3f2357be5a4959", "question": "How do they measure the usefulness of obtained ontologies compared to domain expert ones?", "answer": ["We use KB-LDA, phrase_hLDA, and LDA+GSHL as our baseline methods, and compare ontologies extracted from hrLDA, KB-LDA, phrase_hLDA, and LDA+GSHL with DBpedia ontologies. We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. ", "precision ... recall ... F-measure"], "num_words": 4651, "gpt4_answer": "They measure the usefulness of obtained ontologies compared to domain expert ones using precision, recall, and F-measure for ontology evaluation."}
{"id": "1708.09025.ce504a7ee2c1f068ef4dde8d435245b4e77bb0b5", "question": "How do they obtain syntax from raw documents in hrLDA?", "answer": [" syntax and document structures such as paragraph indentations and item lists, assigns multiple topic paths for every document, and allows topic trees to grow vertically and horizontally. ... . By contrast, a complex sentence can be subdivided into multiple atomic sentences. Given that the syntactic verb in a relation triplet is determined by the subject and the object, a document INLINEFORM4 in a corpus INLINEFORM5 can be ultimately reduced to INLINEFORM6 subject phrases (we convert objects to subjects using passive voice) associated with INLINEFORM7 relation triplets INLINEFORM8 ...  The idea is to find all syntactically related noun phrases and their connections using a language parser such as the Stanford NLP parser BIBREF24 and Ollie BIBREF25 . ", "By extracting syntactically related noun phrases and their connections using a language parser."], "num_words": 4651, "gpt4_answer": "In hrLDA, they obtain syntax from raw documents by using a language parser such as the Stanford NLP parser or Ollie to find all syntactically related noun phrases and their connections."}
{"id": "2004.04478.468eb961215a554ace8088fa9097a7ad239f2d71", "question": "What datasets are available for CDSA task?", "answer": ["DRANZIERA ", "DRANZIERA benchmark dataset"], "num_words": 4564, "gpt4_answer": "The paper uses the DRANZIERA benchmark dataset, which consists of Amazon reviews from 20 domains such as automotives, baby products, beauty products, etc."}
{"id": "2004.04478.57d07d2b509c5860880583efe2ed4c5620a96747", "question": "What two novel metrics proposed?", "answer": ["LM3 (Chameleon Words Similarity) and LM4 (Entropy Change)", "ULM4 ... ULM5"], "num_words": 4564, "gpt4_answer": "The two novel metrics proposed are LM3: Chameleon Words Similarity and LM4: Entropy Change."}
{"id": "2004.04478.d126d5d6b7cfaacd58494f1879547be9e91d1364", "question": "What similarity metrics have been tried?", "answer": ["LM1: Significant Words Overlap ...  LM2: Symmetric KL-Divergence (SKLD) ... LM3: Chameleon Words Similarity ... LM4: Entropy Change ...  ULM1: Word2Vec ... ULM2: Doc2Vec ... ULM3: GloVe ... ULM4 and ULM5: FastText ... ULM6: ELMo ... ULM7: Universal Sentence Encoder", "LM1: Significant Words Overlap ... LM2: Symmetric KL-Divergence (SKLD) ... LM3: Chameleon Words Similarity ... LM4: Entropy Change ... ULM1: Word2Vec ... ULM2: Doc2Vec ... ULM3: GloVe ... ULM4 and ULM5: FastText ...  ULM6: ELMo"], "num_words": 4564, "gpt4_answer": "The paper investigates 11 similarity metrics, including Significant Words Overlap, Chameleon Words Similarity, Symmetric KL Divergence, Entropy Change, Word2Vec embeddings, GloVe embeddings, FastText word embeddings, ELMo based embeddings, Universal Sentence Encoder based embeddings, Doc2Vec embeddings, and FastText sentence embeddings."}
{"id": "2004.04478.7dca806426058d59f4a9a4873e9219d65aea0987", "question": "What 20 domains are available for selection of source domain?", "answer": ["Amazon Instant Video\nAutomotive\nBaby\nBeauty\nBooks\nClothing Accessories\nElectronics\nHealth\nHome Kitchen\nMovies TV\nMusic\nOffice Products\nPatio\nPet Supplies\nShoes\nSoftware\nSports Outdoors\nTools Home Improvement\nToys Games\nVideo Games", "Amazon Instant Video,  Automotive, Baby,  Beauty, Books,  Clothing Accessories,  Electronics,  Health, Home, Kitchen, Movies, Music,  Office Products, Patio, Pet Supplies, Shoes,  Software,  Sports Outdoors,  Tools Home Improvement, Toys Games, Video Games."], "num_words": 4564, "gpt4_answer": "The paper does not provide a detailed list of all 20 domains used in the study."}
{"id": "1805.04558.800fcd8b08d36c5276f9e5e1013208d41b46de59", "question": "why do they think sentiment features do not result in improvement?", "answer": ["Because sentiment features extracted the same information as other features.", "did not observe any improvement in the cross-validation experiments"], "num_words": 4312, "gpt4_answer": "The sentiment features did not result in improvement because they were not helpful in both tasks, which focused on classifying tweets mentioning adverse drug reactions and personal medication intake."}
{"id": "1805.04558.cdbbba22e62bc9402aea74ac5960503f59e984ff", "question": "what was the size of the datasets?", "answer": ["10822, 4845", "training set containing 10,822 tweets and a development set containing 4,845 tweets ... test set of 9,961 tweets was provided without labels ... training set containing 8,000 tweets and a development set containing 2,260 tweets ... test set of 7,513 tweets"], "num_words": 4312, "gpt4_answer": "The training set contained 10,822 tweets, the development set had 4,845 tweets, and the test set had 9,961 tweets for Task 1. For Task 2, the training set contained 8,000 tweets, the development set had 2,260 tweets, and the test set had 7,513 tweets."}
{"id": "1805.04558.301a453abaa3bc15976817fefce7a41f3b779907", "question": "what were the evaluation metrics?", "answer": ["F-score for class 1 (ADR) ... micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)", "micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)"], "num_words": 4312, "gpt4_answer": "The evaluation metrics were F-score for class 1 (ADR) in Task 1 and micro-averaged F-score of class 1 (intake) and class 2 (possible intake) in Task 2."}
{"id": "1805.04558.f3673f6375f065014e8e4bb8c7adf54c1c7d7862", "question": "what were their results on both tasks?", "answer": ["0.435 on Task1 and 0.673 on Task2.", "Unanswerable"], "num_words": 4312, "gpt4_answer": "The results for their submissions were an F-score of 0.435 on Task 1 (ranking first) and 0.673 on Task 2 (ranking third)."}
{"id": "1805.04558.0bd3bea892c34a3820e98c4a42cdeda03753146b", "question": "what domain-specific features did they train on?", "answer": ["INLINEFORM0 -grams generalized over domain terms ... Pronoun Lexicon features ... domain word embeddings ... domain word clusters"], "num_words": 4312, "gpt4_answer": "The domain-specific features they trained on include: 1) n-grams generalized over domain terms, 2) Pronoun Lexicon features, 3) domain word embeddings, and 4) domain word clusters."}
{"id": "1805.04558.8cf5abf0126f19253930478b02f0839af28e4093", "question": "what are the sentiment features used?", "answer": ["The following set of features were calculated separately for each tweet and each lexicon:\n\nthe number of tokens with INLINEFORM0 ;\n\nthe total score = INLINEFORM0 ;\n\nthe maximal score = INLINEFORM0 ;\n\nthe score of the last token in the tweet.", "the number of tokens with INLINEFORM0 ... the total score = INLINEFORM0 ... the maximal score = INLINEFORM0 ... the score of the last token in the tweet"], "num_words": 4312, "gpt4_answer": "The sentiment features used are the number of tokens with sentiment scores, the total score, the maximal score, and the score of the last token in the tweet, calculated separately for each tweet and each lexicon (Hu and Liu Lexicon, Norms of Valence, Arousal, and Dominance, labMT, and NRC Emoticon Lexicon)."}
{"id": "1805.04558.d211a37830c59aeab4970fdb2e03d9b7368b421c", "question": "what surface-form features were used?", "answer": ["INLINEFORM0 -grams ... General-domain word embeddings ... General-domain word clusters ... Negation ... Twitter-specific features ... Punctuation", "INLINEFORM0 -grams ... General-domain word embeddings ... General-domain word clusters ... Negation: presence of simple negators ... the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words ... presence of exclamation and question marks, whether the last token contains an exclamation or question mark"], "num_words": 4312, "gpt4_answer": "The surface-form features used include word n-grams, non-contiguous word n-grams, character n-grams, unigram stems, negation, Twitter-specific features (uppercase tokens, hashtags, emoticons, elongated words), and punctuation (exclamation and question marks)."}
{"id": "1911.03324.c3ce95658eea1e62193570955f105839de3d7e2d", "question": "How does their BERT-based model work?", "answer": ["It takes the query and document as input and encodes the query relevance, document context and salient meaning to be passed to the output layer to make the prediction.", "The model takes the concatenation of the query and the document as input. The query-sentence and sentence-sentence relationships are jointly modeled by the self-attention mechanism BIBREF12. The model is fine-tuned to utilize the general language representations of BERT BIBREF13."], "num_words": 3757, "gpt4_answer": "The BERT-based model for query-focused extractive summarization takes the concatenation of the query and the document as input, using self-attention mechanism to jointly model query-sentence and sentence-sentence relationships. The contextualized sentence representations are then fed into an output layer to predict the label or estimate the score of each sentence."}
{"id": "1911.03324.389cc454ac97609e9d0f2b2fe70bf43218dd8ba7", "question": "How do they use Wikipedia to automatically collect a query-focused summarization dataset?", "answer": ["They use the article and section titles to build a query and use the body text of citation as the summary.", "To automatically construct query-focused summarization examples using Wikipedia, the statements' citations in Wikipedia articles as pivots to align the queries and documents. Figure FIGREF1 shows an example that is constructed by the proposed method. We first take the highlighted statement as the summary. Its supporting citation is expected to provide an adequate context to derive the statement, thus can serve as the source document. On the other hand, the section titles give a hint about which aspect of the document is the summary's focus. Therefore, we use the article title and the section titles of the statement to form the query. "], "num_words": 3757, "gpt4_answer": "They use Wikipedia articles and their corresponding references to construct the dataset, with statements as summaries, supporting citations as documents, and article titles along with section titles as queries."}
{"id": "1910.03177.2c4db4398ecff7e4c1c335a2cb3864bfdc31df1a", "question": "How is GPU-based self-critical Reinforcement Learing model designed?", "answer": ["Unanswerable", "We used the self-critical model of BIBREF13 proposed for image captioning ... Additionally, we have used enthttps://stackoverflow.com/questions/19053077/looping-over-data-and-creating-individual-figuresropy regularization. ... To the best of our knowledge, ours is the first GPU based implementation."], "num_words": 4235, "gpt4_answer": "The GPU-based self-critical Reinforcement Learning model is designed by training the model using a self-critic policy gradient method, optimizing the non-differentiable metrics such as ROUGE, and using a learning rate of 0.00005, which is manually changed to 0.00001 when needed, with  = 0.0001."}
{"id": "1910.03177.4738158f92b5b520ceba6207e8029ae082786dbe", "question": "What are previoius similar models authors are referring to?", "answer": ["Abstractive and extractive models from Nallapati et al., 2016, Pointer generator models with and without coverage from See et al., 2017, and Reinforcement Learning models from Paulus et al., 2018, and Celikyilmaz et al., 2018.", "HierAttn \nabstractive model \nPointer Generator \nPointer Generator + coverage \nMLE+RL, with intra-attention\n DCA, MLE+RL\nPlain NSE"], "num_words": 4235, "gpt4_answer": "The authors refer to previous similar models such as traditional sequence-to-sequence (seq2seq) models, hierarchical attention models, and Neural Semantic Encoders (NSE) for text summarization tasks."}
{"id": "1910.03177.4dadde7c61230553ef14065edd8c1c7e41b9c329", "question": "What was previous state of the art on factored dataset?", "answer": ["41.69 ROUGE-1", "ROUGE-1 41.69\nROUGE-2 19.47\nROUGE-L 37.92"], "num_words": 4235, "gpt4_answer": "The previous state-of-the-art on the factored dataset was outperformed by the hierarchical NSE model by nearly 4 ROUGE points."}
{"id": "2004.02143.014830892d93e3c01cb659ad31c90de4518d48f3", "question": "How much did the model outperform", "answer": ["Automatic evaluation metrics show  relative improvements of  11.11, 6.07, 19.29 for BLEU-4, ROUGE-L and SF Coverage respectively (over average baseline). \nHuman evaluation  relative improvement for Difficulty, Naturalness and SF Coverage are 8.44,  32.64,  13.57 respectively.", "the absolute improvement of $4.02$ and $3.18$ points compared to NQG and Max-out Pointer model, respectively, in terms of BLEU-4 metric"], "num_words": 4149, "gpt4_answer": "The proposed MultiHop-QG model achieves an absolute improvement of 4.02 and 3.18 points compared to NQG and Max-out Pointer models, respectively, in terms of BLEU-4 metric."}
{"id": "2004.02143.ae7c5cf9c2c121097eb00d389cfd7cc2a5a7d577", "question": "What language is in the dataset?", "answer": ["English"], "num_words": 4149, "gpt4_answer": "The dataset uses the English language."}
{"id": "2004.02143.af948ea91136c700957b438d927f58d9b051c97c", "question": "How big is the HotPotQA dataset?", "answer": [" over 113k Wikipedia-based question-answer pairs", "113k Wikipedia-based question-answer pairs"], "num_words": 4149, "gpt4_answer": "The HotPotQA dataset consists of over 113k Wikipedia-based question-answer pairs."}
{"id": "1907.01339.a913aa14d4e05cc9d658bf6697fe5b2652589b1b", "question": "Which labeling scheme do they use?", "answer": ["Unanswerable"], "num_words": 1646, "gpt4_answer": "They use a sequence labeling scheme for parsing."}
{"id": "1907.01339.b065a3f598560fdeba447f0a100dd6c963586268", "question": "What parts of their multitask model are shared?", "answer": ["Unanswerable", "stacked bilstms"], "num_words": 1646, "gpt4_answer": "In the multitask model, the sentence is first processed by stacked BiLSTMs shared across all tasks, with a task-dependent feed-forward network on the top of it to compute each task's outputs."}
{"id": "1907.01339.9d963d385bd495a7e193f8a498d64c1612e6c20c", "question": "Which dataset do they use?", "answer": [" English Penn Treebank ... spmrl datasets", "English Penn Treebank ... spmrl datasets"], "num_words": 1646, "gpt4_answer": "They use two parallel datasets: the English Penn Treebank and the SPMRL datasets, which provide syntactic analyses for both dependency and constituency parsing."}
{"id": "1704.04451.179bc57b7b5231ea6ad3e93993a6935dda679fa2", "question": "Do they compare against Reinforment-Learning approaches?", "answer": ["Yes"], "num_words": 3131, "gpt4_answer": "Yes, they compare their approach to clark-manning:2016:EMNLP2016, which uses reinforcement learning and reward rescaling."}
{"id": "1704.04451.a59e86a15405c8a11890db072b99fda3173e5ab2", "question": "How long is the training dataset?", "answer": ["3,492 documents", "3492"], "num_words": 3131, "gpt4_answer": "The paper does not provide the exact length of the training dataset."}
{"id": "1704.04451.9489b0ecb643c1fc95c001c65d4e9771315989aa", "question": "What dataset do they use?", "answer": ["CoNLL 2012", "English portion of CoNLL 2012 data BIBREF15"], "num_words": 3131, "gpt4_answer": "The authors use the English portion of CoNLL 2012 data for their experiments."}
{"id": "1604.02201.b210c3e48c15cdc8c47cf6f4b6eb1c29a1933654", "question": "What high-resource language pair is the parent model trained on?", "answer": ["French-English", "French-English parent model"], "num_words": 2832, "gpt4_answer": "The parent model is trained on the French-English language pair."}
{"id": "1604.02201.00341a46a67d31d36e6dc54d5297626319584891", "question": "Did they use any regularization method to constrain the training?", "answer": ["Freezing certain portions of the parent model and fine tuning others", "Yes"], "num_words": 2832, "gpt4_answer": "Yes, they used parameter freezing as a hard approximation to a tight prior or strong regularization applied to some of the parameters."}
{"id": "1604.02201.d0dc6729b689561370b6700b892c9de8871bb44d", "question": "How did they constrain training using the parameters?", "answer": ["Freezing certain portions of the parent model and fine tuning others", "first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model).  ... by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model. ... When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. "], "num_words": 2832, "gpt4_answer": "They constrained training by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model."}
{"id": "1908.11053.17fd6deb9e10707f9d1b70165dedb045e1889aac", "question": "What are their evaluation metrics?", "answer": ["average F1-score ... accuracy", "average F1-scores"], "num_words": 4005, "gpt4_answer": "The evaluation metrics used are average F1-scores for the end-to-end query generation task."}
{"id": "1908.11053.c4a3f270e942803dab9b40e5e871a2e8886ce444", "question": "Are their formal queries tree-structured?", "answer": ["No"], "num_words": 4005, "gpt4_answer": "Unanswerable"}
{"id": "1908.11053.1faccdc78bbd99320c160ac386012720a0552119", "question": "What knowledge base do they rely on?", "answer": ["DBpedia", "DBpedia (2016-04) ... DBpedia (2015-10)"], "num_words": 4005, "gpt4_answer": "The paper relies on knowledge bases (KBs) such as DBpedia and Freebase."}
{"id": "1908.11053.804466848f4fa1c552f0d971dce226cd18b9edda", "question": "How do they recover from noisy entity linking?", "answer": ["by filtering errors in noisy entity linking by the empty query check or domain/range check in query structure ranking", "ranked query structures first and considered linking results in the last step ... empty query check or domain/range check"], "num_words": 4005, "gpt4_answer": "SubQG handles noisy entity linking results by ranking query structures first and considering linking results in the last step. Many error linking results can be filtered out by the empty query check or domain/range check."}
{"id": "1908.11053.8d683d2e1f46626ceab60ee4ab833b50b346c29e", "question": "What datasets do they evaluate on?", "answer": ["(LC-QuAD) BIBREF8 ... (QALD-5) dataset BIBREF9", "LC-QuAD ... QALD-5"], "num_words": 4005, "gpt4_answer": "They evaluate on the large-scale complex question answering dataset (LC-QuAD) and the fifth edition of question answering over linked data (QALD-5) dataset."}
{"id": "1806.06571.5ae005917efc17a505ba1ba5e996c4266d6c74b6", "question": "Did they use the same dataset as Skip-gram to train?", "answer": ["No", "Yes"], "num_words": 2444, "gpt4_answer": "Yes, they used the same training data for both Skip-gram and SubGram models."}
{"id": "1806.06571.72c04eb3fc323c720f7f8da75c70f09a35abf3e6", "question": "How much were the gains they obtained?", "answer": ["Only 0.2% accuracy gain in morpho-sintactic questions in original test set, and 12.7% accuracy gain on their test set", "between 21-57% in several morpho-syntactic questions"], "num_words": 2444, "gpt4_answer": "The SubGram model achieved large gains on the Skip-gram original test set."}
{"id": "1906.00424.0715d510359eb4c851cf063c8b3a0c61b8a8edc0", "question": "What is the extractive technique used for summarization?", "answer": ["Answer with content missing: (baseline list) TextRank, KLSum, Lead-1, Lead-K and Random-K", "TextRank, KLSum, Lead-1, Lead-K, Random-K"], "num_words": 2597, "gpt4_answer": "The extractive techniques used for summarization in this paper include TextRank and Greedy KL."}
{"id": "1906.00424.4e106b03cc2f54373e73d5922e97f7e5e9bf03e4", "question": "How big is the dataset?", "answer": ["446", "446 sets of parallel text"], "num_words": 2597, "gpt4_answer": "The dataset contains 446 sets of parallel text."}
{"id": "1802.06053.f8edc911f9e16559506f3f4a6bda74cde5301a9a", "question": "By how much they outperform the baseline?", "answer": ["18.08 percent points on F-score", "Unanswerable"], "num_words": 2308, "gpt4_answer": "Our results outperform a pure speech-based baseline based on segmental DTW with an F-score of 19.3% on the exact same corpus."}
{"id": "1802.06053.8c288120139615532838f21094bba62a77f92617", "question": "How long are the datasets?", "answer": ["5130", "5130 Mboshi speech utterances"], "num_words": 2308, "gpt4_answer": "The paper does not provide the exact length of the datasets."}
{"id": "1802.06053.a464052fd11af1d2d99e407c11791269533d43d1", "question": "What bayesian model is trained?", "answer": ["Structured Variational AutoEncoder (SVAE) AUD ... Bayesian Hidden Markov Model (HMM)", "non-parametric Bayesian Hidden Markov Model"], "num_words": 2308, "gpt4_answer": "The Bayesian model trained is a non-parametric Bayesian Hidden Markov Model (HMM) for unsupervised Acoustic Unit Discovery (AUD)."}
{"id": "1802.06053.5f6c1513cbda9ae711bc38df08fe72e3d3028af2", "question": "What low resource languages are considered?", "answer": ["Mboshi ", "Mboshi (Bantu C25)"], "num_words": 2308, "gpt4_answer": "The paper focuses on Mboshi, a Bantu language spoken in Congo-Brazzaville."}
{"id": "1909.00871.130d73400698e2b3c6860b07f2e957e3ff022d48", "question": "How is cluster purity measured?", "answer": ["V-measure", "V-measure BIBREF16"], "num_words": 4325, "gpt4_answer": "Cluster purity is measured using V-measure, which is a metric that combines homogeneity and completeness of the clusters."}
{"id": "1909.00871.7e9aec2bdf4256c6249cad9887c168d395b35270", "question": "What was the previous state of the art for bias mitigation?", "answer": ["WED ... CDA"], "num_words": 4325, "gpt4_answer": "The previous state-of-the-art for bias mitigation included Word Embedding Debiasing (WED) and Counterfactual Data Augmentation (CDA)."}
{"id": "1909.00871.1acf06105f6c1930f869347ef88160f55cbf382b", "question": "How are names paired in the Names Intervention?", "answer": ["By solving the Euclidean-distance bipartite matching problem of names by frequency\nand gender-specificity", "name frequency ... the degree of gender-specificity"], "num_words": 4325, "gpt4_answer": "In the Names Intervention, names are paired using a bipartite-graph matching technique that considers both name frequency and gender-specificity, and the problem is solved using the Hungarian method."}
{"id": "1909.00871.9ce90f4132b34a328fa49a63e897f376a3ad3ca8", "question": "Which tasks quantify embedding quality?", "answer": ["word similarity ... sentiment classification", "word similarity ... sentiment classification ... understanding of gender using non-biased analogies"], "num_words": 4325, "gpt4_answer": "Word similarity and sentiment classification are tasks that quantify embedding quality."}
{"id": "1909.00871.3138f916e253abed643d3399aa8a4555b2bd8c0f", "question": "What empirical comparison methods are used?", "answer": ["Direct bias ... Indirect bias ... Word similarity ... Sentiment classification ... Non-biased gender analogies", "test the degree to which the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can still be used in two NLP tasks standardly performed with embeddings, word similarity and sentiment classification"], "num_words": 4325, "gpt4_answer": "The empirical comparison methods used include measuring direct bias with the Word Embedding Association Test (WEAT), assessing indirect bias through clustering and classification, evaluating word similarity with SimLex-999, performing sentiment classification on the Stanford Large Movie Review dataset, and testing non-biased gender analogies using the family analogy subset of the Google Analogy Test set."}
{"id": "1912.03010.810e6d09813486a64e87ef6c1fb9b1e205871632", "question": "How do they define their tokens (words, word-piece)?", "answer": ["Unanswerable", "a word or a word-piece"], "num_words": 2347, "gpt4_answer": "SentencePiece is employed as the tokenizer, and the vocabulary size is 5000."}
{"id": "1912.03010.ab8b0e6912a7ca22cf39afdac5531371cda66514", "question": "By how much do they outperform existing state-of-the-art model on end-to-end Speech recognition?s ", "answer": ["10%", "relative 4.5$\\%$ gain ... built upon the code base of ESPnet, and achieves relative $10\\%$ gains due to the better architecture and masking strategy"], "num_words": 2347, "gpt4_answer": "The model achieves a relative 10% gain over the existing state-of-the-art end-to-end speech recognition system on the Librispeech test set."}
{"id": "1901.02780.89373db8ced1fe420eae0093b2736f06b565616e", "question": "Did the authors collect new data for evaluation?", "answer": ["No"], "num_words": 2852, "gpt4_answer": "No, the authors used two existing datasets of Czech Facebook posts and Mall product reviews for evaluation."}
{"id": "1805.09960.74a17eb3bf1d4f36e2db1459a342c529b9785f6e", "question": "what were the evaluation metrics?", "answer": ["BLEU", "BLEU "], "num_words": 3966, "gpt4_answer": "The evaluation metrics used were case-insensitive 4-gram BLEU score for translation quality and a subjective evaluation based on faithfulness of translation results."}
{"id": "1805.09960.4b6745982aa64fbafe09f7c88c8d54d520b3f687", "question": "what language pairs are explored?", "answer": ["Chinese-English  ... English-Japanese", "Chinese-English ... English-Japanese"], "num_words": 3966, "gpt4_answer": "The language pairs explored in this paper are Chinese-to-English (CH-EN) and English-to-Japanese (EN-JA)."}
{"id": "1805.09960.6656a9472499331f4eda45182ea697a4d63e943c", "question": "what datasets did they use?", "answer": ["NIST 2003 (MT03) ... NIST2004-2006 (MT04-06) ... NIST 2008 (MT08) ... KFTT ", "NIST 2003 ... NIST2004-2006 ... NIST 2008 ... KFTT"], "num_words": 3966, "gpt4_answer": "The authors used two datasets for their experiments: 1) Chinese-to-English (CH-EN) translation with small and large-scale data sets, and 2) English-to-Japanese (EN-JA) translation using the KFTT dataset."}
{"id": "1805.09960.430ad71a0fd715a038f3c0fe8d7510e9730fba23", "question": "which attention based nmt method did they compare with?", "answer": [" BIBREF23 , BIBREF24", "attention-based NMT system BIBREF23 , BIBREF24"], "num_words": 3966, "gpt4_answer": "They compared their method with the baseline attention-based NMT system."}
{"id": "1805.09960.b79ff0a50bf9f361c5e5fed68525283856662076", "question": "by how much did their system improve?", "answer": ["In CH-EN translation, the average improvement is up to 2.23 BLEU points, in EN-JA translation, the improvement can reach 1.96 BLEU point.", "The average improvement for CH-EN is 3.99 BLEU points, for EN-JA it is 3.59 BLEU points."], "num_words": 3966, "gpt4_answer": "For Chinese-English translation, the system improved by an average of 2.23 BLEU points, and for English-Japanese translation, the improvement reached 1.96 BLEU points."}
{"id": "1907.00937.d66c31f24f582c499309a435ec3c688dc3a41313", "question": "What were the baseline methods?", "answer": ["DSSM ... Match Pyramid ... ARC-II ... our model with frozen, randomly initialized embeddings", "DSSM BIBREF12 , Match Pyramid BIBREF18 , ARC-II BIBREF15 "], "num_words": 3967, "gpt4_answer": "The baseline methods were DSSM, Match Pyramid, ARC-II, and the model with frozen, randomly initialized embeddings."}
{"id": "1907.00937.c47312f2ca834ee75fa9bfbf912ea04239064117", "question": "What dataset is used for training?", "answer": ["11 months of search logs", "a self-collected dataset of 11 months of search logs as query-product pairs"], "num_words": 3967, "gpt4_answer": "The dataset used for training consists of 11 months of search logs."}
{"id": "1811.01183.5499440674f0e4a9d6912b9ac29fa1f7b7cd5253", "question": "Do they compare to previous work?", "answer": ["Yes"], "num_words": 4228, "gpt4_answer": "The paper compares its unsupervised approach to other unsupervised methods for information extraction from biomedical texts, such as Named Entity Recognition and relation extraction techniques. However, it does not directly compare its results to those of previous works."}
{"id": "1811.01183.de313b5061fc22e8ffef1706445728de298eae31", "question": "What is the source of their data?", "answer": ["GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays", "a curated database of high-quality in vivo rodent uterotrophic bioassay data"], "num_words": 4228, "gpt4_answer": "The source of their data is a curated database of high-quality in vivo rodent uterotrophic bioassay data extracted from research publications."}
{"id": "1811.01183.47b7bc232af7bf93338bd3926345e23e9e80c0c1", "question": "What is their binary classifier?", "answer": ["Bernoulli Nave Bayes classifier"], "num_words": 4228, "gpt4_answer": "The binary classifier is a Bernoulli Nave Bayes classifier trained on binary occurrence matrices created using 1-3-grams extracted from the publications."}
{"id": "1811.01183.0b5c599195973c563c4b1a0fe5d8fc77204d71a0", "question": "How long is their dataset?", "answer": ["670", "670 publications"], "num_words": 4228, "gpt4_answer": "The dataset consists of 670 publications with results from 2,615 uterotrophic bioassays."}
{"id": "1811.01183.1397b1c51f722a4ee2b6c64dc9fc6afc8bd3e880", "question": "What is a study descriptor?", "answer": ["Study descriptor is a set of structured data elements extracted from a publication text that contains specific expert knowledge pertaining to domain topics.", "Unanswerable"], "num_words": 4228, "gpt4_answer": "A study descriptor is a specific data element or parameter in a research publication, such as species, route of administration, and dosing regimen, that helps describe the study and its characteristics."}
{"id": "2004.02401.230f127e83ac62dd65fccf6b1a4960cf0f7316c7", "question": "How are experiments designed to measure impact on performance by different choices?", "answer": ["CLR is selected by the range test ... Shrink strategy is applied when examining the effects of CLR in training NMT ... The optimizers (Adam and SGD) are assigned with two options: 1) without shrink (as nshrink\"); 2) with shrink at a rate of 0.5 (yshrink\")", "The learning rate boundary of the CLR is selected by the range test (shown in Figure FIGREF7). The base and maximal learning rates adopted in this study are presented in Table TABREF13. Shrink strategy is applied when examining the effects of CLR in training NMT. The optimizers (Adam and SGD) are assigned with two options: 1) without shrink (as nshrink\"); 2) with shrink at a rate of 0.5 (yshrink\"), which means the maximal learning rate for each cycle is reduced at a decay rate of 0.5."], "num_words": 3304, "gpt4_answer": "The experiments are designed to measure the impact on performance by applying Cyclical Learning Rate (CLR) and various batch sizes to train NMT models, comparing training under CLR with an inverse square for two popular optimizers (Adam and SGD), and examining the effects of different learning rate decay policies and batch sizes on the performance."}
{"id": "2004.02401.75c221920bee14a6153bd5f4c1179591b2f48d59", "question": "What impact on performance is shown for different choices of optimizers and learning rate policies?", "answer": ["Applying CLR has positive impacts on NMT training for both Adam and SGD ... it can be observed that the effects of applying CLR to Adam are more significant than those of SGD ... we see that the trend of CLR with a larger batch size for NMT training does indeed lead to better performance. ... The benefit of a larger batch size afforded by CLR means that training time can be cut down considerably.", "The training takes fewer epochs to converge to reach a local minimum with better BLEU scores"], "num_words": 3304, "gpt4_answer": "The choice of optimizers and the associated cyclical learning rate policy can have a significant impact on the performance of neural machine translation models."}
{"id": "1910.09387.4eb42c5d56d695030dd47ea7f6d65164924c4017", "question": "What domain do the audio samples fall under?", "answer": ["from the online platform Freesound BIBREF8", "speech, speak, and woman"], "num_words": 3405, "gpt4_answer": "The audio samples in Clotho dataset are diverse and come from various domains, as they are collected from the Freesound platform. The 10 most common tags are ambient, water, nature, birds, noise, rain, city, wind, metal, and people."}
{"id": "1910.09387.eff9192e05d23e9a67d10be0c89a7ab2b873995b", "question": "How did they evaluate the quality of annotations?", "answer": ["They manually check the captions and employ extra annotators to further revise the annotations.", "different annotators are instructed to correct any grammatical errors ... score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 ... top $N_{\\text{cp}}$ captions are selected"], "num_words": 3405, "gpt4_answer": "The annotators scored each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better)."}
{"id": "1910.09387.87523fb927354ddc8ad1357a81f766b7ea95f53c", "question": "How many annotators did they have?", "answer": ["Unanswerable"], "num_words": 3405, "gpt4_answer": "Each audio sample in Clotho has five captions, collected by different annotators."}
{"id": "1910.09387.9e9aa8af4b49e2e1e8cd9995293a7982ea1aba0e", "question": "What is their baseline method?", "answer": ["previously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention", "we use a previously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention"], "num_words": 3405, "gpt4_answer": "The baseline method used for audio captioning is an encoder-decoder scheme with attention, which is based on a series of bi-directional gated recurrent units (bi-GRUs) and has been previously utilized for audio captioning."}
{"id": "1809.03695.1fa9b6300401530738995f14a37e074c48bc9fd8", "question": "In what language are the captions written in?", "answer": ["Unanswerable"], "num_words": 1304, "gpt4_answer": "Unanswerable"}
{"id": "1809.03695.9d98975ab0b75640b2c83e29e1438c76a959fbde", "question": "What is the average length of the captions?", "answer": ["Unanswerable"], "num_words": 1304, "gpt4_answer": "Unanswerable"}
{"id": "1809.03695.cc8bcea4052bf92f249dda276acc5fd16cac6fb4", "question": "Does each image have one caption?", "answer": ["Yes"], "num_words": 1304, "gpt4_answer": "Each image in the 8k-Flickr dataset is paired with five different captions."}
{"id": "1809.03695.35f48b8f73728fbdeb271b170804190b5448485a", "question": "What is the size of the dataset?", "answer": ["819", "829 instances"], "num_words": 1304, "gpt4_answer": "The dataset comprises 829 instances."}
{"id": "1809.03695.16edc21a6abc89ee2280dccf1c867c2ac4552524", "question": "What is the source of the images and textual captions?", "answer": [" Image Descriptions dataset, which is a subset of 8k-picture of Flickr ... Image Descriptions dataset which is a subset of the PASCAL VOC-2008 dataset BIBREF16", "PASCAL VOC-2008 dataset ... 8k-Flicker"], "num_words": 1304, "gpt4_answer": "The images and textual captions are derived from the PASCAL VOC-2008 dataset and the 8k-Flickr dataset."}
{"id": "2002.08902.3b8da74f5b359009d188cec02adfe4b9d46a768f", "question": "what evaluation metrics did they use?", "answer": ["Precision \nRecall\nF1", "Precision, recall and F1 score."], "num_words": 1952, "gpt4_answer": "The evaluation metric used in the experiments is the F1 value."}
{"id": "2002.08902.6bce04570d4745dcfaca5cba64075242308b65cf", "question": "what was the baseline?", "answer": ["BiGRU+CRF"], "num_words": 1952, "gpt4_answer": "The baseline model was BiGRU+CRF, which achieved an F1 value of 90.32."}
{"id": "2002.08902.37e6ce5cfc9d311e760dad8967d5085446125408", "question": "what were roberta's results?", "answer": [" the RoBERTa model achieves the highest F1 value of 94.17", "F1 value of 94.17"], "num_words": 1952, "gpt4_answer": "RoBERTa achieved the highest F1 value of 94.17 on the MSRA-2006 dataset for the NER task."}
{"id": "2002.08902.6683008e0a8c4583058d38e185e2e2e18ac6cf50", "question": "which was the worst performing model?", "answer": ["ERNIE-tiny"], "num_words": 1952, "gpt4_answer": "The worst performing model was ERNIE2.0-tiny."}
{"id": "2002.04815.7bd24920163a4801b34d0a50aed957ba8efed0ab", "question": "How long is their sentiment analysis dataset?", "answer": ["2900, 4700, 6900", "Three datasets had total of 14.5k samples."], "num_words": 1568, "gpt4_answer": "The paper does not provide the exact length of the sentiment analysis dataset."}
{"id": "2002.04815.df01e98095ba8765d9ab0d40c9e8ef34b64d3700", "question": "What NLI dataset was used?", "answer": ["SNLI", "Stanford Natural Language Inference BIBREF7"], "num_words": 1568, "gpt4_answer": "The Stanford Natural Language Inference (SNLI) dataset was used."}
{"id": "2002.04815.a7a433de17d0ee4dd7442d7df7de17e508baf169", "question": "What aspects are considered?", "answer": ["Unanswerable", "dot-product attention module to dynamically combine all intermediates"], "num_words": 1568, "gpt4_answer": "Unanswerable"}
{"id": "2002.04815.abfa3daaa984dfe51289054f4fb062ce93f31d19", "question": "What layer gave the better results?", "answer": ["12", "BERT-Attention and BERT-LSTM perform better than vanilla BERT$_{\\tiny \\textsc {BASE}}$"], "num_words": 1568, "gpt4_answer": "Both BERT-LSTM and BERT-Attention have respective advantages on different datasets, and there isn't a clear winner between the two."}
{"id": "2003.12450.1702985a3528e876bb19b8e223399729d778b4e4", "question": "How many annotators were used for sentiment labeling?", "answer": ["Each labelled Data point was verified by at least one other person after initial labelling.", "Three people"], "num_words": 839, "gpt4_answer": "Three annotators were used for sentiment labeling."}
{"id": "2003.12450.f44a9ed166a655df1d54683c91935ab5e566a04f", "question": "How is data collected?", "answer": ["Unanswerable", "original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner)"], "num_words": 839, "gpt4_answer": "Data is collected by calculating compound sentiment scores for about 14,000 Nigerian Pidgin tweets using the original and updated VADER lexicon."}
{"id": "2003.12450.0ba1514fb193c52a15c31ffdcd5c3ddbb2bb2c40", "question": "How much better is performance of Nigerian Pitdgin English sentiment classification of models that use additional Nigerian English data compared to orginal English-only models?", "answer": ["Unanswerable"], "num_words": 839, "gpt4_answer": "The updated VADER lexicon (with 300 Pidgin tokens) performed better than the original VADER lexicon, but the exact improvement percentage is not provided in the paper."}
{"id": "2003.12450.d14118b18ee94dafe170439291e20cb19ab7a43c", "question": "What full English language based sentiment analysis models are tried?", "answer": ["Unanswerable", "the original VADER English lexicon."], "num_words": 839, "gpt4_answer": "Unanswerable"}
{"id": "1905.09439.d922eaa5aa135c1ae211827c6a599b4d69214563", "question": "Do they treat differerent turns of conversation differently when modeling features?", "answer": ["No", "Unanswerable"], "num_words": 1781, "gpt4_answer": "No, the paper does not mention treating different turns of conversation differently when modeling features."}
{"id": "1905.09439.ff668c7e890064756cdd2f9621e1cedb91eef1d0", "question": "How do they bootstrap with contextual information?", "answer": ["Unanswerable", "pre-trained word embeddings need to be tuned with local context during our experiments"], "num_words": 1781, "gpt4_answer": "They bootstrap with contextual information by introducing a Gated Recurrent Neural Network (GRU) model with an attention layer and training it with a multigenre corpus. They also utilize sentiment, subjectivity, and emotion lexicon features."}
{"id": "1905.09439.d3cfbe497a30b750a8de3ea7f2cecf4753a4e1f9", "question": "Which word embeddings do they utilize for the EmoContext task?", "answer": ["ELMo ... fasttext", "word2vec ... GloVe BIBREF7 ... fasttext BIBREF8 ... ELMo"], "num_words": 1781, "gpt4_answer": "The authors utilize fasttext and ELMo word embeddings for the EmoContext task."}
{"id": "1704.03279.73d87f6ead32653a518fbe8cdebd81b4a3ffcac0", "question": "What were the performance results of their network?", "answer": ["For the test set a BLEU score of 25.7 on Ja-En and 20.7 (2014 test set), 23.1 (2015 test set), and 26.1 (2016 test set) on En-De", "gain of 2.2 BLEU compared to the original single NMT network"], "num_words": 3726, "gpt4_answer": "The best results on Japanese-English either yield a gain of 2.2 BLEU compared to the original single NMT network at about the same decoding speed, or a 3.4x CPU decoding speed up with only a minor drop in BLEU."}
{"id": "1704.03279.fda47c68fd5f7b44bd539f83ded5882b96c36dd7", "question": "What were the baselines?", "answer": ["a widely used, simple ensembling method (prediction averaging) ", "simple ensembling method (prediction averaging)"], "num_words": 3726, "gpt4_answer": "The baseline used in the experiments was a widely used, simple ensembling method (prediction averaging)."}
{"id": "1704.03279.643645e02ffe8fde45918615ec92013a035d1b92", "question": "What dataset is used?", "answer": [" Japanese-English (Ja-En) ASPEC data set BIBREF26 ... WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. ", "Japanese-English (Ja-En) ASPEC data set BIBREF26 ... WMT data set for English-German (En-De) ... news-test2014 ... news-test2015 and news-test2016"], "num_words": 3726, "gpt4_answer": "The primary dataset used in this paper is the Japanese-English (Ja-En) ASPEC data set. The authors also apply their method to the WMT dataset for English-German (En-De)."}
{"id": "1704.03279.a994cc18046912a8c9328dc572f4e4310736c0e2", "question": "Do they explore other language pairs?", "answer": ["English-German (En-De)", "Yes"], "num_words": 3726, "gpt4_answer": "Yes, they apply their method to the WMT dataset for English-German (En-De) in addition to the Japanese-English (Ja-En) ASPEC dataset."}
{"id": "1901.05389.9baca9bdb8e7d5a750f8cbe3282beb371347c164", "question": "How do they preprocess Tweets?", "answer": ["They removed retweets, URLs, emoticons, mentions of other users, hashtags; lowercased the text and removed the punctuation.", "re-tweets do not bring any additional information to our study, thus we removed them ...  removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags ... downcased and stripped the punctuation", "removing URLs, emoticons, mentions of other users, hashtags; downcasing and stripping punctuations"], "num_words": 5465, "gpt4_answer": "They preprocess Tweets by removing retweets, semantically meaningless expressions like URLs, emoticons, mentions of other users, and hashtags, and also downcasing and stripping the text of punctuation."}
{"id": "1901.05389.2cb20bae085b67e357ab1e18ebafeac4bbde5b4a", "question": "What kind of inference model do they build to estimate socioeconomic status?", "answer": ["XGBoost", "XGBoost algorithm BIBREF43", "XGBoost, an ensemble of gradient-based decision trees algorithm "], "num_words": 5465, "gpt4_answer": "The authors build inference models using gradient-boosted decision trees, specifically the XGBoost algorithm, to estimate socioeconomic status based on user profile information and tweets collected from Twitter users."}
{"id": "1901.05389.892ee7c2765b3764312c3c2b6f4538322efbed4e", "question": "How much data do they gather in total?", "answer": ["90,369,215 tweets", "90,369,215 tweets written in French, posted by 1.3 Million users", "They created 3 datasets with combined size of 37193."], "num_words": 5465, "gpt4_answer": "The paper gathered a total of 90,369,215 tweets written in French, posted by 1.3 million users between August 2014 to July 2015."}
{"id": "1901.05389.c68946ae2e548ec8517c7902585c032b3f3876e6", "question": "Do they analyze features which help indicate socioeconomic status?", "answer": ["No", "Yes"], "num_words": 5465, "gpt4_answer": "Yes, the paper analyzes features such as user semantics in communication, habitat, occupation, and social network to infer the socioeconomic status of French Twitter users."}
{"id": "1901.05389.7557f2c3424ae70e2a79c51f9752adc99a9bdd39", "question": "What inference models are used?", "answer": ["XGBoost ... AdaBoost ... Random Forest"], "num_words": 5465, "gpt4_answer": "The inference models used in this study are XGBoost, AdaBoost, and Random Forest."}
{"id": "1901.05389.b03249984c26baffb67e7736458b320148675900", "question": "What baseline model is used?", "answer": ["AdaBoost ... Random Forest"], "num_words": 5465, "gpt4_answer": "The baseline models used for comparison in this study are AdaBoost and Random Forest."}
{"id": "1901.05389.9595fdf7b51251679cd39bc4f6befc81f09c853c", "question": "How is the remotely sensed data annotated?", "answer": ["The SES score was assigned by architects based on the satellite and Street View images of users' homes."], "num_words": 5465, "gpt4_answer": "The remotely sensed data is annotated by involving architects to assign a SES score (from 1 to 9) to a sample set of selected locations based on the satellite and Street View images around it."}
{"id": "1901.05389.08c0d4db14773cbed8a63e69381a2265e85f8765", "question": "Where are the professional profiles crawled from?", "answer": ["LinkedIn"], "num_words": 5465, "gpt4_answer": "The professional profiles are crawled from LinkedIn."}
{"id": "1808.10290.5e29f16d7302f24ab93b7707d115f4265a0d14b0", "question": "How much additional data do they manage to generate from translations?", "answer": ["45680", "In case of 2-votes they used 9,298 samples and in case of 3-votes they used 1,298 samples. "], "num_words": 1917, "gpt4_answer": "The size of the extra data used in the paper is about 10 times larger than their 2-votes data."}
{"id": "1808.10290.26844cec57df6ff0f02245ea862af316b89edffe", "question": "Do they train discourse relation models with augmented data?", "answer": ["No", "Yes"], "num_words": 1917, "gpt4_answer": "Yes, they train discourse relation models with augmented data."}
{"id": "1808.10290.d1d59bca40b8b308c0a35fed1b4b7826c85bc9f8", "question": "How many languages do they at most attempt to use to generate discourse relation labelled data?", "answer": ["4", "four languages"], "num_words": 1917, "gpt4_answer": "The authors attempt to use three languages to generate discourse relation labelled data."}
{"id": "1612.04118.4d824b49728649432371ecb08f66ba44e50569e0", "question": "by how much did the system improve?", "answer": ["By more than 90%", "false positives improved by 90% and recall improved by 1%"], "num_words": 1690, "gpt4_answer": "The system reduced the number of false positive extractions in financial information extraction application by more than 50% relative to a mature system."}
{"id": "1612.04118.02a5acb484bda77ef32a13f5d93d336472cf8cd4", "question": "what existing databases were used?", "answer": ["a database containing historical time series data", "database containing historical time series data"], "num_words": 1690, "gpt4_answer": "The paper used a database containing historical time series data for training the neural network."}
{"id": "1612.04118.863d8d32a1605402e11f0bf63968a14bcfd15337", "question": "what existing parser is used?", "answer": ["Unanswerable", "candidate-generating parser "], "num_words": 1690, "gpt4_answer": "Unanswerable"}
{"id": "1804.01155.d4b84f48460517bc0a6d4e0c38f6853c58081166", "question": "How do they combine the socioeconomic maps with Twitter data? ", "answer": ["By matching users to locations using geolocated tweets data, then matching locations to socioeconomic status using INSEE sociodemographic data.", "Match geolocation data for Twitter users with patches from INSEE socioeconomic maps."], "num_words": 7150, "gpt4_answer": "They combine the socioeconomic maps with Twitter data by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This allows them to obtain socioeconomic status indicators for the Twitter users."}
{"id": "1804.01155.90756bdcd812b7ecc1c5df2298aa7561fd2eb02c", "question": "Does the fact that people are active during the day time define their SEC?", "answer": ["No", "No, but the authors identified a correlation."], "num_words": 7150, "gpt4_answer": "People of higher socioeconomic status are active to a greater degree during the daytime and use a more standard language."}
{"id": "1804.01155.028d0d9b7a71133e51a14a32cd09dea1e2f39f05", "question": "How did they define standard language?", "answer": ["Standard usage of negation ... Standard usage of plural ending of written words ... lexical diversity", "Use of both French negative particles and spelling out plural ending on adjectives and nouns"], "num_words": 7150, "gpt4_answer": "The standard language was defined based on the usage of standard negation and plural ending of written words. Standard negation includes both negative particles (ne and pas) in French, while standard plural refers to the correct usage of plural endings (s or x) in written French."}
{"id": "1804.01155.cfc73e0c82cf1630b923681c450a541a964688b9", "question": "How do they operationalize socioeconomic status from twitter user data?", "answer": ["Unanswerable", "we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location"], "num_words": 7150, "gpt4_answer": "Socioeconomic status is operationalized by combining Twitter data with the socioeconomic maps from the National Institute of Statistics and Economic Studies (INSEE) of France. Each geolocated Twitter user is assigned to a patch closest to their estimated home location, which provides them with a set of socioeconomic status indicators like average yearly income per capita, fraction of real estate owners, and population density."}
{"id": "1711.11017.3746aaa1a81d9c725bc7a4a67086634c11998d39", "question": "Do the authors provide any benchmark tasks in this new environment?", "answer": ["No"], "num_words": 1490, "gpt4_answer": "Yes, the authors provide several tasks such as reinforcement learning, language grounding, sound-based navigation, robotics, and multi-agent learning."}
{"id": "1805.06648.143409d16125790c8db9ed38590a0796e0b2b2e2", "question": "What dimensions do the considered embeddings have?", "answer": ["100, 200, 400", "Answer with content missing: (Models sections) 100, 200 and 400"], "num_words": 1771, "gpt4_answer": "Unanswerable"}
{"id": "1805.06648.8ba582939823faae6822a27448ea011ab6b90ed7", "question": "How are global structures considered?", "answer": ["Unanswerable", "global structure in the learned embeddings is related to a linearity in the training objective"], "num_words": 1771, "gpt4_answer": "Global structures are considered by incorporating dependencies between distant regions of the input space and exploiting global symmetries or regularities in the representation of words and sentences to support extrapolation outside the training data."}
{"id": "1610.06510.65c7a2b734dab51c4c81f722527424ff33b023f8", "question": "Which translation model do they employ?", "answer": ["BPE level ... PBSMT models at morpheme and OS levels", "subword level phrase-based SMT model"], "num_words": 4053, "gpt4_answer": "They employ phrase-based statistical machine translation (PBSMT) models."}
{"id": "1610.06510.11ef46187a5bf15e89d63220fdeaecbeb92d818e", "question": "Which datasets do they experiment on?", "answer": ["Indian Language Corpora Initiative (ILCI) corpus ... OpenSubtitles2016 section of the OPUS corpus", "multilingual Indian Language Corpora Initiative (ILCI) corpus BIBREF25 ... OpenSubtitles2016 section of the OPUS corpus collection BIBREF26"], "num_words": 4053, "gpt4_answer": "The datasets used in the experiments are obtained from the multilingual Indian Language Corpora Initiative (ILCI) corpus and the OpenSubtitles2016 section of the OPUS corpus collection."}
{"id": "1610.06510.45aab23790161cbc55f78e16fdf5678a3f5b4b92", "question": "Which other units of text do they experiment with (apart from BPE and ortographic syllables)?", "answer": ["character ...  morpheme ... word", "character ... morpheme ... word"], "num_words": 4053, "gpt4_answer": "They experimented with character, morpheme, and word units in addition to BPE and orthographic syllables."}
{"id": "1610.06510.bf5e80f1ab4eae2254b4f4d7651969a3cf945fb4", "question": "How many steps of BPE do they experiment with?", "answer": ["Unanswerable", "from 1000 to 4000"], "num_words": 4053, "gpt4_answer": "The authors experimented with the number of BPE merge operations ranging from 1000 to 4000."}
{"id": "1910.01160.0a70af6ba334dfd3574991b1dd06f54fc6a700f2", "question": "What nuances between fake news and satire were discovered?", "answer": ["satire articles are more sophisticated, or less easy to read, than fake news articles", "semantic and linguistic differences between ...  satire articles are more sophisticated, or less easy to read, than fake news articles"], "num_words": 2157, "gpt4_answer": "Nuances discovered between fake news and satire include that satire articles are more sophisticated and less easy to read than fake news articles, and they have differences in semantic features and coherence metrics."}
{"id": "1910.01160.98b97d24f31e9c535997e9b6cb126eb99fc72a90", "question": "What empirical evaluation was used?", "answer": ["Empirical  evaluation  has done using 10 fold cross-validation considering semantic representation with BERT and  measuring   differences between fake news and satire  using coherence metric.", "coherence metrics"], "num_words": 2157, "gpt4_answer": "The empirical evaluation used was the F1 score with a ten-fold cross-validation on the dataset of fake news and satire articles."}
{"id": "1910.01160.71b07d08fb6ac8732aa4060ae94ec7c0657bb1db", "question": "What is the baseline?", "answer": ["Naive Bayes Multinomial algorithm", "model using the Naive Bayes Multinomial algorithm"], "num_words": 2157, "gpt4_answer": "The baseline is a model using the Naive Bayes Multinomial algorithm from Golbeck et al.'s work, which is a word-based classification approach for distinguishing between fake news and satirical articles."}
{"id": "1910.01160.812c974311747f74c3aad23999bfef50539953c8", "question": "Which linguistic features are used?", "answer": ["Coh-Metrix indices", "First person singular pronoun incidence\nSentence length, number of words, \nEstimates of hypernymy for nouns \n...\nAgentless passive voice density,\nAverage word frequency for content words ,\nAdverb incidence\n\n..."], "num_words": 2157, "gpt4_answer": "The linguistic features used include text statistics, referential cohesion, text readability formulas, and different types of connective words."}
{"id": "1910.01160.180c7bea8caf05ca97d9962b90eb454be4176425", "question": "What contextual language model is used?", "answer": ["BERT", "BERT "], "num_words": 2157, "gpt4_answer": "BERT is used as the contextual language model."}
{"id": "1602.07776.95083d486769b9b5e8c57fe2ef1b452fc3ea5012", "question": "what state of the art models do they compare to?", "answer": ["IKN 5-gram, LSTM LM", "Vinyals et al (2015) for English parsing, Wang et al (2015) for Chinese parsing, and LSTM LM for Language modeling both in English and Chinese "], "num_words": 3949, "gpt4_answer": "The paper does not explicitly mention specific state-of-the-art models they compare to. However, they mention that their generative model outperforms every previously published parser built on a single supervised generative model in English and is slightly behind the best-reported generative model in Chinese. As language models, RNNGs outperform the best single-sentence language models."}
{"id": "1908.05908.4c7ec282697f4f6646eb1c19f46bbaf8670b0de6", "question": "What is the weak supervision signal used in Baidu Baike corpus?", "answer": ["NER Pretraining", "consider the title of each sample as a pseudo label and conduct NER pre-training"], "num_words": 2879, "gpt4_answer": "The weak supervision signal used in Baidu Baike corpus is the title of each sample, which is considered as a pseudo label for NER pre-training."}
{"id": "1908.05908.07104dd36a0e7fdd2c211ad710de9a605495b697", "question": "How is BERT optimized for this task?", "answer": ["NER (Named Entity Recognition) is the first task in the joint multi-head selection model ... relation classification task as a multi-head selection problem ... auxiliary sentence-level relation classification prediction task", "We also optimize the pre-training process of BERT by introducing a semantic-enhanced task."], "num_words": 2879, "gpt4_answer": "BERT is optimized for this task by introducing a semantic-enhanced task during pre-training, which incorporates previous sentence prediction and document-level prediction."}
{"id": "1908.05908.3e88fcc94d0f451e87b65658751834f6103b2030", "question": "What is a soft label?", "answer": [" To solve the problem that one entity belongs to multiple triplets, a multi-sigmoid layer is applied ... soft label embedding, which takes the logits as input to preserve probability of each entity type", "we proposed soft label embedding, which takes the logits as input to preserve probability of each entity type"], "num_words": 2879, "gpt4_answer": "A soft label is a label that takes the logits as input to preserve the probability of each entity type, allowing for more effective information transmission between entity recognition and relation extraction tasks."}
{"id": "1809.02208.c8cf20afd75eb583aef70fcb508c4f7e37f234e1", "question": "Do the authors examine the real-world distribution of female workers in the country/countries where the gender neutral languages are spoken?", "answer": ["No", "Unanswerable"], "num_words": 5563, "gpt4_answer": "The authors do not specifically examine the real-world distribution of female workers in the countries where the gender-neutral languages are spoken. Instead, they use data from the U.S. Bureau of Labor Statistics to compare female participation in various job positions with the gender pronoun distribution in Google Translate outputs."}
{"id": "1809.02208.3567241b3fafef281d213f49f241071f1c60a303", "question": "Which of the 12 languages showed the strongest tendency towards male defaults?", "answer": ["Malay", "in general ... exception of the Basque idiom"], "num_words": 5563, "gpt4_answer": "Basque showed the strongest tendency towards male defaults."}
{"id": "1809.02208.d5d48b812576470edbf978fc18c00bd24930a7b7", "question": "How many different sentence constructions are translated in gender neutral languages?", "answer": ["17", "Unanswerable"], "num_words": 5563, "gpt4_answer": "12 different gender-neutral languages are used for translation in the study."}
{"id": "1908.09283.643527e94e8eed1e2229915fcf8cd74d769173fc", "question": "What are the evaluation metrics used?", "answer": ["ROC curve (AUC) BIBREF32 ... $F_{1}$ score BIBREF33", "ROC curve (AUC) BIBREF32 and $F_{1}$ score BIBREF33 "], "num_words": 3817, "gpt4_answer": "The evaluation metrics used are area under the ROC curve (AUC) and F1 score."}
{"id": "1908.09283.bfd55ae9630a08a9e287074fff3691dfbffc3258", "question": "What are the baselines?", "answer": ["GBDT ... LR ... RNN", "LR Here ... GBDT Here ... RNN Here ... ours-MTL "], "num_words": 3817, "gpt4_answer": "The baselines are LR (logistic regression), GBDT (Gradient Boosting Decision Trees), RNN (Recurrent Neural Networks), and ours-MTL (encoder-decoder model without multi-task learning)."}
{"id": "1908.09283.3a06d40a4bf5ba6e26d9138434e9139a014deb40", "question": "Which language learning datasets are used?", "answer": ["Duolingo SLA modeling shared datasets", "Duolingo SLA modeling shared datasets, which have three datasets and are collected from English students who can speak Spanish (en_es), Spanish students who can speak English (es_en), and French students who can speak English (fr_en) BIBREF29"], "num_words": 3817, "gpt4_answer": "The paper uses Duolingo SLA modeling shared datasets, which include three datasets: English students who can speak Spanish (en_es), Spanish students who can speak English (es_en), and French students who can speak English (fr_en)."}
{"id": "1802.03052.641fe5dc93611411582e6a4a0ea2d5773eaf0310", "question": "What does it mean for sentences to be \"lexically overlapping\"?", "answer": ["They share words.", "share words"], "num_words": 4489, "gpt4_answer": "Lexically overlapping sentences share common words or phrases, indicating a connection between the concepts or ideas expressed in those sentences."}
{"id": "1802.03052.7d34cdd9cb1c988e218ce0fd59ba6a3b5de2024a", "question": "How many tables are in the tablestore?", "answer": ["62"], "num_words": 4489, "gpt4_answer": "There are 62 tables in the tablestore."}
{"id": "1809.08899.83db51da819adf6faeb950fe04b4df942a887fb5", "question": "what dataset is used?", "answer": ["Essays collected from students from American Institutes for Research tests, Synthetic responses from Reddit and Teen Line", "Student responses to the American Institutes for Research tests."], "num_words": 3864, "gpt4_answer": "The dataset used consists of student responses collected by the American Institutes for Research (AIR), which includes free-form text responses, comments, and notes from online tests. Synthetic responses from Reddit and Teen Line were also used in the initial data collection."}
{"id": "1809.08899.7e7471bc24970c6f23baff570be385fd3534926c", "question": "what neural network models are used?", "answer": ["GRU and LSTM models with a combination of the following characteristics: bidirectional vs normal, attention vs no attention, stacked vs flat.", "GRU, Stacked GRU, Bidirectional GRU, Bidirectional stacked GRU, GRU with attention, Stacked GRU with Attention, Bidirectional GRU with attention, Bidirectional Stacked GRU with Attention, LSTM, Stacked LSTM, Bidirectional LSTM, Bidirectional stacked LSTM, LSTM with attention, Stacked LSTM with Attention, Bidirectional LSTM with attention, Bidirectional Stacked LSTM with Attention"], "num_words": 3864, "gpt4_answer": "The neural network models used in this paper are Gated Recurrent Units (GRUs) and Long-Short-Term-Memory (LSTM) units."}
{"id": "1809.08899.ec5e84a1d1b12f7185183d165cbb5eae66d9833e", "question": "Do they report results only on English data?", "answer": ["Unanswerable", "Yes"], "num_words": 3864, "gpt4_answer": "Unanswerable"}
{"id": "1809.08899.7f958017cbb08962c80e625c2fd7a1e2375f27a3", "question": "What baseline model is used?", "answer": ["Logistic regression with TF-IDF with latent semantic analysis representations", "logistic regression applied to a TF-IDF model"], "num_words": 3864, "gpt4_answer": "The baseline model used is logistic regression applied to a TF-IDF model with latent semantic analysis used to reduce the representations of words to three hundred dimensions."}
{"id": "1809.08899.4130651509403becc468bdbe973e63d3716beade", "question": "What type of neural network models are used?", "answer": ["GRU, LSTM", "Recurrent neural network"], "num_words": 3864, "gpt4_answer": "The paper uses recurrent neural network (RNN) models, specifically Gated Recurrent Units (GRUs) and Long-Short-Term-Memory (LSTM) units."}
{"id": "1809.08899.6edef748370e63357a57610b5784204c9715c0b4", "question": "How is validity identified and what metric is used to quantify it?", "answer": ["Unanswerable", "approximation of the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts"], "num_words": 3864, "gpt4_answer": "The metric used to evaluate the efficacy of the models is an approximation of the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts."}
{"id": "1809.08899.6b302280522c350c4d1527d8c6ebc5b470f9314c", "question": "How is severity identified and what metric is used to quantify it?", "answer": ["Severity is manually identified by a team of reviewers.", "Unanswerable"], "num_words": 3864, "gpt4_answer": "The paper does not provide a specific metric for quantifying severity in student responses."}
{"id": "1809.08899.7da138ec43a88ea75374c40e8491f7975db29480", "question": "How is urgency identified and what metric is used to quantify it?", "answer": ["Unanswerable", "Urgency is manually identified by a team of reviewers."], "num_words": 3864, "gpt4_answer": "Urgency is not explicitly identified or quantified in the paper; the focus is on detecting alarming student responses using neural network models."}
{"id": "1711.11118.d5d4504f419862275a532b8e53d0ece16e0ae8d1", "question": "How many of the attribute-value pairs are found in video?", "answer": ["Unanswerable"], "num_words": 2071, "gpt4_answer": "Unanswerable"}
{"id": "1711.11118.f1e70b63c45ab0fc35dc63de089c802543e30c8f", "question": "How many of the attribute-value pairs are found in audio?", "answer": ["Unanswerable"], "num_words": 2071, "gpt4_answer": "Unanswerable"}
{"id": "1711.11118.39d20b396f12f0432770c15b80dc0d740202f98d", "question": "How many of the attribute-value pairs are found in images?", "answer": ["Unanswerable"], "num_words": 2071, "gpt4_answer": "Unanswerable"}
{"id": "1711.11118.4e0df856b39055a9ba801cc9c8e56d5b069bda11", "question": "How many of the attribute-value pairs are found in semi-structured text?", "answer": ["Unanswerable"], "num_words": 2071, "gpt4_answer": "The paper does not provide a specific number for attribute-value pairs found in semi-structured text."}
{"id": "1711.11118.bbc6d0402cae16084261f8558cebb4aa6d5b1ea5", "question": "How many of the attribute-value pairs are found in unstructured text?", "answer": ["Unanswerable"], "num_words": 2071, "gpt4_answer": "Unanswerable"}
{"id": "1711.11118.a7e03d24549961b38e15b5386d9df267900ef4c8", "question": "How many different semi-structured templates are represented in the data?", "answer": ["7.6 million", "Unanswerable"], "num_words": 2071, "gpt4_answer": "Unanswerable"}
{"id": "1711.11118.036c400424357457e42b22df477b7c3cdc2eefe9", "question": "Are all datapoints from the same website?", "answer": ["No"], "num_words": 2071, "gpt4_answer": "No, the data is collected from a large number of e-commerce sites using the Diffbot Product API."}
{"id": "1711.11118.63eda2af88c35a507fbbfda0ec1082f58091883a", "question": "Do they consider semi-structured webpages?", "answer": ["No", "Yes"], "num_words": 2071, "gpt4_answer": "Yes, they consider semi-structured web pages as one of the varied types of data on the web."}
{"id": "1911.09753.fe6181ab0aecf5bc8c3def843f82e530347d918b", "question": "What are the baseline models?", "answer": [" MLE model ... Baseline$+(t)$", "MLE model"], "num_words": 4630, "gpt4_answer": "The baseline models are 1) an MLE model trained on the Conceptual Captions training split alone, referred to as Baseline, and 2) Baseline$+(t)$ models, which merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and fine-tune the baseline model."}
{"id": "1911.09753.0b1b8e1b583242e5be9b7be73160630a0d4a96b2", "question": "What image caption datasets were used in this work?", "answer": ["Conceptual Captions", "Conceptual Captions BIBREF0"], "num_words": 4630, "gpt4_answer": "The authors used the Conceptual Captions dataset and the Caption-Quality dataset in this work."}
{"id": "1911.09753.830f9f9499b06fb4ac3ce2f2cf035127b4f0ec63", "question": "How long does it take to train the model on the mentioned dataset? ", "answer": ["3M iterations with the batch size of 4,096", "Unanswerable"], "num_words": 4630, "gpt4_answer": "The paper does not provide exact training time for the model on the mentioned dataset."}
{"id": "1911.09753.a606bffed3bfeebd1b66125be580f908244e5d92", "question": "How big is the human ratings dataset?", "answer": ["1K images sampled from the Open Images Dataset", "validation and test splits containing approximately 130K, 7K and 7K"], "num_words": 4630, "gpt4_answer": "The Caption-Quality human ratings dataset used in the experiments contains approximately 130K, 7K, and 7K rated captions in the training, validation, and test splits, respectively."}
{"id": "1709.05700.f8fe4049bea86d0518d1881f32049e60526d0f34", "question": "What existing techniques do the authors compare against?", "answer": ["ANGE ... ATEEMA ... GENTREE ", "ANGE, ATEEMA, GENTREE, and NUMNORM"], "num_words": 6666, "gpt4_answer": "The authors compare MERF against existing techniques such as CPSL, SystemT, TEXTMARKER, QARAB, and the work in urbain2012user."}
{"id": "1908.05758.a9eb8039431e2cb885cfcf96eb58c0675b36b3bd", "question": "Is the dataset completely automatically generated?", "answer": ["Yes"], "num_words": 3195, "gpt4_answer": "Yes, the dataset is automatically generated without human labor involved in labeling."}
{"id": "1908.05758.998fa38634000f2d7b52d16518b9e18e898ce933", "question": "Does the SESAME dataset include discontiguous entities?", "answer": ["No"], "num_words": 3195, "gpt4_answer": "Unanswerable"}
{"id": "1908.05758.a82686c054b96f214521e468b17f0435e6cdf7cf", "question": "How big is the resulting SESAME dataset?", "answer": ["3,650,909 sentences", "3,650,909 sentences ...  87,769,158 tokens"], "num_words": 3195, "gpt4_answer": "The resulting SESAME dataset consists of 3,650,909 sentences and 87,769,158 tokens."}
{"id": "1903.00384.80d425258d027e3ca3750375d170debb9d92fbc6", "question": "Can their method be transferred to other Q&A platforms (in other languages)?", "answer": ["No", "Unanswerable"], "num_words": 2505, "gpt4_answer": "The paper does not specifically mention the transferability of their method to other Q&A platforms in other languages."}
{"id": "1903.00384.2ae66798333b905172e2c0954e9808662ab7f221", "question": "What measures of quality do they use for a Q&A platform?", "answer": ["MAE and RMSE ", "Rating scores given by users"], "num_words": 2505, "gpt4_answer": "The measures of quality used for a Q&A platform in this paper are Mean Absolute Error (MAE) and Root Mean Square Error (RMSE)."}
{"id": "1809.00129.9d80ad8cf4d5941a32d33273dc5678195ad1e0d2", "question": "Do they evaluate whether local or global context proves more important?", "answer": ["No"], "num_words": 1962, "gpt4_answer": "The paper does not specifically evaluate whether local or global context is more important."}
{"id": "1809.00129.bd817a520a62ddd77e65e74e5a7e9006cdfb19b3", "question": "How many layers of recurrent neural networks do they use for encoding the global context?", "answer": ["2", "8"], "num_words": 1962, "gpt4_answer": "The model uses two BiGRU (bi-directional gated recurrent unit) layers for encoding the global context."}
{"id": "1809.00129.c635295c2b77aaab28faecca3b5767b0c4ab3728", "question": "How did their model rank in three CMU WMT2018 tracks it didn't rank first?", "answer": ["3rd in En-De (SMT), 2nd in En-De (NNT) and 2nd ibn De-En", "Second on De-En and En-De (NMT) tasks, and third on En-De (SMT) task."], "num_words": 1962, "gpt4_answer": "The model ranked second place on English-German (NMT) and German-English word-level QE tasks."}
{"id": "1710.11027.7f8fc3c7d59aba80a3e7c839db6892a1fc329210", "question": "Do they evaluate only on English datasets?", "answer": ["Unanswerable"], "num_words": 2400, "gpt4_answer": "Yes, the evaluation is performed on the Ritter dataset, which is an English dataset."}
{"id": "1710.11027.2d92ae6b36567e7edb6afdd72f97b06ac144fbdf", "question": "What is the Ritter dataset?", "answer": [" a gold standard for NER in microblogs", "Unanswerable"], "num_words": 2400, "gpt4_answer": "The Ritter dataset is a gold standard dataset for Named Entity Recognition (NER) in microblogs."}
{"id": "1710.11027.a5df7361ae37b9512fb57cb93efbece9ded8cab1", "question": "Does this model perform better than the state of the art?", "answer": ["No"], "num_words": 2400, "gpt4_answer": "The proposed model slightly outperforms state-of-the-art models that do not rely on encoded rules, achieving an average of 0.59 F1, while other models achieve 0.49 and 0.54 F1."}
{"id": "1710.11027.915e4d0b3cb03789a20380ead961d473cb95bfc3", "question": "What features are extracted from text?", "answer": ["extracted features using a classical sparse vectorizer (Term frequency-Inverse document frequency - TF-ID", "word feature"], "num_words": 2400, "gpt4_answer": "Features are extracted from text using a classical sparse vectorizer (Term frequency-Inverse document frequency - TF-IDF)."}
{"id": "1710.11027.c01a8b42fd27b0a3bec717ededd98b6d085a0f5c", "question": "What features are extracted from images?", "answer": ["BoF (Bag of Features) BIBREF13 ... SIFT (Scale Invariant Feature Transform) features BIBREF12", "LOC (Building, Suburb, Street, City, Country, Mountain, Highway, Forest, Coast and Map), ORG (Company Logo), PER (Human Face )."], "num_words": 2400, "gpt4_answer": "The features extracted from images are SIFT (Scale Invariant Feature Transform) features."}
{"id": "1911.10401.8e113fd9661bc8af97e30c75a20712f01fc4520a", "question": "What are the baseline models?", "answer": ["ELMo ... USE  ... NBSVM  ... FastText  ... XLnet base cased model (XLnet ... BERT base cased (BERT-Cased)  ... BERT base uncased (BERT-Uncased) ... RoBERTa ", "ELMo ... USE ... NBSVM ... FastText ... XLnet base cased model (XLnet) ... BERT base cased (BERT-Cased) ... BERT base uncased (BERT-Uncased) ... RoBERTa base model"], "num_words": 4770, "gpt4_answer": "The baseline models include ELMo, USE, NBSVM, FastText, XLnet, BERT-Cased, and BERT-Uncased."}
{"id": "1911.10401.35e0e6f89b010f34cfb69309b85db524a419c862", "question": "How are the three different forms defined in this work?", "answer": ["Irony, sarcasm and metaphor are figurative language form. Irony and sarcasm are considered as a way of indirect denial.", "We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial."], "num_words": 4770, "gpt4_answer": "The three common figurative language (FL) expression forms identified in this work are irony, sarcasm, and metaphor."}
{"id": "1911.10401.992e67f706c728bc0e534f974c1656da10e7a724", "question": "What datasets are used for training and testing?", "answer": ["SemEval-2018 ...  Riloffs high quality sarcastic unbalanced dataset ...  a large dataset containing political comments from Reddit ... SA dataset that contains tweets with various FL forms from SemEval-2015 Task 11 ", "dataset provided in Semantic Evaluation Workshop Task 3 ...  ironic tweets BIBREF95 ... Riloffs high quality sarcastic unbalanced dataset BIBREF96 ...  a large dataset containing political comments from Reddit BIBREF97 ... SA dataset that contains tweets with various FL forms from SemEval-2015 Task 11 BIBREF66"], "num_words": 4770, "gpt4_answer": "The datasets used for training and testing include SemEval-2018 Task 3, Reddit SARC politics, Riloff Twitter, and SemEval 2015 Task 11."}
{"id": "1911.10401.61e96abdc924c34c6b82a587168ea3d14fe792d1", "question": "Does approach handle overlapping forms (e.g., metaphor and irony)?", "answer": ["Yes"], "num_words": 4770, "gpt4_answer": "Unanswerable"}
{"id": "1911.10401.ee8a77cddbe492c686f5af3923ad09d401a741b5", "question": "Does this work differentiate metaphor(technique) from irony and sarcasm (purpose)? ", "answer": ["No", "Yes"], "num_words": 4770, "gpt4_answer": "The paper focuses on identifying figurative language forms such as irony, sarcasm, and metaphor but does not specifically differentiate between metaphor as a technique and irony and sarcasm as purposes."}
{"id": "1911.03854.552b1c813f25bf39ace6cd5eefa56f4e4dd70c84", "question": "What classification tasks do they experiment on?", "answer": ["They experiment on 3 types of classification tasks with different inputs:\n2-way: True/False\n3-way: True/False news with text true in real world/False news with false text\n5-way: True/Parody/Missleading/Imposter/False Connection", "fake news detection through text, image and text+image modes"], "num_words": 1979, "gpt4_answer": "They experiment on 2-way, 3-way, and 5-way classification tasks."}
{"id": "1911.03854.1100e442e00c9914538a32aca7af994ce42e1b66", "question": "What categories of fake news are in the dataset?", "answer": ["Satire/Parody ... Misleading Content ... Imposter Content ... False Connection"], "num_words": 1979, "gpt4_answer": "The dataset includes categories such as Satire/Parody, Misleading Content, Imposter Content, and False Connection."}
{"id": "1708.03699.82b93ecd2397e417e1e80f93b7cf49c7bd9aeec3", "question": "How much gain in performance was obtained with user embeddings?", "answer": ["16.89 points on G-test from the baseline tBase", "On test set RNN that uses user embedding has AUC of 80.53 compared to base RNN 79.24."], "num_words": 2216, "gpt4_answer": "User embeddings led to the biggest performance gains in the experiment."}
{"id": "1608.01972.2973fe3f5b4bf70ada02ac4a9087dd156cc3016e", "question": "By how much does their similarity measure outperform BM25?", "answer": ["Unanswerable", "embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007"], "num_words": 4061, "gpt4_answer": "The embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007, respectively."}
{"id": "1608.01972.42269ed04e986ec5dc4164bf57ef306aec4a1ae1", "question": "How do they represent documents when using their proposed similarity measure?", "answer": ["documents are represented by normalized bag-of-words (BOW) vectors", "normalized bag-of-words vectors"], "num_words": 4061, "gpt4_answer": "Documents are represented by normalized bag-of-words (BOW) vectors, with word weights incorporating inverse document frequency (idf)."}
{"id": "1608.01972.31a3ec8d550054465e55a26b0136f4d50d72d354", "question": "How do they propose to combine BM25 and word embedding similarity?", "answer": ["They merge features of BM25 and semantic measures.", "Yes"], "num_words": 4061, "gpt4_answer": "The authors propose to combine BM25 and word embedding similarity using the learning to rank approach, LambdaMART."}
{"id": "1608.01972.a7e1b13cc42bfe78d37b9c943de6288e5f00f01b", "question": "Do they use pretrained word embeddings to calculate Word Mover's distance?", "answer": ["No"], "num_words": 4061, "gpt4_answer": "No, they used the skip-gram model of word2vec trained on titles and abstracts from over 25 million PubMed documents to obtain word embeddings."}
{"id": "1705.01306.49cd18448101da146c3187a44412628f8c722d7b", "question": "Which Twitter sentiment treebank is used?", "answer": ["Our training data for this part was created by taking a random sample from Twitter and having it manually annotated on a 5-label basis to produce fully sentiment-labeled parse-trees", "They built their own"], "num_words": 2320, "gpt4_answer": "The Twitter sentiment treebank used is a private dataset created by taking a random sample from Twitter and manually annotating it on a 5-label basis to produce fully sentiment-labeled parse-trees, similar to the Stanford sentiment treebank."}
{"id": "1705.01306.e9260f6419c35cbd74143f658dbde887ef263886", "question": "Where did the system place in the other sub-tasks?", "answer": ["Unanswerable", "which we scored lower"], "num_words": 2320, "gpt4_answer": "The paper does not provide specific rankings for the other sub-tasks."}
{"id": "1705.01306.2834a340116026d5995e537d474a47d6a74c3745", "question": "What were the five labels to be predicted in sub-task C?", "answer": ["very negative ... negative ... neutral ... positive ... very positive", "very negative, negative, neutral, positive, very positive"], "num_words": 2320, "gpt4_answer": "The five labels to be predicted in sub-task C were very negative, negative, neutral, positive, and very positive."}
{"id": "2003.13028.bd53399be8ff59060792da4c8e42a7fc1e6cbd85", "question": "What is the previous state-of-the-art?", "answer": ["BART LARGE", "Transformer-based encoder-decoder"], "num_words": 3319, "gpt4_answer": "The previous state-of-the-art model for abstractive summarization was PEGASUS$_\\mathrm {HugeNews}$."}
{"id": "2003.13028.a7313c29b154e84b571322532f5cab08e9d49e51", "question": "What is the architecture of the decoder?", "answer": ["M blocks, each consisting of self-attention module, context-attention module, and a two-layer feed-forward network.", "self-attention module, a context-attention module, and a two-layer feed-forward network"], "num_words": 3319, "gpt4_answer": "The decoder consists of M layer decoder blocks, which include a self-attention module, a context-attention module, and a two-layer feed-forward network."}
{"id": "2003.13028.cfe21b979a6c851bdafb2e414622f61e62b1d98c", "question": "What is the architecture of the encoder?", "answer": ["M blocks, each consisting of self-attention module and a two-layer feed-forward network.", "encoder block consists of a self-attention module and a two-layer feed-forward network"], "num_words": 3319, "gpt4_answer": "The encoder consists of M layer encoder blocks, including a self-attention module and a two-layer feed-forward network."}
{"id": "2003.13028.3e3d123960e40bcb1618e11999bd2031ccc1d155", "question": "What are the languages of the datasets?", "answer": ["Unanswerable"], "num_words": 3319, "gpt4_answer": "The languages of the datasets are not explicitly mentioned in the paper."}
{"id": "2003.13028.2e37eb2a2a9ad80391e57acb53616eab048ab640", "question": "What is the architecture of the saliency model?", "answer": ["M blocks, each consisting of a self-attention module and two-layer feed-forward network, combined with a  single-layer feed-forward network.", "basic saliency model consists of $M$-layer Transformer encoder blocks ($\\mathrm {Encoder}_\\mathrm {sal}$) and a single-layer feed-forward network"], "num_words": 3319, "gpt4_answer": "The basic saliency model consists of M-layer Transformer encoder blocks (Encoder_sal) and a single-layer feed-forward network."}
{"id": "1911.03977.54002c15493d4082d352a66fb9465d65bfe9ddca", "question": "What are special architectures this review focuses on that are related to multimodal fusion?", "answer": ["attention mechanism ... bilinear pooling"], "num_words": 9646, "gpt4_answer": "This review focuses on special architectures related to multimodal fusion, such as attention-based fusion and bilinear pooling-based fusion."}
{"id": "1910.01992.7caeb5ef6f2985b2cf383cd01765d247c936605f", "question": "What other model inference optimization schemes authors explore?", "answer": [" frame-skipping ... multi-threaded lazy computation", " shortcut connections ... batch normalization (BN) ... self-normalizing neural networks (SNNs)"], "num_words": 2309, "gpt4_answer": "The authors explore frame-skipping and multi-threaded lazy computation as additional model inference optimization schemes."}
{"id": "1910.01992.1fcd25e9a63a53451cac9ad2b8a1b529aff44a97", "question": "On what dataset is model trained/tested?", "answer": ["Siri internal datasets (en_US and zh_CN)"], "num_words": 2309, "gpt4_answer": "The models are trained and tested on Siri internal datasets (en_US and zh_CN)."}
{"id": "2003.03728.049415676f8323f4af16d349f36fbcaafd7367ae", "question": "By how much do they improve on domain classification?", "answer": ["F-1 score was improved by 1.19 percent points.", "F1 is improved from 80.15 to 80.50 and from 80.71 to 81.69 of Shortlister and Hipothesis Reranker models respectively."], "num_words": 2189, "gpt4_answer": "The proposed approach significantly improves the performance of domain classification with hypothesis reranking, but the exact improvement percentage is not provided in the paper."}
{"id": "2003.03728.fee498457774d9617068890ff29528e9fa05a2ac", "question": "Which dataset do they evaluate on?", "answer": ["10K random utterances from the user log data", "The dataset was created by extracting utterances from the user log data from an intelligent conversational system."], "num_words": 2189, "gpt4_answer": "They evaluate on a dataset of 10K random utterances extracted from the user log data of an intelligent conversational system."}
{"id": "2003.03728.c626637ed14dee3049b87171ddf326115e59d9ee", "question": "How does their approach work for domains with few overlapping utterances? ", "answer": ["Unanswerable"], "num_words": 2189, "gpt4_answer": "Unanswerable"}
{"id": "2003.03728.b160bfb341f24ae42a268aa18641237a4b3a6457", "question": "How do they decide by how much to decrease confidences of incorrectly predicted domains?", "answer": ["The confidence of the incorrectly predicted domain is decreased only when it is highest among all predictions.", "demote the confidences of the negative ground-truths only when they are the highest so that the influence of using the negative ground-truths is not overwhelming"], "num_words": 2189, "gpt4_answer": "The confidences of incorrectly predicted domains are decreased by utilizing a loss function that demotes the confidences of the negative ground-truth domains only when they are the highest."}
{"id": "1912.00342.c0120d339fcdb3833884622e532e7513d1b2c7dd", "question": "Is some baseline method trained on new dataset?", "answer": ["No", "Yes"], "num_words": 5595, "gpt4_answer": "Unanswerable"}
{"id": "1912.00342.f52c9744a371104eb2677c181a7004f7a77d9dd3", "question": "What potential applications are demonstrated?", "answer": ["a general AI that talks with human beings ... making up the paraphrase corpus ... supporting the semantic web search", "for a general AI that talks with human beings without making the users feel isolated ... making up the paraphrase corpus ... supporting the semantic web search"], "num_words": 5595, "gpt4_answer": "The potential applications demonstrated include corpus generation and mitigating class imbalance in the final dataset."}
{"id": "1912.00342.867b1bb1e6a38de525be7757d49928a132d0dbd8", "question": "What method is proposed to mitigate class imbalance in final dataset?", "answer": ["we annotate an existing corpus and then augment the dataset ", "we increased the dataset size by obtaining various types of sentences from intent arguments, specifically via human-aided sentence rewriting"], "num_words": 5595, "gpt4_answer": "The method proposed to mitigate class imbalance in the final dataset is to increase the dataset size by obtaining various types of sentences from intent arguments, specifically via human-aided sentence rewriting."}
{"id": "1909.01860.6167618e0c53964f3a706758bdf5e807bc5d7760", "question": "What are remaining challenges in VQA?", "answer": [" object level details, segmentation masks, and sentiment of the question", "develop better deep learning models ...  more challenging datasets for VQA"], "num_words": 2058, "gpt4_answer": "Remaining challenges in VQA include developing better deep learning models, creating more challenging datasets, incorporating object-level details, segmentation masks, deeper models, and considering the sentiment of the question."}
{"id": "1909.01860.78a0c25b83cdeaeaf0a4781f502105a514b2af0e", "question": "How quickly is this hybrid model trained?  ", "answer": ["Unanswerable"], "num_words": 2058, "gpt4_answer": "Unanswerable"}
{"id": "1909.01860.08202b800a946b8283c2684e23b51c0ec1e8b2ac", "question": "What are the new deep learning models discussed in the paper?  ", "answer": ["Stacked Attention Networks BIBREF11 ... Teney et al. Model BIBREF13 ... Neural-Symbolic VQA BIBREF23 ... Focal Visual Text Attention (FVTA) BIBREF24 ... Pythia v1.0 BIBREF27 ... Differential Networks BIBREF19:", "Vanilla VQA ... Stacked Attention Networks ... Teney et al. Model ... Neural-Symbolic VQA ... Focal Visual Text Attention (FVTA) ... Pythia v1.0 ... Differential Networks"], "num_words": 2058, "gpt4_answer": "The new deep learning models discussed in the paper include Vanilla VQA, Stacked Attention Networks, Teney et al. Model, Neural-Symbolic VQA, Focal Visual Text Attention (FVTA), Pythia v1.0, and Differential Networks."}
{"id": "1909.01860.00aea97f69290b496ed11eb45a201ad28d741460", "question": "What was the architecture of the 2017 Challenge Winner model?", "answer": ["R-CNN architecture", "Region-based CNN"], "num_words": 2058, "gpt4_answer": "The 2017 VQA Challenge winner model used object detection with R-CNN architecture."}
{"id": "1909.01860.4e1293592e41646a6f5f0cb00c75ee8de14eb668", "question": "What is an example of a common sense question?", "answer": ["Can you park here?\nIs something under the sink broken?\nDoes this man have children?", "How many giraffes are drinking water?"], "num_words": 2058, "gpt4_answer": "A common sense question in the context of Visual Question Answering might be \"What color is the car in the image?\""}
{"id": "1908.08788.15aeda407ae3912419fd89211cdb98989d9cde58", "question": "What pretrained language representations are used?", "answer": [" GloVe", "BERT BIBREF12"], "num_words": 2287, "gpt4_answer": "The paper utilizes BERT as the pretrained language representation."}
{"id": "1908.08788.c8b2fb9e0d5fb9014a25b88d559d93b6dceffbc0", "question": "How many instances are explored in the few-shot experiments?", "answer": ["Unanswerable"], "num_words": 2287, "gpt4_answer": "The few-shot experiments explore 5 instances as support set for each label in the test set."}
{"id": "1908.08788.c24f7c030010ad11e71ef4912fd79093503f3a8d", "question": "What tasks are explored?", "answer": ["69 tasks", "Unanswerable"], "num_words": 2287, "gpt4_answer": "The paper explores few-shot text classification tasks using the multi-domain sentiment classification dataset ARSC, which comprises English reviews for 23 types of products on Amazon."}
{"id": "1803.02155.1d7b99646a1bc05beec633d7a3beb083ad1e8734", "question": "How is the training time compared to the original position encoding? ", "answer": ["7% decrease in steps per second", "a modest 7% decrease in steps per second"], "num_words": 2132, "gpt4_answer": "The training time with relative position representations resulted in a modest 7% decrease in steps per second compared to the original position encoding."}
{"id": "1803.02155.4d887ce7dc43528098e7a3d9cd13c6c36f158c53", "question": "Does the new relative position encoder require more parameters?", "answer": ["No", "Yes"], "num_words": 2132, "gpt4_answer": "Yes, the new relative position encoder requires more parameters."}
{"id": "1803.02155.d48b5e4a7cf1f96c5b939ba9b46350887c5e5268", "question": "Can the new position representation be generalized to other tasks?", "answer": ["Not sure", "Unanswerable"], "num_words": 2132, "gpt4_answer": "Yes, the new position representation can be generalized to other tasks."}
{"id": "1803.06745.de344aeb089affebd15a8c370ae9ab5734e99203", "question": "which social media platforms was the data collected from?", "answer": ["Twitter"], "num_words": 2503, "gpt4_answer": "The data was collected from Twitter using the Twitter4j API."}
{"id": "1803.06745.84327a0a9321bf266e22d155dfa94828784595ce", "question": "how many data pairs were there for each dataset?", "answer": ["18461 for Hindi-English and 5538 for Bengali-English", "HI-EN dataset has total size of of 18461 while BN-EN has total size of 5538. "], "num_words": 2503, "gpt4_answer": "There were two code-mixed data pairs: Hindi-English (HI-EN) and Bengali-English (BN-EN)."}
{"id": "1803.06745.c2037887945abbdf959389dc839a86bc82594505", "question": "how many systems were there?", "answer": ["Six", "nine"], "num_words": 2503, "gpt4_answer": "There were nine unique submitted systems for evaluation."}
{"id": "1803.06745.e9a0a69eacd554141f56b60ab2d1912cc33f526a", "question": "what was the baseline?", "answer": [" randomly assigning any of the sentiment values to each of the test instances", "Random labeling"], "num_words": 2503, "gpt4_answer": "The baseline systems were developed by randomly assigning any of the sentiment values to each of the test instances."}
{"id": "1803.06745.5b2839bef513e5d441f0bb8352807f673f4b2070", "question": "what metrics did they use for evaluation?", "answer": ["The macro average precision, recall, and f-score", "precision, recall and f-score "], "num_words": 2503, "gpt4_answer": "The metrics used for evaluation were precision, recall, and macro average f-score."}
{"id": "1803.06745.2abf916bc03222d3b2a3d66851d87921ff35c0d2", "question": "what datasets did they use?", "answer": ["Bengali-English and Hindi-English", "HI-EN ... BN-EN"], "num_words": 2503, "gpt4_answer": "They used two code-mixed data pairs, Hindi-English (HI-EN) and Bengali-English (BN-EN), collected from Twitter using the Twitter4j API."}
{"id": "1911.01248.a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6", "question": "What is the Semantic Web?", "answer": ["Unanswerable", "aims to make information available on the Web easier to process for machines and humans ...  in RDF are expressed as so-called triples of the form (subject, predicate, object) ... OWL is equivalent to the description logic $\\mathcal {SROIQ}(D)$"], "num_words": 4792, "gpt4_answer": "The Semantic Web aims to make information available on the Web easier to process for machines and humans."}
{"id": "1909.06502.7de0b2df60d3161dd581ed7915837d460020bc11", "question": "How many tags are included in the ENE tag set?", "answer": ["141 ", "200", "200 fine-grained categories"], "num_words": 2077, "gpt4_answer": "The ENE tag set includes 200 fine-grained categories."}
{"id": "1909.06502.0a3a7e412682ce951329c37b06343d2114acad9d", "question": "Does the paper evaluate the dataset for smaller NE tag tests? ", "answer": ["No"], "num_words": 2077, "gpt4_answer": "The paper does not evaluate the dataset for smaller NE tag tests."}
{"id": "1709.05295.74cc0300e22f60232812019011a09df92bbec803", "question": "Do they report results only on English data?", "answer": ["Unanswerable", "Yes"], "num_words": 3574, "gpt4_answer": "Unanswerable"}
{"id": "1709.05295.865811dcf63a1dd3f22c62ec39ffbca4b182de31", "question": "What are the most discriminating patterns which are analyzed?", "answer": ["Patterns containing the preposition \"of\" make up the vast majority of prepositional phrases for both the fact and feel classes and  patterns with the preposition for are substantially more common in the feel class than the fact class.", "forms associated with the fact and feel"], "num_words": 3574, "gpt4_answer": "The most discriminating patterns analyzed are those that are highly correlated with factual and emotional argumentation styles in online debate forums."}
{"id": "1709.05295.9e378361b6462034aaf752adf04595ef56370b86", "question": "What bootstrapping methodology was used to find new patterns?", "answer": ["They embed AutoSlog-TS in a bootstrapping framework to learn additional linguistic expressions from the unannotated texts -  they give the labeled training data to AutoSlog-TS, which generates patterns and associated statistics and then identifies high-precision patterns that can be used to label some of the unannotated texts as factual or feeling.", "flow diagram for the bootstrapping system is shown in Figure FIGREF10"], "num_words": 3574, "gpt4_answer": "The bootstrapping methodology used is embedding AutoSlog-TS in a bootstrapping framework to learn additional linguistic expressions from unannotated texts."}
{"id": "1709.05295.667dce60255d8ab959869eaf8671312df8c0004b", "question": "What patterns were extracted which were correlated with emotional arguments?", "answer": ["Examples of extracted patters with high probability that contain of are: MARRIAGE FOR, STANDING FOR, SAME FOR, TREATMENT FOR, DEMAND FOR, ATTENTION FOR, ADVOCATE FOR, NO EVIDENCE FOR, JUSTIFICATION FOR, EXCUSE FOR", "Pattrn based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible or they involve assessment or evaluations of the arguments of the other speaker.  They are typically also very creative and diverse."], "num_words": 3574, "gpt4_answer": "Patterns associated with emotional arguments often include the speaker's own beliefs or claims, assessment or evaluations of the other speaker's arguments, and creative and diverse expressions. They typically involve adjectives and active voice verb phrases."}
{"id": "1709.05295.d5e716c1386b6485e63075e980f80d44564d0aa2", "question": "What patterns were extracted which were correlated with factual arguments?", "answer": [" patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases", "Examples of extracted patters with high probability that correlate with factual argument are: RESULT OF, ORIGIN OF, THEORY OF, EVIDENCE OF, PARTS OF, EVOLUTION OF, PERCENT OF, THOUSANDS OF, EXAMPLE OF, LAW OF"], "num_words": 3574, "gpt4_answer": "Patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. Some examples include prepositional phrases (especially NP Prep) and passive voice verb phrases (PassVP)."}
{"id": "1709.05295.1fd31fdfff93d65f36e93f6919f6976f5f172197", "question": "How were the factual and feeling forum posts annotated?", "answer": ["binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class.", "manually"], "num_words": 3574, "gpt4_answer": "The factual and feeling forum posts were annotated using the Internet Argument Corpus (IAC), where each quote-response pair was manually annotated with respect to whether the response is primarily a factual or feeling based argument."}
{"id": "1906.05685.d2d9c7177728987d9e8b0c44549bbe03c8c00ef2", "question": "What evaluation metrics did they use?", "answer": ["BLEU"], "num_words": 3293, "gpt4_answer": "The evaluation metric used in this paper is BLEU score."}
{"id": "1906.05685.6657ece018b1455035421b822ea2d7961557c645", "question": "What NMT techniques did they explore?", "answer": ["ConvS2S ... Transformer"], "num_words": 3293, "gpt4_answer": "The paper explored two neural machine translation (NMT) techniques: Convolutional Sequence-to-Sequence (ConvS2S) and Transformer architectures."}
{"id": "1906.05685.175cddfd0bcd77b7327b62f99e57d8ea93f8d8ba", "question": "What was their best performing model?", "answer": ["Transformer"], "num_words": 3293, "gpt4_answer": "The best performing model was the Transformer."}
{"id": "1906.05685.f0afc116809b70528226d37190e8e79e1e9cd11e", "question": "What datasets did they use?", "answer": ["Autshumato", "English to Afrikaans, isiZulu, N. Sotho,\nSetswana, and Xitsonga parallel corpora from the Autshumato project"], "num_words": 3293, "gpt4_answer": "The authors used the publicly-available Autshumato parallel corpora, which are aligned corpora of South African governmental data created for use in machine translation systems."}
{"id": "1811.00051.3588988f2230f3329d7523fbb881b20bf177280d", "question": "Which ontologies did they use?", "answer": ["We experimented with three owl ontologies: (1) the Wine Ontology, which provides information about wines, wine producers etc.; (2) the Consumer Electronics Ontology, intended to help exchange information about consumer electronics products; and (3) the Disease Ontology, which describes diseases, including their symptoms, causes etc. ", "Wine Ontology ... Consumer Electronics Ontology ... Disease Ontology"], "num_words": 18100, "gpt4_answer": "The authors used three ontologies from different knowledge domains: wines, consumer electronics, and diseases."}
{"id": "1901.05415.78f8dad0f1acf024f69b3218b2d204b8019bb0d2", "question": "how is user satisfaction estimated?", "answer": [" Workers chatted with our baseline dialogue agent and assigned a rating 1-5 for the quality of each of the agent's responses. Contexts with rating 1 were mapped to the negative class (dissatisfied) and ratings INLINEFORM0 mapped to the positive class (satisfied). ", "via crowdsourcing"], "num_words": 7721, "gpt4_answer": "User satisfaction is estimated by training the chatbot on the Satisfaction task, where it predicts whether or not a speaking partner is satisfied with the quality of the current conversation based on the context and the partner's response."}
{"id": "1901.05415.73a5783cad4ed468a8dbb31b5de2c618ce351ad1", "question": "by how much did performance improve?", "answer": [" an increase of up to 9.4 accuracy points, a 31% improvement.", " the trained classifier significantly outperforms both the uncertainty-based methods and our original regular expression, by as much as 0.28 and 0.42 F1 points, respectively"], "num_words": 7721, "gpt4_answer": "Performance improved by up to 9.4 accuracy points, a 31% improvement, when the Human-Human (HH) Dialogue training set was small."}
{"id": "1802.08969.1128a600a813116cba9a2cf99d8568ae340f327a", "question": "What datasets do they use in the experiment?", "answer": ["CoNLL 2000 chunking ... CoNLL 2003 English NER ... Wall Street Journal(WSJ) portion of Penn Treebank (PTB) ... 14 datasets are product reviews ... two sub-datasets about movie reviews", "Wall Street Journal(WSJ) portion of Penn Treebank (PTB)  ... CoNLL 2000 chunking ... CoNLL 2003 English NER  ... Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen ... IMDB The movie reviews with labels of subjective or objective ... MR The movie reviews with two classes"], "num_words": 3923, "gpt4_answer": "In the experiment, they use 16 classification datasets, including 14 Amazon product review datasets, IMDB, and MR. For sequence tagging tasks, they use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB), CoNLL 2000 chunking, and CoNLL 2003 English NER datasets."}
{"id": "1802.08969.d64fa192a7e9918c6a22d819abad581af0644c7d", "question": "What new tasks do they use to show the transferring ability of the shared meta-knowledge?", "answer": ["choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task", "we take turns choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task."], "num_words": 3923, "gpt4_answer": "The authors test the transferability of the learned Meta-LSTM on 16 classification datasets, taking turns choosing 15 tasks to train the model with multi-task learning and transferring the learned Meta-LSTM to the remaining one task."}
{"id": "1802.08969.788f70a39c87abf534f4a9ee519f6e5dbf2543c2", "question": "What kind of meta learning algorithm do they use?", "answer": ["a function-level sharing scheme for multi-task learning", "a function-level sharing scheme for multi-task learning, in which a shared meta-network is used to learn the meta-knowledge of semantic composition among the different tasks"], "num_words": 3923, "gpt4_answer": "The paper proposes a function-level sharing scheme for multi-task learning, using a shared meta-network to capture the meta-knowledge of semantic composition among different tasks and generate the parameters of task-specific semantic composition models."}
{"id": "1909.07158.3d1ad8a4aaa2653d0095bafba74738bd20795acf", "question": "what dataset were used?", "answer": ["HatEval ... YouToxic ... OffensiveTweets"], "num_words": 3413, "gpt4_answer": "Three datasets were used: HatEval, YouToxic, and OffensiveTweets."}
{"id": "1909.07158.ec54ae2f4811196fcaafa45e76130239e69995f9", "question": "what was the baseline?", "answer": [" logistic regression (LR) ... Support Vector Machines (SVM)", "logistic regression (LR) ... Support Vector Machines (SVM) ... LSTM network from the Keras library "], "num_words": 3413, "gpt4_answer": "The baseline classification models used were logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library."}
{"id": "1909.07158.5102dc911913e9ca0311253e44fd31c73eed0a57", "question": "what text embedding methods were used in their approach?", "answer": ["Word2Vec ... ELMo", "Word2Vec and ELMo embeddings."], "num_words": 3413, "gpt4_answer": "The text embedding methods used in their approach include TF-IDF, word2vec, ELMo, and Universal Sentence Encoder."}
{"id": "1707.02892.5752c8d333afc1e6c666b18d1477c8f669b7a602", "question": "Do they compare against state-of-the-art?", "answer": ["Yes"], "num_words": 3139, "gpt4_answer": "Yes, the authors compare their model against state-of-the-art models such as NBOW, PV, MT-RNN, MT-CNN, MT-DNN, and GRNN."}
{"id": "1707.02892.fcdafaea5b1c9edee305b81f6865efc8b8dc50d3", "question": "What are the benchmark datasets?", "answer": ["SST-1 ... SST-2 ... IMDB ... Multi-Domain Sentiment Dataset ... RN ... QC", "SST-1 BIBREF14 ... SST-2 ... IMDB BIBREF15 ... Multi-Domain Sentiment Dataset BIBREF16 ... RN BIBREF17 ... QC BIBREF18"], "num_words": 3139, "gpt4_answer": "The benchmark datasets include SST-1, SST-2, IMDB, Multi-Domain Sentiment Dataset (Books, DVDs, Electronics, and Kitchen), RN, and QC."}
{"id": "1707.02892.91d4fd5796c13005fe306bcd895caaed7fa77030", "question": "What tasks are the models trained on?", "answer": ["Sentiment classification, topics classification, question classification.", "different average lengths and class numbers ... Multi-Domain Product review datasets on different domains ... Multi-Objective Classification datasets with different objectives"], "num_words": 3139, "gpt4_answer": "The models are trained on five benchmark datasets for text classification, involving multi-cardinality tasks (movie review datasets), multi-domain tasks (product review datasets), and multi-objective tasks (classification datasets with different objectives, such as sentiment analysis, topic classification, and question type judgment)."}
{"id": "1707.02892.27d7a30e42921e77cfffafac5cb0d16ce5a7df99", "question": "What recurrent neural networks are explored?", "answer": ["LSTM", "LSTM with 4 types of recurrent neural layers."], "num_words": 3139, "gpt4_answer": "The paper explores Long Short-Term Memory Networks (LSTM) as the main recurrent neural network for their multi-task learning architecture."}
{"id": "2002.06851.7561bd3b8ba7829b3a01ff07f9f3e93a7b8869cc", "question": "What extractive models were trained on this dataset?", "answer": [" LEAD-$k$  ... TextRank ... LexRank  ... SumBasic  ... C_SKIP ", "LEAD-$k$ ... TextRank ... LexRank ... SumBasic ... C_SKIP"], "num_words": 2484, "gpt4_answer": "The extractive models trained on this dataset include LEAD-$k$, TextRank, LexRank, SumBasic, C_SKIP, and SemSentSum."}
{"id": "2002.06851.a3ba21341f0cb79d068d24de33b23c36fa646752", "question": "What abstractive models were trained?", "answer": ["Conv2Conv  ... Transformer  ...  TransformerLM", "Conv2Conv ... Transformer ... TransformerLM"], "num_words": 2484, "gpt4_answer": "The abstractive models trained include Conv2Conv, Transformer, and its language model variant, TransformerLM."}
{"id": "2002.06851.96295e1fe8713417d2b4632438a95d23831fbbdc", "question": "Do the reviews focus on a specific video game domain?", "answer": ["No", "Unanswerable"], "num_words": 2484, "gpt4_answer": "Yes, the reviews focus on the video game domain."}
{"id": "2002.06851.5bfbc9ca7fd41be9627f6ef587bb7e21c7983be0", "question": "What is the size of this dataset?", "answer": ["$265\\,000$ professional reviews for around $72\\,000$ games and $26\\,000$ Wikipedia gameplay sections", "14652"], "num_words": 2484, "gpt4_answer": "The size of the GameWikiSum dataset is 14,652 samples."}
{"id": "1909.11980.5181527e6a61a9a192db5f8064e56ec263c42661", "question": "What language(s) does the system answer questions in?", "answer": ["French"], "num_words": 1514, "gpt4_answer": "The system answers questions in French."}
{"id": "1909.11980.334aa5540c207768931a0fe78aa4981a895ba37c", "question": "What metrics are used for evaluation?", "answer": ["macro precision ... recall  ... F-1", "macro precision, recall and F-1 ... average precision, recall and F-1"], "num_words": 1514, "gpt4_answer": "The metrics used for evaluation are precision, recall, and F-1 score."}
{"id": "1909.11980.b8bbdc3987bb456739544426c6037c78ede01b77", "question": "Is the proposed system compared to existing systems?", "answer": ["No"], "num_words": 1514, "gpt4_answer": "The paper does not provide a direct comparison to existing systems."}
{"id": "1908.06259.fea9b4d136156f23a88e5c7841874a467f2ba86d", "question": "How do they determine that a decoder handles an easier task than the encoder?", "answer": ["adding more layers to the encoder achieves larger improvements than adding more layers to the decoder ... decoder converges faster than the encoder", "adding more layers to the encoder achieves larger improvements than adding more layers to the decoder ... the decoder converges faster than the encoder"], "num_words": 3215, "gpt4_answer": "The paper determines that a decoder handles an easier task than the encoder through two analyses: 1) adding more layers to the encoder achieves larger improvements than adding more layers to the decoder, and 2) the decoder converges faster than the encoder during training."}
{"id": "1908.06259.4e59808a7f73ac499b9838d3c0ce814196a02473", "question": "How do they measure conditional information strength?", "answer": ["by masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart", "distance between the dropping token and the current predicted token"], "num_words": 3215, "gpt4_answer": "The strength of conditional information is measured by dropping previous tokens at varying distances from the predicted token and observing the performance changes in the decoder."}
{"id": "1908.06259.7ef7a5867060f91eac8ad857c186e51b767c734b", "question": "How do they generate input noise for the encoder and decoder?", "answer": ["random dropping ... random noising ... random swapping"], "num_words": 3215, "gpt4_answer": "They generate input noise for the encoder and decoder using three typical operations: 1) random dropping: randomly drop input tokens with different drop rates; 2) random noising: randomly select tokens and replace their embeddings with random noise; 3) random swapping: randomly reverse the order of adjacent tokens."}
{"id": "1908.07721.0b10cfa61595b21bf3ff13b4df0fe1c17bbbf4e9", "question": "How do they perform the joint training?", "answer": ["They perform joint learning through shared parameters for NER and RC.", "They train a single model that integrates a BERT language model as a shared parameter layer on NER and RC tasks."], "num_words": 4335, "gpt4_answer": "They perform joint training by integrating the BERT language model as a shared parameter layer through the dynamic range attention mechanism and using a shared task representation encoder (STR-encoder) for both NER and RC tasks."}
{"id": "1908.07721.67104a5111bf8ea626532581f20b33b851b5abc1", "question": "How many parameters does their model have?", "answer": ["Unanswerable"], "num_words": 4335, "gpt4_answer": "The paper does not provide an exact number of parameters for their model."}
{"id": "1908.07721.1d40d177c5e410cef1142ec9a5fab9204db22ae1", "question": "What is the previous model that achieved state-of-the-art?", "answer": ["Joint Bi-LSTM", "RDCNN, Joint-Bi-LSTM"], "num_words": 4335, "gpt4_answer": "The previous state-of-the-art model is the Joint-Bi-LSTM model."}
{"id": "1908.06024.344238de7208902f7b3a46819cc6d83cc37448a0", "question": "Did the survey provide insight into features commonly found to be predictive of abusive content on online platforms?", "answer": ["Yes"], "num_words": 5674, "gpt4_answer": "The survey indicates that character n-grams are one of the most effective features for addressing obfuscation due to their robustness to spelling variations. Additionally, personal and community-based profiling features of users significantly enhance the state of the art."}
{"id": "1908.06024.56bbca3fe24c2e9384cc57f55f35f7f5ad5c5716", "question": "Is deep learning the state-of-the-art method in automated abuse detection", "answer": ["Unanswerable", "Yes"], "num_words": 5674, "gpt4_answer": "Yes, recent state-of-the-art approaches in automated abuse detection rely on deep learning methods, particularly word-level CNNs and RNNs."}
{"id": "1908.06024.4c40fa01f626def0b69d1cb7bf9181b574ff6382", "question": "What datasets were used in this work?", "answer": ["DATA-TWITTER-WH, DATA-TWITTER-W, DATA-TWITTER-DAVID, DATA-TWITTER-F,  DATA-WIKI-ATT,  DATA-WIKI-AGG,  DATA-WIKI-TOX,  DATA-FOX-NEWS,  DATA-GAZZETTA, DATA-FACEBOOK, Arabic News,  GermEval,  Ask.fm.", "DATA-TWITTER-WH, DATA-TWITTER-W, DATA-TWITTER-DAVID, DATA-TWITTER-F, DATA-WIKI-ATT, DATA-WIKI-AGG, DATA-WIKI-TOX, DATA-FOX-NEWS, DATA-GAZZETTA, DATA-FACEBOOK, Arabic News, GermEval, Ask.fun"], "num_words": 5674, "gpt4_answer": "This paper is a survey of methods used for automated abuse detection and does not use specific datasets for its own work. However, it reviews and discusses various datasets that have been used by researchers in the field, such as data-smokey, data-harass, data-yahoo, data-twitter, data-wiki, and data-gazzetta, among others."}
{"id": "1908.06024.71b29ab3ddcdd11dcc63b0bb55e75914c07a2217", "question": "How is abuse defined for the purposes of this research?", "answer": ["we define abuse as any expression that is meant to denigrate or offend a particular person or group."], "num_words": 5674, "gpt4_answer": "For the purposes of this research, abuse is defined as any expression that is meant to denigrate or offend a particular person or group."}
{"id": "1705.00571.22225ba18a6efe74b1315cc08405011d5431498e", "question": "Do they use external financial knowledge in their approach?", "answer": ["No", "Yes"], "num_words": 2117, "gpt4_answer": "Yes, they trained a word2vec model on a set of 189,206 financial articles containing 161,877,425 tokens, manually downloaded from Factiva."}
{"id": "1705.00571.bd3562d2b3c162e9d27404d56b77e15f707d8b0f", "question": "Which evaluation metrics do they use?", "answer": [" Metric 1 ... Metric 2 ... Metric 3", "weighted cosine similarity ... classification metric for sentences with one aspect"], "num_words": 2117, "gpt4_answer": "The paper uses three evaluation metrics: weighted cosine similarity (Metric 1), a classification metric (Metric 2), and a metric presented in BIBREF18 (Metric 3)."}
{"id": "1705.00571.9c529bd3f7565b2178a79aae01c98c90f9d372ad", "question": "Which finance specific word embedding model do they use?", "answer": ["a word2vec BIBREF10 word embedding model on a set of 189,206 financial articles containing 161,877,425 tokens", "word2vec"], "num_words": 2117, "gpt4_answer": "They used a word2vec model trained on a set of 189,206 financial articles containing 161,877,425 tokens, manually downloaded from Factiva."}
{"id": "2004.04498.cf82251a6a5a77e29627560eb7c05c3eddc20825", "question": "How does lattice rescoring improve inference?", "answer": ["By transducing initial hypotheses produced by the biased baseline system to create gender-inflected search spaces which can\nbe rescored by the adapted model", "initial hypotheses produced by the biased baseline system are transduced to create gender-inflected search spaces which can be rescored"], "num_words": 4882, "gpt4_answer": "Lattice rescoring improves inference by constraining the search space to gender-inflected versions of the biased translation, allowing debiased models to rescore these alternatives and maintain most of the WinoMT accuracy improvement without sacrificing general translation performance."}
{"id": "2004.04498.b1fe6a39b474933038b44b6d45e5ca32af7c3e36", "question": "What three languages are used in the translation experiments?", "answer": ["German ... Spanish ... Hebrew"], "num_words": 4882, "gpt4_answer": "The three languages used in the translation experiments are German (en-de), Spanish (en-es), and Hebrew (en-he)."}
{"id": "2004.04498.919681faa9731057b3fae5052b7da598abd3e04b", "question": "What metrics are used to measure bias reduction?", "answer": ["$\\mathbf {\\Delta G}$  difference in $F_1$ score between the set of sentences with masculine entities and the set with feminine entities ... $\\mathbf {\\Delta S}$  difference in accuracy between the set of sentences with pro-stereotypical (`pro') entities and those with anti-stereotypical (`anti') entities", "Accuracy ... $\\mathbf {\\Delta G}$ ... $\\mathbf {\\Delta S}$ ... BLEU"], "num_words": 4882, "gpt4_answer": "The metrics used to measure bias reduction are Accuracy, G (difference in F1 score between sentences with masculine and feminine entities), S (difference in accuracy between pro-stereotypical and anti-stereotypical entities), and M:F (ratio of hypotheses with male predictions to those with female predictions)."}
{"id": "2004.04498.2749fb1725a2c4bdba5848e2fc424a43e7c4be51", "question": "How is the set of trusted, gender-balanced examples selected?", "answer": [" create a tiny, handcrafted profession-based dataset", "They select professions from the list collected by BIBREF4 from US labour statistics and manually translate masculine and feminine examples"], "num_words": 4882, "gpt4_answer": "The trusted, gender-balanced examples are created by constructing a tiny, handcrafted profession-based dataset for transfer learning. Sentences follow the template: \"The [PROFESSION] finished [his|her] work.\" The dataset uses 194 professions, resulting in 388 gender-balanced sentences."}
{"id": "1602.03483.7239c02a0dcc0c3c9d9cddb5e895bcf9cfcefee6", "question": "Which data sources do they use?", "answer": ["SICK  ... STS 2014", "Toronto Books Corpus ... STS 2014 dataset BIBREF37 ... SICK dataset BIBREF36"], "num_words": 3537, "gpt4_answer": "The authors used the Toronto Books Corpus, the COCO dataset, En-Fr and En-De data from the 2015 Workshop on Statistical MT (WMT), and other structured resources like dictionary definitions for training and evaluating their models."}
{"id": "1602.03483.9dcc10a4a325d4c9cb3bb8134831ee470be47e93", "question": "Which tasks do they evaluate supervised systems on?", "answer": ["paraphrase identification (MSRP) ... movie review sentiment (MR) ... product reviews (CR) ... subjectivity classification (SUBJ) ... opinion polarity (MPQA) ...  question type classification (TREC) ", "paraphrase identification (MSRP) ... movie review sentiment (MR) ... product reviews (CR) ... subjectivity classification (SUBJ) ... opinion polarity (MPQA) ... question type classification (TREC)"], "num_words": 3537, "gpt4_answer": "They evaluate supervised systems on 6 sentence classification tasks: paraphrase identification (MSRP), movie review sentiment (MR), product reviews (CR), subjectivity classification (SUBJ), opinion polarity (MPQA), and question type classification (TREC)."}
{"id": "1602.03483.31236a876277c6e1c80891a3293c105a1b1be008", "question": "How do they evaluate domain portability?", "answer": ["Unanswerable", "We constrain our comparison to methods that do not require labelled data"], "num_words": 3537, "gpt4_answer": "The paper does not specifically evaluate domain portability."}
{"id": "1602.03483.19ebfba9aa5a9596b09a0cfb084ff8ebf24a3b91", "question": "Which unsupervised representation-learning objectives do they introduce?", "answer": ["FastSent and Sequential Denoising Autoencoders", "Sequential Denoising Autoencoders (SDAEs) and FastSent"], "num_words": 3537, "gpt4_answer": "They introduce Sequential Denoising Autoencoders (SDAEs) and FastSent, a sentence-level log-linear bag-of-words model."}
{"id": "1710.11334.2288f567d2f5cfbfc5097d8eddf9abd238ffbe25", "question": "Do they manage to consistenly outperform the best performing methods?", "answer": ["Unanswerable", "Yes"], "num_words": 2113, "gpt4_answer": "The paper's parser shows better performance than Baseline_2, which uses abundant lexical and syntax features."}
{"id": "1710.11334.caebea05935cae1f5d88749a2fc748e62976eab7", "question": "Do they try to use other models aside from Maximum Entropy?", "answer": ["No"], "num_words": 2113, "gpt4_answer": "Yes, they also report two baselines: Baseline1 (based on a probabilistic model) and Baseline2 (based on a Support Vector Machine model)."}
{"id": "1710.11334.e381f1811774806be109f9b05896a2a3c5e1ef43", "question": "What methods to they compare to?", "answer": [" Baseline_1, which applies the probability information ... Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model with numeric type feature from the hashcode of the textual type feature", "(1) Baseline_1, which applies the probability information ... (2) Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model"], "num_words": 2113, "gpt4_answer": "They compare their parser to two baselines: Baseline_1, which applies probability information, and Baseline_2, which uses Support Vector Machine with numeric type features from the hashcode of textual type features."}
{"id": "1710.11334.9eec16e560f9ccafd7ba6f1e0db742b330b42ba9", "question": "Which dataset to they train and evaluate on?", "answer": ["PDTB as training set, Section 22 as testing set", "Penn Discourse Treebank"], "num_words": 2113, "gpt4_answer": "They train and evaluate on the Penn Discourse Treebank (PDTB), using Section 02-21 as the training set and Section 22 as the testing set."}
{"id": "1710.11334.d788076c0d19781ff3f6525bd9c05b0ef0ecd0f1", "question": "Do they attempt to jointly learn connectives, arguments, senses and non-explicit identiifers end-to-end?", "answer": ["No"], "num_words": 2113, "gpt4_answer": "No, the parser connects the subtasks together in a pipeline, not in an end-to-end manner."}
{"id": "1908.11664.ec70c7c560e08cff2820bad93f5216bc0a469f5a", "question": "What settings did they experiment with?", "answer": ["in-domain ... out-of-domain ... cross-dataset", "in-domain, out-of-domain and cross-dataset"], "num_words": 3829, "gpt4_answer": "They experimented with three evaluation settings: in-domain, out-of-domain, and cross-dataset."}
{"id": "1908.11664.940a16e9db8be5b5f4e67d9c7622b3df99ac10a5", "question": "what domains are explored in this paper?", "answer": ["NYTimes ... WashingtonPost ... FoxNews ... TheGuardian ... NYDailyNews ... WSJ ... USAToday ... CNN ... Time ... Mashable", "Unanswerable"], "num_words": 3829, "gpt4_answer": "In this paper, the authors explore news publications as domains in the context of extractive text summarization."}
{"id": "1908.11664.0b1cc6c0de286eb724b1fd18dbc93e67ab89a236", "question": "what multi-domain dataset is repurposed?", "answer": ["MULTI-SUM", "dataset Newsroom BIBREF16"], "num_words": 3829, "gpt4_answer": "The multi-domain dataset repurposed is MULTI-SUM, a subset of the Newsroom dataset."}
{"id": "1908.11664.1c2d4dc1e842b962c6407d6436f3dc73dd44ce55", "question": "what four learning strategies are investigated?", "answer": ["Model@!START@$^{I}_{Base}$@!END@ ... $Model^{I}_{Base}$ with BERT BIBREF28 ... Model@!START@$^{III}_{Tag}$@!END@ ... Model@!START@$^{IV}_{Meta}$@!END@", "Model@!START@$^{I}_{Base}$@!END@ ... Model@!START@$^{II}_{BERT}$@!END@ ... Model@!START@$^{III}_{Tag}$@!END@ ... Model@!START@$^{IV}_{Meta}$@!END@"], "num_words": 3829, "gpt4_answer": "The four learning strategies investigated are: 1) Model$^{I}_{Base}$, 2) Model$^{II}_{BERT}$, 3) Model$^{III}_{Tag}$, and 4) Model$^{IV}_{Meta}$."}
{"id": "1912.07076.654306d26ca1d9e77f4cdbeb92b3802aa9961da1", "question": "By how much did the new model outperform multilingual BERT?", "answer": ["For POS,  improvements for cased BERT are 1.26 2.52  0.5 for TDT,  FTB and PUD datasets respectively.\nFor NER in-domain test set, improvement is  2.11 F1 and for NER out-of-domain test set, improvement is  5.32 F1.\nFor Dependency parsing, improvements are in range from 3.35 to 6.64 LAS for cased BERT.", "absolute improvements for FinBERT cased ranging between 0.4 and 1.7% points ... LAS results are 2.33.6% points above the previous state of the art ... absolute advantage for FinBERT models ranging from 3% points for 1K examples to just over 1% point for 100K examples"], "num_words": 5246, "gpt4_answer": "The custom Finnish BERT model (FinBERT) systematically outperformed the multilingual BERT model, but the exact improvement varies depending on the task. For example, in POS tagging, the improvement ranged between 0.4 and 1.7 percentage points, while in dependency parsing, FinBERT achieved LAS results 2.33.6 percentage points higher than the previous state of the art."}
{"id": "1912.07076.5a7d1ae6796e09299522ebda7bfcfad312d6d128", "question": "What previous proposed methods did they explore?", "answer": ["ELMo  ... ULMFit  ... BERT", "che2018towards ... lim2018sex ... FiNER-tagger BIBREF32 ... gungor2018 ... HIT-SCIR BIBREF22 ... BIBREF33"], "num_words": 5246, "gpt4_answer": "The authors explored methods such as part-of-speech (POS) tagging, named entity recognition (NER), dependency parsing, and text classification tasks using Finnish BERT and multilingual BERT models. They also implemented probing tasks to evaluate the models' ability to capture linguistic properties."}
{"id": "1912.07076.bd191d95806cee4cf80295e9ce1cd227aba100ab", "question": "What was the new Finnish model trained on?", "answer": ["Yle corpus ... STT corpus ... Suomi24 corpus (version 2017H2) ... luotolahti2015towards ... Common Crawl ... Finnish Wikipedia", "news, online discussion, and an internet crawl"], "num_words": 5246, "gpt4_answer": "The new Finnish BERT model (FinBERT) was trained on a large and varied unannotated corpus compiled from three primary sources: news, online discussion, and an internet crawl. The news sources included the Yle corpus and the STT corpus, the online discussion source was the Suomi24 corpus, and the internet crawl sources were from SpiderLing crawler, Common Crawl project, and Finnish Wikipedia."}
{"id": "1611.02378.a9cae57f494deb0245b40217d699e9a22db0ea6e", "question": "How many TV series are considered?", "answer": ["3", "Three tv series are considered."], "num_words": 3902, "gpt4_answer": "Three TV series are considered."}
{"id": "1611.02378.0a736e0e3305a50d771dfc059c7d94b8bd27032e", "question": "How long is the dataset?", "answer": ["Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series.", "Unanswerable"], "num_words": 3902, "gpt4_answer": "The dataset contains 5000 reviews for each TV series."}
{"id": "1611.02378.283d358606341c399e369f2ba7952cd955326f73", "question": "Is manual annotation performed?", "answer": ["Yes"], "num_words": 3902, "gpt4_answer": "Yes, manual annotation was performed to label the collected TV series' reviews into generic categories."}
{"id": "1611.02378.818c85ee26f10622c42ae7bcd0dfbdf84df3a5e0", "question": "What are the eight predefined categories?", "answer": ["Eight categories are: Plot of the TV series, Actor/actress actors, Role, Dialogue discussion, Analysis, Platform, Thumb up or down and Noise or others.", "Plot of the TV series, Actor/actress, Role, Dialogue, Analysis, Platform, Thumb up or down, Noise or others"], "num_words": 3902, "gpt4_answer": "The eight predefined categories are: Role, Actor/Actress, Plot, Analysis, Production, Thumb Up or Down, Follow the Post, and Others."}
{"id": "1905.08392.37b0ee4a9d0df3ae3493e3b9114c3f385746da5c", "question": "Do they report results only on English data?", "answer": ["Unanswerable"], "num_words": 1145, "gpt4_answer": "Unanswerable"}
{"id": "1905.08392.bba70f3cf4ca1e0bb8c4821e3339c655cdf515d6", "question": "When the authors say their method largely outperforms the baseline, does this mean that the baseline performed better in some cases? If so, which ones?", "answer": ["Baseline performed better in \"Fascinating\" and \"Jaw-dropping\" categories.", "Weninger et al. (SVM) model outperforms on the Fascinating category."], "num_words": 1145, "gpt4_answer": "The authors do not mention any specific cases where the baseline performed better. They state that their neural network models largely outperform the classical methods in general."}
{"id": "1905.08392.c5f9894397b1a0bf6479f5fd9ee7ef3e38cfd607", "question": "What baseline method was used?", "answer": ["LinearSVM, LASSO, Weninger at al. (SVM)", "LinearSVM, LASSO, Weninger et al."], "num_words": 1145, "gpt4_answer": "The paper used a baseline of classical machine learning techniques with hand-engineered features."}
{"id": "1905.08392.9f8c0e02a7a8e9ee69f4c1757817cde85c7944bd", "question": "What was the motivation for using a dependency tree based recursive architecture?", "answer": ["It performs better than other models predicting TED talk ratings.", "Unanswerable"], "num_words": 1145, "gpt4_answer": "The motivation for using a dependency tree-based recursive architecture was to represent the input sentences in the form of a dependency tree, which resulted in slightly higher performance (average F-score 0.77) than the word sequence model (average F-score 0.76) in predicting TED talk ratings."}
{"id": "1905.08392.6cbbedb34da50286f44a0f3f6312346e876e2be5", "question": "How was a causal diagram used to carefully remove this bias?", "answer": ["By confining to transcripts only and normalizing ratings to remove the effects of speaker's reputations, popularity gained by publicity, contemporary hot topics, etc.", "Unanswerable"], "num_words": 1145, "gpt4_answer": "A causal diagram was used to analyze and remove the effects of data bias (e.g., speakers' reputations, popularity gained by publicity, etc.) in the prediction model."}
{"id": "1905.08392.173060673cb15910cc310058bbb9750614abda52", "question": "How does publicity bias the dataset?", "answer": ["Unanswerable"], "num_words": 1145, "gpt4_answer": "Publicity bias can affect the dataset by introducing unwanted variables such as the speakers' reputations, contemporary hot topics, and popularity gained by publicity, which can influence the ratings of the talks."}
{"id": "1905.08392.98c8ed9019e43839ffb53a714bc37fbb1c28fe2c", "question": "How do the speakers' reputations bias the dataset?", "answer": ["Unanswerable"], "num_words": 1145, "gpt4_answer": "The speakers' reputations bias the dataset by potentially influencing the ratings due to their popularity, rather than the content of their speech."}
{"id": "1911.11161.50c441a9cc7345a0fa408d1ce2e13f194c1e82a8", "question": "What is the state-of-the-art approach?", "answer": ["For particular Empathetic-Dialogues corpus released Raskin et al. is state of the art (as well as the baseline) approach. Two terms are used interchangeably in the paper.", "Rashkin et al. BIBREF3 "], "num_words": 1888, "gpt4_answer": "The state-of-the-art approach uses large pre-trained language models like BERT, GPT-2, and XL-Net for transfer learning to fine-tune and condition these models on a given emotion, producing more emotionally relevant responses for a given situation."}
{"id": "1710.07695.2895a3fc63f6f403445c11043460584e949fb16c", "question": "what do they mean by description length?", "answer": ["Minimum description length (MDL) as the basic framework to reconcile the two contradicting objectives: generality and specificity.", "the code length of phrases."], "num_words": 3435, "gpt4_answer": "Description length refers to the code length used to represent or describe data in the context of the minimum description length (MDL) principle, which is a framework for model selection and data compression."}
{"id": "1710.07695.1e7e3f0f760cd628f698b73d82c0f946707855ca", "question": "do they focus on english verbs?", "answer": ["Yes"], "num_words": 3435, "gpt4_answer": "Yes, they focus on English verbs."}
{"id": "1710.07695.64632981279c7aa16ffc1a44ffc31f4520f5559e", "question": "what evaluation metrics are used?", "answer": ["INLINEFORM0  ... INLINEFORM1 ", "coverage and precision"], "num_words": 3435, "gpt4_answer": "The evaluation metrics used are coverage (how much of the verb phrases in natural language the solution can find corresponding patterns) and precision (how much of the phrases and their corresponding patterns are correctly matched)."}
{"id": "1605.05195.deed225dfa94120fafcc522d4bfd9ea57085ef8d", "question": "Do the authors mention any possible confounds in this study?", "answer": ["No", "Yes"], "num_words": 4061, "gpt4_answer": "Yes, the authors mention the possible confound of using obscure emoticons and emojis, which might introduce selection biases in the dataset."}
{"id": "1605.05195.3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd", "question": "Do they report results only on English data?", "answer": ["Yes"], "num_words": 4061, "gpt4_answer": "Yes, they report results only on English data."}
{"id": "1605.05195.9aabcba3d44ee7d0bbf6a2c019ab9e0f02fab244", "question": "Are there any other standard linguistic features used, other than ngrams?", "answer": ["No"], "num_words": 4061, "gpt4_answer": "The paper mainly focuses on n-grams as the standard linguistic features for sentiment classification."}
{"id": "1605.05195.242c626e89bca648b65af135caaa7ceae74e9720", "question": "What is the relationship between author and emotional valence?", "answer": ["Among those who wrote more than 50 tweets, 16% of the authors have average sentiment within [0.95, 1.00], while only 1.5% of the authors have average sentiment within [-1.00, -0.95]\n", "people have different baseline emotional valences from one another"], "num_words": 4061, "gpt4_answer": "People have different baseline attitudes, some are optimistic and positive, some are pessimistic and negative, and some are in between. This difference in personalities can manifest itself in the sentiment of tweets."}
{"id": "1605.05195.bba677d1a1fe38a41f61274648b386bdb44f1851", "question": "What is the relationship between time and emotional valence?", "answer": ["The closer the day of the week to Friday and Saturday, the more positive the sentiment; tweets made between 10 a.m. 12 noon are most positive, while those made around 3 a.m. and 20 p.m. are least positive; tweets made in April and May are most positive, while those made in August and September are least positive.", "people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays"], "num_words": 4061, "gpt4_answer": "The relationship between time and emotional valence is that people generally have different sentiments at different times, such as being happier on weekends and certain hours of the day."}
{"id": "1605.05195.b6c2a391c4a94eaa768150f151040bb67872c0bf", "question": "What is the relationship between location and emotional valence?", "answer": ["happier in certain states in the United States", "ranking of US states based on their Twitter sentiment correlates with the ranking of US states based on the well-being index"], "num_words": 4061, "gpt4_answer": "The relationship between location and emotional valence is that different locations have different average sentiments, which can affect the emotional valence expressed in tweets from those locations."}
{"id": "1604.05559.06d5de706348dbe8c29bfacb68ce65a2c55d0391", "question": "What is the computational complexity of old method", "answer": ["O(2**N)", "Unanswerable"], "num_words": 700, "gpt4_answer": "The computational complexity of the old method is not explicitly mentioned in the paper."}
{"id": "1604.05559.6014c2219d29bae17279625716e7c2a1f8a2bd05", "question": "Could you tell me more about the old method?", "answer": ["$$freq(*, word) = freq(word, *) = freq(word)$$ (Eq. 1)", "freq(*, word) = freq(word, *) = freq(word)"], "num_words": 700, "gpt4_answer": "The old method calculates bigram frequencies using the approximation $$freq(*, word) = freq(word, *) = freq(word)$$ (Eq. 1), which is efficient but may cause errors for words occurring near the beginning or end of the text. The statistical importance of these miscalculations diminishes as the text grows larger."}
{"id": "2002.03056.9be9354eeb2bb1827eeb1e23a20cfdca59fb349a", "question": "How this system recommend features for the new application?", "answer": ["Based upon these proximity estimates as well as expected relevance of features for existing applications, system would recommend features for the new application in a ranked order", "estimating its semantic proximity (from the perspective of a NLP data scientist) with existing applications with known features ... system would recommend features for the new application in a ranked order"], "num_words": 3533, "gpt4_answer": "The system recommends features for a new application by estimating its semantic proximity to existing applications with known features, and based on these proximity estimates and the expected relevance of features for existing applications, it recommends features for the new application in a ranked order."}
{"id": "2002.03056.5d5c25d68988fa5effe546507c66997785070573", "question": "What is the similarity of manually selected features across related applications in different domains?", "answer": ["Applications share similar sets of features (of the 7 set of features, 6 selected are the same)", "Examples of common features are: N-gram, POS, Context based Features, Morphological Features, Orthographic, Dependency and Lexical"], "num_words": 3533, "gpt4_answer": "The similarity of manually selected features across related applications in different domains is significant, as demonstrated by the example in the paper, where three different types of events in different domains shared similar sets of features despite domain differences."}
{"id": "2002.03056.ca595151735444b5b30a003ee7f3a7eb36917208", "question": "What type of features are extracted with this language?", "answer": ["Linguistic Features ... Semantic Similarity and Relatedness based Features ... Statistical Features", "Linguistic, Semantic, and Statistical."], "num_words": 3533, "gpt4_answer": "The language extracts different classes of features, including linguistic, semantic, and statistical for various levels of analysis, such as words, phrases, sentences, paragraphs, documents, and corpus."}
{"id": "2002.03056.a2edd0454026811223b8f31512bdae91159677be", "question": "What are meta elements of language for specifying NLP features?", "answer": ["Analysis Unit (AU) (Corpus level, Document level, Para (paragraph) level,  Sentence level);\nSyntactic Unit (SU) (Word, Phrase, N-gram, Regex, POS Regex,);\nLOGICAL (AND, OR, AND NOT,  OR NOT);\nNormalize Morphosyntactic Variants (yes or no).", "Analysis Unit (AU) ... Syntactic Unit (SU) ... LOGICAL ... Normalize Morphosyntactic Variants"], "num_words": 3533, "gpt4_answer": "The meta elements of the NLP Feature Specification Language (nlpFSpL) include Analysis Unit (AU), Syntactic Unit (SU), and Normalize Morphosyntactic Variants."}
{"id": "1904.02306.3b4077776f4e828f0d1687d0ce8018c9bce4fdc6", "question": "what previous work do they also look at?", "answer": ["N18-1126 ... UDPipe ... D15-1272 ... Morfette", "N18-1126 ... UDPipe system of K17-3009 ... D15-1272 ... Morfette"], "num_words": 3859, "gpt4_answer": "The authors compare their approach against recent competing methods that report results on UD datasets, including the state-of-the-art system by N18-1126, the UDPipe system of K17-3009, the system of D15-1272, and Morfette."}
{"id": "1904.02306.d1a88fe6655c742421da93cf88b5c541c09866d6", "question": "what languages did they experiment with?", "answer": ["Arabic, Basque, Croatian, Dutch, Estonian, Finnish, German, Greek, Hindi, Hungarian, Italian, Latvian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Turkish, Urdu", "They experiment with: arabic, basque, croatian, dutch, estonian, finnish, german, greek, hindi, hungarian, italian, latvian, polish, portuguese, romanian, russian, slovak, slovenian, turkish and urdu."], "num_words": 3859, "gpt4_answer": "The paper experimented with 20 languages from the Universal Dependencies corpora."}
{"id": "2003.00864.184382af8f58031c6e357dbee32c90ec95288cb3", "question": "What are state of the art results on OSA and PD corpora used for testing?", "answer": ["PD : i-vectors had segment level F1 score 66.6 and for speaker level had 75.6 F1 score\n\nOSA: For the same levels it had F1 scores of 65.5 and 75.0", "State of the art F1 scores are:\nPPD: Seg 66.7, Spk 75.6\nOSA: Seg 73.3, Spk 81.7\nSPD: Seg 79.0, Spk 87.0"], "num_words": 2819, "gpt4_answer": "Unanswerable"}
{"id": "2003.00864.97abc2e7b39869f660986b91fc68be4ba196805c", "question": "How better does x-vectors perform than knowlege-based features in same-language corpora?", "answer": ["For OSA detection x-vectors outperform all other approaches at the segment level, most importantly they significantly outperform KB features by 8%.", "For Portuguese PD corpus, x-vector outperform KB for segment and speaker level  for 2.2 and 2.2 F1 respectively.\nFor Portuguese OSA corpus, x-vector outperform KB for segment and speaker level  for 8.5 and 0.1 F1 respectively.", "Portuguese PD Corpus: for segment level i-vectors had better F1 score comparing to KB by 2.1% and for speaker level by 3.5%\nIn case of Spanish PD corpus, KB had higher F1 scores in terms of Segment level and Speaker level by 3.3% and 2.0%.  "], "num_words": 2819, "gpt4_answer": "X-vectors perform better than knowledge-based features in same-language corpora, with an improvement of around 8% for the obstructive sleep apnea detection task at the segment level."}
{"id": "2003.00864.9ec0527bda2c302f4e82949cc0ae7f7769b7bfb8", "question": "What is meant by domain missmatch occuring?", "answer": ["tasks whose domain does not match that of the training data"], "num_words": 2819, "gpt4_answer": "Domain mismatch occurs when the data used for training and testing are from different domains, such as different languages, tasks, or recording conditions."}
{"id": "2003.00864.330fe3815f74037a9be93a4c16610c736a2a27b3", "question": "How big are OSA and PD corporas used for testing?", "answer": ["15 percent of the corpora is used for testing. OSA contains 60 speakers, 3495 segments and PD 140 speakers and 3365 segments.", "For Portuguese PD have  for patient 1.24h and for control 1.07 h.\nFor Portuguese OSA have  for patient 1.10h and for control 1.05 h.\nFor Spanish PD have for patient 0.49h  and for control 0.50h."], "num_words": 2819, "gpt4_answer": "The OSA and PD corpora used for testing are limited in size, with fewer than 3 hours each."}
{"id": "1605.04278.7546125f43eec5b09a3368c95019cb2bf1478255", "question": "How do they think this treebank will support research on second language acquisition?", "answer": ["It will improve tagging and parsing performance, syntax based grammatical error correction.", "Unanswerable"], "num_words": 4603, "gpt4_answer": "The treebank will support research on second language acquisition by providing a large scale dataset of learner language, manually annotated with POS tags and universal dependencies, which can be used for empirical study of learner syntax in NLP, corpus linguistics, and second language acquisition."}
{"id": "1605.04278.e96b0d64c8d9fdd90235c499bf1ec562d2cbb8b2", "question": "What are their baseline models?", "answer": ["Turbo tagger ... Turbo parser", "version 2.2 of the Turbo tagger and Turbo parser BIBREF18"], "num_words": 4603, "gpt4_answer": "The baseline models used in the paper are version 2.2 of the Turbo tagger and Turbo parser, which are state-of-the-art tools for statistical POS tagging and dependency parsing."}
{"id": "1605.04278.576a3ed6e4faa4c3893db632e97a52ac6e864aac", "question": "How long is the dataset?", "answer": [" 5,124 sentences (97,681 tokens)", "5124"], "num_words": 4603, "gpt4_answer": "The TLE dataset contains 5,124 sentences."}
{"id": "1605.04278.73c535a7b46f0c2408ea2b1da0a878b376a2bca5", "question": "Did they use crowdsourcing to annotate the dataset?", "answer": ["No"], "num_words": 4603, "gpt4_answer": "No, they used six students (five undergraduates and one graduate) for annotation and did not utilize crowdsourcing."}
{"id": "1908.09590.620b6c410a055295d137511d3c99207a47c03b5e", "question": "How significant are the improvements over previous approaches?", "answer": ["Increase of 2.4%, 1.3%, and 1.6% accuracy on IMDB, Yelp 2013, and Yelp 2014", "with performance increases of 2.4%, 1.3%, and 1.6% on IMDB, Yelp 2013, and Yelp 2014, respectively"], "num_words": 4362, "gpt4_answer": "The proposed method (CHIM-embedding) achieves performance increases of 2.4%, 1.3%, and 1.6% on IMDB, Yelp 2013, and Yelp 2014 datasets, respectively, compared to previous state-of-the-art models."}
{"id": "1908.09590.e459760879f662b2205cbdc0f5396dbfe41323ae", "question": "Which other tasks are evaluated?", "answer": ["Product Category Classification ... Review Headline Generation", "product category classification and review headline generation"], "num_words": 4362, "gpt4_answer": "Other tasks evaluated are product category classification and review headline generation."}
{"id": "1908.09590.1c3a20dceec2a86fb61e70fab97a9fb549b5c54c", "question": "What are the performances associated to different attribute placing?", "answer": ["Best accuracy is for proposed CHIM methods (~56% IMDB, ~68.5 YELP datasets), most common bias attention (~53%IMDB, ~65%YELP), and oll others are worse than proposed method.", "Sentiment classification (datasets IMDB, Yelp 2013, Yelp 2014): \nembedding 56.4% accuracy, 1.161 RMSE, 67.8% accuracy, 0.646 RMSE, 69.2% accuracy, 0.629 RMSE;\nencoder 55.9% accuracy, 1.234 RMSE, 67.0% accuracy, 0.659 RMSE, 68.4% accuracy, 0.631 RMSE;\nattention 54.4% accuracy, 1.219 RMSE, 66.5% accuracy, 0.664 RMSE, 68.5% accuracy, 0.634 RMSE;\nclassifier 55.5% accuracy, 1.219 RMSE, 67.5% accuracy, 0.641 RMSE, 68.9% accuracy, 0.622 RMSE.\n\nProduct category classification and review headline generation:\nembedding 62.26  0.22% accuracy, 42.71 perplexity;\nencoder 64.62  0.34% accuracy, 42.65 perplexity;\nattention 60.95  0.15% accuracy, 42.78 perplexity;\nclassifier 61.83  0.43% accuracy, 42.69 perplexity."], "num_words": 4362, "gpt4_answer": "The performances associated with different attribute placing vary depending on the task and settings. For sentiment classification, CHIM-embedding achieves the best accuracy, while CHIM-classifier achieves the best RMSE. For attribute transfer tasks, CHIM-encoder produces the most transferable attribute encoding for both product category classification and review headline generation."}
{"id": "1906.08584.9686f3ff011bc6e3913c329c6a5671932c27e63e", "question": "What architecture is used in the encoder?", "answer": ["Transformer", "Unanswerable"], "num_words": 2081, "gpt4_answer": "The encoder architecture is modified from the Transformer architecture to afford a fixed-size representation for the encoder output."}
{"id": "1806.09652.1f053f338df6d238cb163af1a0b1b073e749ed8a", "question": "Do they evaluate their parallel sentence generation?", "answer": ["Yes"], "num_words": 2278, "gpt4_answer": "Yes, they perform a qualitative analysis of the extracted parallel sentences, as there was no gold standard evaluation set for sentences extracted from Wikipedia."}
{"id": "1806.09652.fb06ed5cf9f04ff2039298af33384ca71ddbb461", "question": "How much data do they manage to gather online?", "answer": ["INLINEFORM0 bilingual English-Tamil ... INLINEFORM1 English-Hindi titles", "INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia"], "num_words": 2278, "gpt4_answer": "The paper does not provide an exact number of gathered data online."}
{"id": "1806.09652.754d7475b8bf50499ed77328b4b0eeedf9cb2623", "question": "Which models do they use for phrase-based SMT?", "answer": ["Moses BIBREF14", "Phrase-Based SMT systems were trained using Moses, grow-diag-final-and heuristic were used for extracting phrases,  and lexicalised reordering and Batch MIRA for tuning."], "num_words": 2278, "gpt4_answer": "The authors used Moses for training Phrase-Based SMT systems."}
{"id": "1806.09652.1d10e069b4304fabfbed69acf409f0a311bdc441", "question": "Which models do they use for NMT?", "answer": [" TensorFlow BIBREF17 implementation of OpenNMT", "OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19"], "num_words": 2278, "gpt4_answer": "They use the TensorFlow implementation of OpenNMT with attention-based transformer architecture for NMT."}
{"id": "1806.09652.718c0232b1f15ddb73d40c3afbd6c5c0d0354566", "question": "What are the BLEU performance improvements they achieve?", "answer": [" 11.03% and 14.7% for enta and enhi pairs respectively", "11.03% and 14.7% for enta and enhi pairs respectively"], "num_words": 2278, "gpt4_answer": "The model achieved a percentage increase in BLEU scores of 11.03% for English-Tamil and 14.7% for English-Hindi language pairs due to the use of parallel-sentence pairs extracted from comparable corpora using the neural architecture."}
{"id": "1703.04357.7cf44877dae8873139aede381fb9908dd0c546c4", "question": "What is the architecture of the model?", "answer": ["attentional encoderdecoder"], "num_words": 675, "gpt4_answer": "Nematus implements an attentional encoderdecoder architecture similar to the one described by DBLP:journals/corr/BahdanauCB14, but with several implementation differences."}
{"id": "1703.04357.86de8de906e30bb2224a2f70f6e5cf5e5ad4be72", "question": "How many translation pairs are used for training?", "answer": ["Unanswerable"], "num_words": 675, "gpt4_answer": "Unanswerable"}
{"id": "1909.00369.361f330d3232681f1a13c6d59abb6c18246e7b35", "question": "Do they use multitask learning?", "answer": ["Yes"], "num_words": 4114, "gpt4_answer": "Yes, they propose a unified model to jointly predict and translate ZPs by leveraging multi-task learning."}
{"id": "1909.00369.f7d61648ae4bd46c603a271185c3adfac5fc5114", "question": "Is Chinese a pro-drop language?", "answer": ["Yes"], "num_words": 4114, "gpt4_answer": "Yes, Chinese is a pro-drop language."}
{"id": "1909.00369.c9a323c152c5d9bc2d244e0ed10afbdb0f93062a", "question": "Is English a pro-drop language?", "answer": ["No"], "num_words": 4114, "gpt4_answer": "No, English is not a pro-drop language."}
{"id": "1803.02914.d6a815d24c46557827d8aca65d3ffd008ac1bc07", "question": "Which movie subtitles dataset did they use?", "answer": ["OpenSubtitles dataset BIBREF18", "the OpenSubtitles dataset BIBREF18"], "num_words": 2491, "gpt4_answer": "They used the OpenSubtitles dataset."}
{"id": "2002.00175.23252644c04a043f630a855b563666dd57179d98", "question": "What are the other two Vietnamese datasets?", "answer": ["MS-COCO dataset translated to Vietnamese using Google Translate and through human annotation", "datasets generated by two methods (translated by Google Translation service and annotated by human)"], "num_words": 3064, "gpt4_answer": "Unanswerable"}
{"id": "2002.00175.2f75b0498cf6a1fc35f1fb1cac44fc2fbd3d7878", "question": "Which English dataset do they evaluate on?", "answer": ["MS-COCO", "the original MS-COCO English dataset"], "num_words": 3064, "gpt4_answer": "They evaluate on the original MS-COCO English dataset with 3,850 sportball images."}
{"id": "2002.00175.0d3193d17c0a4edc8fa9854f279c2a1b878e8b29", "question": "What neural network models do they use in their evaluation?", "answer": ["CNN  ... RNN - LSTM", "Neural Image Captioning (NIC) model BIBREF14 ... Image Captioning model from the Pytorch-tutorial BIBREF15 by Yunjey"], "num_words": 3064, "gpt4_answer": "They use Neural Image Captioning (NIC) model and Image Captioning model from the Pytorch-tutorial by Yunjey in their evaluation."}
{"id": "2002.00175.b424ad7f9214076b963a0077d7345d7bb5a7a205", "question": "Do they use crowdsourcing for the captions?", "answer": ["Yes"], "num_words": 3064, "gpt4_answer": "No, they use five native Vietnamese writers aged 22-25 to manually write the captions for the UIT-ViIC dataset."}
{"id": "2002.00175.0dfe43985dea45d93ae2504cccca15ae1e207ccf", "question": "What methods are used to build two other Viatnamese datsets?", "answer": ["Translation and annotation.", "human translation and Google Translation service"], "num_words": 3064, "gpt4_answer": "There are two Vietnamese datasets: GT-sportball, which is translated from the MS-COCO English dataset using Google Translation API, and UIT-ViIC, which is built with manually written captions for images from MS-COCO."}
{"id": "2002.00175.8276671a4d4d1fbc097cd4a4b7f5e7fadd7b9833", "question": "What deep neural network models are used in evaluation?", "answer": ["CNN ... RNN - LSTM", "encoder-decoder architecture of CNN for encoding and LSTM for decoding"], "num_words": 3064, "gpt4_answer": "The deep neural network models used in evaluation are Neural Image Captioning (NIC) model and Image Captioning model from the Pytorch-tutorial by Yunjey."}
{"id": "2002.00175.79885526713cc16eb734c88ff1169ae802cad589", "question": "How authors evaluate datasets using models trained on different datasets?", "answer": [" The two models are trained with three mentioned datasets, then validated on  subset for each dataset and evaluated using BLEU, ROUGE and CIDEr measures.", "They evaluate on three metrics BLUE, ROUGE and CIDEr trained on the mentioned datasets."], "num_words": 3064, "gpt4_answer": "The authors evaluate datasets using BLEU, ROUGE, and CIDEr metrics on two published Image Captioning models (Pytorch-tutorial and NIC - Show and Tell) trained on different datasets, including English-sportball, GT-sportball, and UIT-ViIC."}
{"id": "1901.09381.0871827cfeceed4ee78ce7407aaf6e85dd1f9c25", "question": "Do they evaluate their model on datasets other than RACE?", "answer": ["No", "Yes, they also evaluate on the ROCStories\n(Spring 2016) dataset which collects 50k five sentence commonsense stories. "], "num_words": 1657, "gpt4_answer": "Unanswerable"}
{"id": "1901.09381.240058371e91c6b9509c0398cbe900855b46c328", "question": "What is their model's performance on RACE?", "answer": ["67% using BERT_base, 74.1% using BERT_large, 75.8% using BERT_large, Passage, and Answer, and 82.8% using XLNET_large with Passage and Answer features", "Model's performance ranges from 67.0% to 82.8%."], "num_words": 1657, "gpt4_answer": "Our model achieves the new state-of-the-art in RACE dataset, outperforming the previous state-of-the-art model by 2.2% in RACE full dataset."}
{"id": "1705.10272.c7d3bccee59ab683e6bf047579bc6eab9de9d973", "question": "What deep learning models do they plan to use?", "answer": ["CNNs in combination with LSTMs  ... TreeStructured LSTMs", "CNNs in combination with LSTMs ... create word embeddings from domain specific materials ... TreeStructured LSTMs"], "num_words": 1134, "gpt4_answer": "The authors plan to use character-based Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short-Term Memory networks (LSTMs)."}
{"id": "1705.10272.376c6c74f008bb79a0dd9f073ac7de38870e80ad", "question": "What baseline, if any, is used?", "answer": ["No", "Unanswerable"], "num_words": 1134, "gpt4_answer": "The baseline used in this paper is the Ngram language models trained on both funny tweets and news data."}
{"id": "1705.10272.c59d67930edd3d369bd51a619849facdd0770644", "question": "How are the language models used to make predictions on humorous statements?", "answer": ["We scored tweets by assigning them a probability based on each model", "scored tweets by assigning them a probability based on each model ... higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data"], "num_words": 1134, "gpt4_answer": "The language models are used to make predictions on humorous statements by assigning probabilities to the input statements based on their similarity to the training data, with higher probabilities indicating funnier statements."}
{"id": "1705.10272.9d6b2672b11d49c37a6bfb06172d39742d48aef4", "question": "What type of language models are used? e.g. trigrams, bigrams?", "answer": ["bigrams  ... trigrams ", "bigrams and trigrams as features ... KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique"], "num_words": 1134, "gpt4_answer": "The paper uses bigram and trigram language models."}
{"id": "1906.01615.b0e894536857cb249bd75188c3ca5a04e49ff0b6", "question": "How do attention, recurrent and convolutional networks differ on the language classes they accept?", "answer": ["Attention neural networks can represent more languages than other networks. Simple recurring networks can describe regular languages. CNNs can describe only strictly local languages. ", "Unanswerable"], "num_words": 5682, "gpt4_answer": "Attention networks have exponential state complexity, allowing them to accept more complex language classes than recurrent networks like SRNs and GRUs, which have finite state complexity and can only accept regular languages. LSTMs, a type of recurrent network, are more powerful than regular languages but less powerful than attention networks. Convolutional networks are weaker than LSTMs and are equivalent to strictly local languages."}
{"id": "1906.01615.94c22f72665dfac3e6e72e40f2ffbc8c99bf849c", "question": "What type of languages do they test LSTMs on?", "answer": ["Counting ... Counting with Noise ... Reversing", "counter languages"], "num_words": 5682, "gpt4_answer": "They test LSTMs on counter languages and string reversal tasks."}
{"id": "1910.10487.ce8d8de78a21a3ba280b658ac898f73d0b52bf1b", "question": "What is possible future improvement for proposed method/s?", "answer": ["Strategies to reduce number of parameters, space out calls over larger time intervals and use context dependent embeddings.", "memory module could be applied to other domains such as summary generation ... future approach might combine memory module architectures with pointer softmax networks"], "num_words": 4761, "gpt4_answer": "Possible future improvements for the proposed methods include increasing computational speed, reducing the number of parameters, incorporating pointer softmax networks, and applying memory modules to other domains such as summary generation."}
{"id": "1910.10487.e069fa1eecd711a573c0d5c83a3493f5f04b1d8a", "question": "What is percentage change in performance for better model when compared to baseline?", "answer": ["9.2% reduction in perplexity", "This is a 0.68 perplexity improvement over the vanilla language model without the NTM augmentation."], "num_words": 4761, "gpt4_answer": "The NTM-LM architecture achieved a 0.68 perplexity improvement over the vanilla language model without the NTM augmentation."}
{"id": "1910.10487.8db11d9166474a0e98b99ac7f81d1f14539d79ec", "question": "Which of two design architectures have better performance?", "answer": [" NTM-LM", "NTM-LM"], "num_words": 4761, "gpt4_answer": "The NTM-LM (Neural Turing Machine Language Model) architecture performed better than the D-NTMS (Dual-NTM Seq2Seq) architecture."}
{"id": "2002.09616.fa5f5f58f6277a1e433f80c9a92a5629d6d9a271", "question": "What evaluation metrics did they use?", "answer": ["BLEU ... accuracy score", "Bilingual Evaluation Understudy (BLEU) BIBREF22 ... accuracy"], "num_words": 3544, "gpt4_answer": "The evaluation metrics used in the paper are BLEU score for the imaginators' generation performance and accuracy score for the arbitrator's classification performance."}
{"id": "2002.09616.3b9da1af1550e01d2e6ba2b9edf55a289f5fa8e2", "question": "By how much does their model outperform the baseline?", "answer": ["Best accuracy result of proposed model is 82.73, 79.35 compared to best baseline result of 80.75, 78.68 on MultiWoz and DailyDialogue datasets respectively.", "Best model outperforms baseline by 1.98% on MultiWoz dataset and .67% on DailyDialogue dataset"], "num_words": 3544, "gpt4_answer": "The ITA models significantly beat the corresponding baselines by nearly 2% to 3%."}
{"id": "2002.09616.f88f45ef563ea9e40c5767ab2eaa77f4700f95f8", "question": "Which models did they compare with?", "answer": ["Bi-GRUs ... TextCNNs ... BERT", "Bidirectional Gated Recurrent Units (Bi-GRUs) BIBREF23, TextCNNs BIBREF12 and BERT BIBREF14"], "num_words": 3544, "gpt4_answer": "They compared their model with several baseline methods, including Bidirectional Gated Recurrent Units (Bi-GRUs), TextCNNs, and BERT."}
{"id": "2002.09616.99e99f2c25706085cd4de4d55afe0ac43213d7c8", "question": "What is the source of their datasets?", "answer": ["MultiWoz 2.0 ... DailyDialogue", "human-to-human conversations"], "num_words": 3544, "gpt4_answer": "The source of their datasets is MultiWOZ 2.0, a task-oriented dialogue dataset, and DailyDialogue, a chitchat dataset. Both datasets are collected from human-to-human conversations."}
{"id": "1612.00866.da10e3cefbbd7ec73eabc6c93d338239ce84709e", "question": "What new advances are included in this dataset?", "answer": ["PETRARCH  ... PETRARCH2  ...  scraping of news content from the web ... geolocation of the coded events ...  a comprehensive pipeline", "PETRARCH ... PETRARCH2 ... realtime event data ... geolocation"], "num_words": 6627, "gpt4_answer": "The Phoenix dataset includes improvements in news collection and event coding software, the use of open-source natural language processing software, a general processing pipeline for daily-updated data, and a more accurate coding process than previously possible."}
{"id": "1612.00866.00c443f8d32d6baf7c7cea8f4ca9fa749532ccfd", "question": "What language is this dataset in?", "answer": ["English"], "num_words": 6627, "gpt4_answer": "The Phoenix dataset is in English."}
{"id": "1906.06349.6e3e9818551fc2f8450bbf09b0fe82ac2506bc7a", "question": "How do they prove that RNNs with arbitrary precision are as powerful as a pushdown automata?", "answer": ["Theorem 1.11. For any context-free language INLINEFORM0 , suppose we relabel and write INLINEFORM1 for some regular language INLINEFORM2 , whose corresponding minimum-size DFA has INLINEFORM3 states. Then there exists a simple RNN with a hidden layer of size INLINEFORM4 whose INLINEFORM5 -language is INLINEFORM6 .", "They prove that for any context-free language L\nthere exists an RNN whose {0}-language is L."], "num_words": 6205, "gpt4_answer": "They prove that RNNs with arbitrary precision are as powerful as pushdown automata by constructing a simple RNN with one hidden layer and ReLU activation whose language is a context-free language, specifically the Dyck language. Then, using the Chomsky-Schtzenberger theorem, they show that any context-free language can be written as a product of a Dyck language and a regular language, which can be recognized by a simple RNN. This demonstrates that RNNs with arbitrary precision are at least as computationally powerful as pushdown automata."}
{"id": "1906.06349.0b5a505c1fca92258b9e83f53bb8cfeb81cb655a", "question": "What are edge weights?", "answer": ["Unanswerable"], "num_words": 6205, "gpt4_answer": "Edge weights are the values assigned to the connections between nodes in a neural network, representing the strength or importance of those connections."}
{"id": "1908.06493.2b32cf05c5e736f764ceecc08477e20ab9f2f5d7", "question": "Does the paper report F1-scores with and without post-processing for the second task?", "answer": ["With post-processing", "Yes"], "num_words": 3076, "gpt4_answer": "Yes, the paper reports F1-scores with and without post-processing for the second task (subtask B)."}
{"id": "1908.06493.014a3aa07686ee18a86c977bf0701db082e8480b", "question": "What does post-processing do to the output?", "answer": ["Set treshold  for prediction.", "Turning this score into the prediction is usually performed by setting a threshold, such as 0 and 0.5, so labels which have a score assigned greater than that are assigned to the sample"], "num_words": 3076, "gpt4_answer": "Post-processing increases the recall but does not surpass the precision measure score."}
{"id": "1908.06493.6e6d64e2cb7734599890fff3f10c18479756d540", "question": "Do they test any neural architecture?", "answer": ["No", "Yes"], "num_words": 3076, "gpt4_answer": "No, they focused on traditional NLP methods and did not test any neural architecture."}
{"id": "1908.06493.8675d39f1647958faab7fa40cdaab207d4fe5a29", "question": "Is the performance of a Naive Bayes approach evaluated?", "answer": ["No"], "num_words": 3076, "gpt4_answer": "Unanswerable"}
{"id": "1909.12208.14fdc8087f2a62baea9d50c4aa3a3f8310b38d17", "question": "What supports the claim that enhancement in training is advisable as long as enhancement in test is at least as strong as in training?", "answer": ["accuracy improves with increasing power of the enhancement on the training data, however, only as long as the enhancement on the training data is not stronger than on the test data", "we performed an extensive experimental evaluation on the acoustically very challenging CHiME-5 dinner party data"], "num_words": 3781, "gpt4_answer": "The extensive experimental evaluation on the acoustically challenging CHiME-5 dinner party data supports the claim that enhancement in training is advisable as long as enhancement in test is at least as strong as in training."}
{"id": "1909.12208.3d2b5359259cd3518f361d760bacc49d84c40d82", "question": "How does this single-system compares to system combination ones?", "answer": ["WER of the best single system 48.6 (46.7) comapred to 41.6 (43.2) of the best multiple system.", "in terms of WER for the DEV (EVAL) set, the single proposed model (GSS1) has higher WER than the multiple proposed model  (GSS6) by 7.4% (4.1%). "], "num_words": 3781, "gpt4_answer": "The paper focuses on single-system results and does not provide a direct comparison with system combination ones."}
{"id": "1909.12208.26a321e242e58ea5f2ceaf37f26566dd0d0a0da1", "question": "What was previous single-system state of the art result on the CHiME-5 data?", "answer": ["BIBREF12 (H/UPB)", "Previous single system state of the art had WER of  58.3 (53.1)."], "num_words": 3781, "gpt4_answer": "The previous single-system state-of-the-art result on the CHiME-5 data was 45.1% on the development set and 47.3% on the evaluation set."}
{"id": "1909.12208.6920fd470e6a99c859971828e20276a1b9912280", "question": "How much is error rate reduced by cleaning up training data?", "answer": ["In case of singe model the WER was better by 10.%  (6.4%) and in case of multi model it was 3.5% ( 4.1%)", "Unanswerable"], "num_words": 3781, "gpt4_answer": "Cleaning up training data can lead to substantial word error rate reduction, with a 35% (24%) relative WER reduction on the development (evaluation) set when compared with unprocessed training and test data."}
{"id": "1805.09821.f741d32b92630328df30f674af16fbbefcad3f93", "question": "Which baselines were they used for evaluation?", "answer": ["aggregation of multilingual word embeddings ... multilingual sentence representations", "we use MultiCCA word embeddings published by BIBREF3 ... joint multilingual sentence representations"], "num_words": 2381, "gpt4_answer": "The paper used two baselines for evaluation: one based on the aggregation of multilingual word embeddings, and another one based on directly learning multilingual sentence representations."}
{"id": "1805.09821.fe7f7bcf37ca964b4dc9e9c7ebf35286e1ee042b", "question": "What is the difference in size compare to the previous model?", "answer": ["Unanswerable", "larger"], "num_words": 2381, "gpt4_answer": "Unanswerable"}
{"id": "1707.07212.d9354c0bb32ec037ff2aacfed58d57887a713163", "question": "What languages are used as input?", "answer": ["English", "English "], "num_words": 3438, "gpt4_answer": "The input language used in the paper is English."}
{"id": "1707.07212.c035a011b737b0a10deeafc3abe6a282b389d48b", "question": "What are the components of the classifier?", "answer": ["Veridicality  class,  log-linear model for  measuring  distribution over a tweet's veridicality, Twitter NER system  to  to identify named entities,  five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword.", "log-linear model ...  five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword"], "num_words": 3438, "gpt4_answer": "The components of the classifier include context words, distance between entities, presence of punctuation, dependency paths, and negated keyword."}
{"id": "1707.07212.d3fb0d84d763cb38f400b7de3daaa59ed2a1b0ab", "question": "Which uncertain outcomes are forecast using the wisdom of crowds?", "answer": ["neutral (Uncertain about the outcome\")"], "num_words": 3438, "gpt4_answer": "The wisdom of crowds is used to forecast uncertain outcomes by aggregating users' explicit predictions on various contests such as the Oscars, election results, sports events, and awards."}
{"id": "1810.12897.6da1320fa25b2b6768358d3233a5ecf99cc73db5", "question": "What set topics are looked at?", "answer": ["We obtained 50 topics using LDA", "debate topics such as healthcare, military programs, administration processes, worker safety, energy projects, gun control."], "num_words": 2541, "gpt4_answer": "The paper does not provide a specific set of topics, but it mentions that 50 topics were obtained using LDA from Mallet run over the training dataset."}
{"id": "1810.12897.351f7b254e80348221e0654478663a5e53d3fe65", "question": "What were the baselines?", "answer": ["GloVe-d2v", "We use pre-trained GloVe BIBREF10 word embeddings to compute vector representation of each VPD by averaging the GloVe vectors for all words in the document. A logistic regression classifier is then trained on the vector representations thus obtained."], "num_words": 2541, "gpt4_answer": "The baselines used for evaluation were GloVe-d2v and GloVe-d2v+TSM."}
{"id": "1810.12897.d323f0d65b57b30ae85fb9f24298927a3d1216e9", "question": "Which widely used dataset did the authors use?", "answer": ["Convote dataset BIBREF3"], "num_words": 2541, "gpt4_answer": "The authors used the publicly available Convote dataset for their experiments."}
{"id": "1611.01884.05118578b46e9d93052e8a760019ca735d6513ab", "question": "How do they perform semi-supervised learning?", "answer": ["At each iteration of the training process, we first generate INLINEFORM6 samples from the generator G then we draw INLINEFORM7 samples from the real dataset. ... We then perform gradient descent on the AC-BLSTM and generative net G and finally update the parameters of both nets.", "On each step, a generative network is used to generate samples, then a classifier labels them to an extra class. A mix of generated data and real data is combined into a batch, then a gradient descent is performed on the batch,  and the parameters are updated."], "num_words": 2887, "gpt4_answer": "They perform semi-supervised learning by adding samples from a generative network G to their dataset and labeling them to an extra class, then updating the parameters of both the AC-BLSTM and generative net G during training."}
{"id": "1611.01884.31b9337fdfbbc33fc456552ad8c355d836d690ff", "question": "What are the five evaluated tasks?", "answer": ["Model is evaluated on six tasks: TREC, MR, SST-1, SST-2, SUBJ and YELP13.", "TREC, MR, SST, SUBJ, YELP13"], "num_words": 2887, "gpt4_answer": "The five evaluated tasks are sentiment analysis, question type classification, subjectivity classification, movie review sentiment analysis, and Yelp 2013 sentiment classification."}
{"id": "1804.09692.389ff1927ba9fc8bac50959fc09f30c2143cc14e", "question": "What downstream tasks are explored?", "answer": ["word similarity ... POS tagging"], "num_words": 3916, "gpt4_answer": "Word similarity and part-of-speech tagging are the two downstream tasks explored."}
{"id": "1804.09692.b968bd264995cd03d7aaad1baba1838c585ec909", "question": "What factors contribute to the stability of the word embeddings?", "answer": ["POS is one of the biggest factors in stability", "curriculum learning ... POS ... domains."], "num_words": 3916, "gpt4_answer": "Domain, part-of-speech, curriculum learning, and algorithm properties are key factors that contribute to the stability of word embeddings."}
{"id": "1804.09692.afcd1806b931a97c0679f873a71b825e668f2b75", "question": "How is unstability defined?", "answer": ["An embedding is unstable if it has a low number of nearest neighbor embeddings of the words within the same frequency bucket.", "We define stability as the percent overlap between nearest neighbors in an embedding space. ... 0% stability indicates complete disagreement"], "num_words": 3916, "gpt4_answer": "Stability is defined as the percent overlap between nearest neighbors in an embedding space."}
{"id": "1804.09692.01c8c3836467a4399cc37e86244b5bdc5dda2401", "question": "What embedding algorithms are explored?", "answer": [" word2vec, GloVe, and PPMI", "word2vec ... GloVe ... PPMI"], "num_words": 3916, "gpt4_answer": "The paper explores three embedding algorithms: PPMI, word2vec, and GloVe."}
{"id": "1909.12016.568466c62dd73a025bfd9643417cdb7a611f23a1", "question": "Which data-selection algorithms do they use?", "answer": ["Infrequent N-gram Recovery (INR) ... Feature Decay Algorithms (FDA)", "Infrequent N-gram Recovery (INR) and Feature Decay Algorithms (FDA)"], "num_words": 3406, "gpt4_answer": "The paper explores two data-selection algorithms: Infrequent N-gram Recovery (INR) and Feature Decay Algorithms (FDA)."}
{"id": "1909.12016.3a19dc6999aeb936d8a1c4509ebd5bfcda50f0f1", "question": "How are the artificial sentences generated?", "answer": ["generating sentences in the source language by translating monolingual sentences in the target language", "they can be considered as candidate sentences for a data-selection algorithm to decide which sentence-pairs should be used to fine-tune the NMT model"], "num_words": 3406, "gpt4_answer": "Artificial sentences are generated using the back-translation technique, which involves translating monolingual sentences in the target language into the source language using an MT model. These generated sentences in both languages are then paired and used to augment the original parallel training set."}
{"id": "1909.12016.338a3758dccfa438a52d173fbe23a165ef74a0f0", "question": "What domain is their test set?", "answer": ["WMT 2017 biomedical translation ... WMT 2015 News Translation", "biomedical ... News"], "num_words": 3406, "gpt4_answer": "The test sets used are in two different domains: BIO test set (Cochrane dataset from the WMT 2017 biomedical translation shared task) and NEWS test set (WMT 2015 News Translation Task)."}
{"id": "1606.02892.2686e8d51caff9a19684e0c9984bcb5a1937d08d", "question": "What morphological features are considered?", "answer": ["case ... number ... gender ... person ... tense ... aspect", "nouns have case, number and gender ... verbs have person, number, tense and aspect ... features may be underspecified"], "num_words": 3382, "gpt4_answer": "Morphological features considered for German to English translation include case, number, gender for nouns, and person, number, tense, aspect for verbs. Features may be underspecified, and the concatenation of all morphological features of a word is treated as a separate feature value."}
{"id": "1606.02892.df623717255ea2c9e0f846859d8a9ef51dc1102b", "question": "What type of attention do they use in the decoder?", "answer": ["Generalized attention", "weighted sum of the annotations"], "num_words": 3382, "gpt4_answer": "The decoder uses an attentional encoder-decoder network with recurrent neural networks."}
{"id": "1808.09716.ac482ab8a5c113db7c1e5f106a5070db66e7ba37", "question": "What set of semantic tags did they use?", "answer": ["Tags categories ranging from anaphoric (definite, possessive pronoun), attribute (colour, concrete quantity, intersective, relation), unnamed entity (concept), logical (alternative, disjunction), discourse (subordinate relation), events (present simple, past simple), etc.", "tagset originated in the Parallel Meaning Bank (PMB) project BIBREF9"], "num_words": 2817, "gpt4_answer": "The semantic tagset consists of 80 semantic tags grouped in 13 coarse-grained classes."}
{"id": "1808.09716.24897f57e3b0550be1212c0d9ebfcf83bad4164e", "question": "How much improvement did they see on the NLI task?", "answer": ["0.5 improvement with LWS over the single-task model", "Accuracy: SNLI - .5, SICK-E - 3.27"], "num_words": 2817, "gpt4_answer": "The Learning What to Share (LWS) setting showed improvements over the single-task (ST) models for the NLI tasks, particularly for the small-sized SICK-E dataset."}
{"id": "2002.10210.d576af4321fe71ced9e521df1f3fe1eb90d2df2d", "question": "How better are results of new model compared to competitive methods?", "answer": ["For Document- level comparison, the model achieves highest CS precision and  F1 score and it achieves higher BLEU score that TMTE, Coatt, CCDT, and HEDT. \nIn terms of Human Evaluation, the model had the highest average score, the highest Fluency score, and the second highest Content Fidelity. \nIn terms of Sentence-level comparison the model had the highest Recall and F1 scores for Content Fidelity.", "Unanswerable"], "num_words": 4405, "gpt4_answer": "The new model achieves 35.02 in Style BLEU and 39.47 F-score in Content Selection, substantially better than baseline methods."}
{"id": "2002.10210.fd651d19046966ca65d4bcf6f6ae9c66cdf13777", "question": "What is the metrics used for benchmarking methods?", "answer": ["Content Fidelity (CF)  ... Content selection, (CS) ... BLEU ", "Content Fidelity (CF) ... Style Preservation ... BLEU score ... Content selection"], "num_words": 4405, "gpt4_answer": "The metrics used for benchmarking methods are Content Fidelity (CF), Style Preservation, and Content Selection (CS)."}
{"id": "2002.10210.08b77c52676167af72581079adf1ca2b994ce251", "question": "What are other competitive methods?", "answer": [" Rule-based Slot Filling Method (Rule-SF) ... Copy-based Slot Filling Method (Copy-SF)  ... Conditional Copy based Data-To-Text (CCDT) ... Data-To-Text (HEDT)  ... Table Encoder (TMTE) ...  Co-attention-based Method (Coatt)", "Rule-based Slot Filling Method (Rule-SF) ... Copy-based Slot Filling Method (Copy-SF) ... Conditional Copy based Data-To-Text (CCDT) ... Hierarchical Encoder for Data-To-Text (HEDT) ... Text Manipulation with Table Encoder (TMTE) ... Co-attention-based Method (Coatt) ... attention-based Seq2Seq method with copy mechanism ... rule-based method ... MAST ... AdvST ... S-SOTA"], "num_words": 4405, "gpt4_answer": "Competitive methods include Rule-based Slot Filling Method (Rule-SF), Copy-based Slot Filling Method (Copy-SF), Conditional Copy based Data-To-Text (CCDT), Hierarchical Encoder for Data-To-Text (HEDT), Text Manipulation with Table Encoder (TMTE), and Co-attention-based Method (Coatt)."}
{"id": "2002.10210.89fa14a04008c93907fa13375f9e70b655d96209", "question": "What is the size of built dataset?", "answer": ["Document-level dataset has total of 4821 instances. \nSentence-level dataset has total of 45583 instances. ", "Total number of documents is 4821. Total number of sentences is 47583."], "num_words": 4405, "gpt4_answer": "The built dataset consists of 4,821 human-written NBA basketball game summaries aligned with their corresponding game tables."}
{"id": "1909.11706.ff36168caf48161db7039e3bd4732cef31d4de99", "question": "which had better results, the svm or the random forest model?", "answer": ["SVM"], "num_words": 3533, "gpt4_answer": "The Support Vector Machine (SVM) had better results compared to the Random Forest model."}
{"id": "1909.11706.556782bb96f8fc07d14865f122362ebcc79134ec", "question": "which network community detection dataset was used?", "answer": ["Text data from Pypestream", "The data set obtained from Pypestream"], "num_words": 3533, "gpt4_answer": "The Louvain method was used for network community detection in this study."}
{"id": "1909.11706.cb58605a7c230043bd0d6e8d5b068f8b533f45fe", "question": "did they collect the human labeled data?", "answer": ["No", "Yes"], "num_words": 3533, "gpt4_answer": "The human-labeled data was obtained from an artificial intelligence company, Pypestream, and was originally used for creating a conversational intelligence system (chatbot) to support customer inquiries about a particular service."}
{"id": "1909.11706.7969b8d80e12aa3ebb89b5622bc564f44e98329f", "question": "how many classes are they classifying?", "answer": ["18 ", "19 "], "num_words": 3533, "gpt4_answer": "The data contains 2,212 unique sentences labeled with 19 distinct classes."}
{"id": "1911.00461.a554cd1ba2a8d1348a898e0cb4b4c16cc8998257", "question": "Do the authors evaluate only on English datasets?", "answer": ["No", "Unanswerable"], "num_words": 1620, "gpt4_answer": "No, the authors evaluate their method on datasets crawled from newspapers in Chile, Peru, and Mexico, which are in Spanish."}
{"id": "1911.00461.3cc9a820c4a2cd2ff61da920c41ed09f3c0135be", "question": "What metrics of gender bias amplification are used to demonstrate the effectiveness of this approach?", "answer": ["bias amplification metric ... bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators", "the bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators (words man and woman)"], "num_words": 1620, "gpt4_answer": "The paper uses bias amplification metric, which compares the bias score of a word during testing to the one during training, to demonstrate the effectiveness of the approach."}
{"id": "1911.00461.95ef89dc29ff291bdbe48cb956329a6a06d36db8", "question": "How is representation learning decoupled from memory management in this architecture?", "answer": [" based on the use of an external memory in which word embeddings are associated to gender information", "considers the notion of a Fair Region to update a subset of the trainable parameters of a Memory Network"], "num_words": 1620, "gpt4_answer": "The architecture decouples representation learning from memory management by introducing an external memory module with separate arrays for addressable keys (latent representations), values (class labels), and gender information, allowing for sparse updates based on content-based lookup and controlling the number of keys associated with each gender type."}
{"id": "1707.07568.79258cea30cd6c0662df4bb712bf667589498a1f", "question": "What method did the highest scoring team use?", "answer": ["CRF model that used morphosyntactic and distributional features, as well as word clusters based on these learned representations.", "employ CRF as a learning model. In the feature engineering process they use morphosyntactic features, distributional ones as well as word clusters based on these learned representations."], "num_words": 2269, "gpt4_answer": "The highest scoring team (Submission 3) used a CRF (Conditional Random Fields) as a learning model with morphosyntactic features, distributional ones, and word clusters based on learned representations."}
{"id": "1707.07568.8e5ce0d2635e7bdec4ba1b8d695cd06790c8cdaa", "question": "What descriptive statistics are provided about the data?", "answer": ["Entity distribution in the training and test data.", "the number of entities, unique entities in the training and test sets"], "num_words": 2269, "gpt4_answer": "The paper provides statistics on inter-annotator agreement, the number of tweets in the dataset (6,685), the distribution of named entity types, and the percentage of unique entities in the training and test sets."}
{"id": "1707.07568.4e568134c896c4616bc7ab4924686d8d59b57ea1", "question": "What was the level of inter-annotator agreement?", "answer": ["Average Cohens Kappa score of inter-annotator agreement was 0.655", "score for Cohen's Kappa (0,70)"], "num_words": 2269, "gpt4_answer": "The level of inter-annotator agreement was measured using Cohen's Kappa, and the score was 0.70, indicating a strong agreement."}
{"id": "1707.07568.55612e92791296baf18013d2c8dd0474f35af770", "question": "What questions were asked in the annotation process?", "answer": ["Identify the entities occurring in the dataset and annotate them with one of the 13 possible types.", "determine entities and annotate them based on the description that matched the type of entity"], "num_words": 2269, "gpt4_answer": "Unanswerable"}
{"id": "1707.07568.2f23bd86a9e27dcd88007c9058ddfce78a1a377b", "question": "Why is NER for tweets more challenging as the number of entities increases?", "answer": ["NER systems are usually trained using texts that follow particular morpho-syntactic rules. The tweets have a different style and don't follow these rules.", "tweets contain informal text with multilingual content that becomes more difficult to classify when there are more options to choose from"], "num_words": 2269, "gpt4_answer": "NER for tweets becomes more challenging as the number of entities increases because it requires identifying a larger variety of entity types, which may not follow standard morpho-syntactic rules found in more formal text sources."}
{"id": "1707.07568.e0b8a2649e384bbdb17472f8da2c3df4134b1e57", "question": "What data preparation steps were used to construct the dataset?", "answer": ["The tweets were gathered using Twitter API plus tweets provided by the French National Railway Corporation; the tweets were split into training and test sets, and then annotated.", "collecting tweets both from the Twitter API and from SNCF to the identifying and annotating entities occurring in the tweets"], "num_words": 2269, "gpt4_answer": "The data preparation steps for constructing the dataset included collecting tweets using the Twitter API and specific French keywords, incorporating additional tweets provided by the French National Railway Corporation (SNCF), dividing the dataset into training and test sets, and annotating the tweets with 13 types of entities."}
{"id": "1911.00202.3f8a42eb0e904ce84c3fded2103f674e9cbc893d", "question": "What is the training objective in the method introduced in this paper?", "answer": ["elastic weight consolidation ... L2 ... cosine distance", "we explore strategies to reduce forgetting for comprehension systems during domain adaption. Our goal is to preserve the source domain's performance as much as possible, while keeping target domain's performance optimal and assuming no access to the source data. "], "num_words": 4268, "gpt4_answer": "The training objective in the method introduced in this paper is to preserve the source domain's performance as much as possible while keeping the target domain's performance optimal, without assuming access to the source data."}
{"id": "1911.00202.521a3e7300567f6e8e4c531f223dbc9fc306c393", "question": "Does regularization of the fine-tuning process hurt performance in the target domain?", "answer": ["No"], "num_words": 4268, "gpt4_answer": "No, incorporating auxiliary penalty terms does not harm the target domain performance and, in some cases, can even improve it."}
{"id": "2001.04346.863b3f29f8c59f224b4cbdb5f1097b45a25f1d88", "question": "What kind of baseline model do they compare against?", "answer": ["Factorization Machines (FM) BIBREF5, SVD BIBREF0, Probabilistic Matrix Factorization (PMF) BIBREF24, Nonnegative Matrix Factorization (NMF) BIBREF25, DeepCoNN BIBREF1, D-ATT BIBREF3, MPCN BIBREF17, and HUITA BIBREF16", "rating-based collaborative filtering methods ... state-of-the-art methods that leverage the semantic information in reviews"], "num_words": 5074, "gpt4_answer": "They compare against both conventional approaches and state-of-the-art approaches, including Factorization Machines (FM), SVD, Probabilistic Matrix Factorization (PMF), Nonnegative Matrix Factorization (NMF), DeepCoNN, D-ATT, MPCN, and HUITA."}
{"id": "2001.04346.e4cbfabf4509ae0f476f950c1079714a9cd3814e", "question": "Do they analyze which types of sentences/reviews are useful or not?", "answer": ["Unanswerable", "Yes"], "num_words": 5074, "gpt4_answer": "Yes, the authors analyze the usefulness of sentences and reviews by assigning attention weights to them, which helps in determining the importance of different sentences and reviews for modeling users and items."}
{"id": "2001.04346.7a84fed904acc1e0380deb6e5a2e1daacfb5907a", "question": "Which set of datasets do they use?", "answer": ["9 Amazon product review datasets for 9 different domains ... large-scale Yelp challenge dataset on restaurant reviews", "9 Amazon product review datasets for 9 different domains, and the large-scale Yelp challenge dataset on restaurant reviews"], "num_words": 5074, "gpt4_answer": "They use 10 different datasets, including 9 Amazon product review datasets for 9 different domains and the large-scale Yelp challenge dataset on restaurant reviews."}
{"id": "2001.11316.16b816925567deb734049416c149747118e13963", "question": "How long is the dataset?", "answer": ["SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences", "Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively."], "num_words": 3315, "gpt4_answer": "Unanswerable"}
{"id": "2001.11316.9b536f4428206ef7afabc4ff0a2ebcbabd68b985", "question": "How are adversarial examples generated?", "answer": ["By using a white-box method using perturbation calculated based on the gradient of the loss function.", "we are searching for the worst perturbations while trying to minimize the loss of the model"], "num_words": 3315, "gpt4_answer": "Adversarial examples are generated by adding perturbations to the input embeddings, which are created using the gradient of the loss function."}
{"id": "2001.11316.9d04fc997689f44e5c9a551b8571a60b621d35c2", "question": "Is BAT smaller (in number of parameters) than post-trained BERT?", "answer": ["No", "Unanswerable"], "num_words": 3315, "gpt4_answer": "Unanswerable"}
{"id": "2001.11316.8a0e1a298716698a305153c524bf03d18969b1c6", "question": "What are the modifications made to post-trained BERT?", "answer": ["They added adversarial examples in training to improve the post-trained BERT model", "adversarial examples from BERT embeddings using the gradient of the loss ... we feed the perturbed examples to the BERT encoder "], "num_words": 3315, "gpt4_answer": "The modifications made to post-trained BERT include conducting an ablation study on the number of training epochs and dropout values, and proposing a novel architecture called BERT Adversarial Training (BAT) to utilize adversarial training in ABSA."}
{"id": "2001.11316.538430077b1820011c609c8ae147389b960932c8", "question": "What aspects are considered?", "answer": ["Unanswerable"], "num_words": 3315, "gpt4_answer": "The two major tasks considered in this paper are Aspect Extraction (AE) and Aspect Sentiment Classification (ASC)."}
{"id": "1610.07149.97055ab0227ed6ac7a8eba558b94f01867bb9562", "question": "Were human evaluations conducted?", "answer": ["Yes"], "num_words": 3641, "gpt4_answer": "Yes, human evaluations were conducted."}
{"id": "1610.07149.3e23cc3c5e4d5cec51d158130d6aeae120e94fc8", "question": "What datasets are used?", "answer": ["They create their own datasets from online text.", "To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities ... the database contains 7 million query-reply pairs for retrieval ... For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs"], "num_words": 3641, "gpt4_answer": "The paper uses a database of 7 million query-reply pairs for information retrieval, collected from online forums, microblogs, and question-answering communities. For the generation part, another dataset is constructed from various public websites, comprising 1,606,741 query-reply pairs."}
{"id": "1610.07149.bfcbb47f3c54ee1a459183e04e4c5a41ac9ae83b", "question": "How does inference time compare to other methods?", "answer": ["Unanswerable"], "num_words": 3641, "gpt4_answer": "Unanswerable"}
{"id": "1901.03253.fe1a74449847755cd7a46647cc9d384abfee789e", "question": "Where can I access the dataset?", "answer": ["BIBREF9"], "num_words": 6195, "gpt4_answer": "The data collected via Unfun.me, as well as the code for analyzing it, is publicly available online."}
{"id": "1901.03253.425d17465ff91019eb87c28ff3942f781ba1bbcb", "question": "Did they release their dataset?", "answer": ["Yes"], "num_words": 6195, "gpt4_answer": "Yes, they made the data collected via Unfun.me, as well as their code for analyzing it, publicly available online."}
{"id": "1901.03253.08561f6ba578ce8f8d284abf90f5b24eb1f804d3", "question": "Did they use Amazon Mechanical Turk to collect data?", "answer": ["No"], "num_words": 6195, "gpt4_answer": "Unfun.me, an online game, was used to collect data, not Amazon Mechanical Turk."}
{"id": "1901.03253.ec2045e0da92989642a5b5f2b1130c8bd765bcc5", "question": "Did they use The Onion as their dataset?", "answer": ["Yes"], "num_words": 6195, "gpt4_answer": "Yes, they used 9,159 headlines published by the well-known satirical newspaper The Onion as their satirical corpus."}
{"id": "2003.10564.25f699c7a33e77bd552782fb3886b9df9d02abb2", "question": "What sources did they get the data from?", "answer": ["Various web resources and couple of private sources as listed in the table.", "online public-domain sources, private sources and actual books"], "num_words": 870, "gpt4_answer": "The data sources included online public-domain sources, actual books, conversational and literary texts, religious sources, news magazines, a book of proverbs, and a Human Rights declaration."}
{"id": "1902.10246.3e4e415e346a313f5a7c3764fe0f51c11f51b071", "question": "What language is the model tested on?", "answer": ["Unanswerable"], "num_words": 2201, "gpt4_answer": "The model is tested on English language data sets."}
{"id": "1902.10246.d622564b250cffbb9ebbe6636326b15ec3c622d9", "question": "How much lower is the computational cost of the proposed model?", "answer": ["BIBREF9 took approximately 4.5 months even after applying optimization of trimming sentences, while the training process of our FOFE-based model took around 3 days", "By 45 times."], "num_words": 2201, "gpt4_answer": "The computational cost of the proposed FOFE-based model is much lower, taking around 3 days for training compared to the LSTM model, which took approximately 4.5 months."}
{"id": "1902.10246.4367617c0b8c9f33051016e8d4fbb44831c54d0f", "question": "What is the state-of-the-art model?", "answer": ["BIBREF4", "LSTM"], "num_words": 2201, "gpt4_answer": "The state-of-the-art model for word sense disambiguation is the LSTM-based WSD classifier."}
{"id": "1902.10246.2c60628d54f2492e0cbf0fb8bacd8e54117f0c18", "question": "What is a pseudo language model?", "answer": ["Pseudo language model abstracts context as embeddings using preceding and succeeding sequences.", "different from language model in terms of that the sense prediction of a target word depends on its surrounding sequence rather than only preceding sequence"], "num_words": 2201, "gpt4_answer": "A pseudo language model is a model that uses both preceding and succeeding sequences to accommodate the purpose of WSD tasks, as opposed to a traditional language model that predicts the subsequent word given only the preceding sequence."}
{"id": "1706.02222.77a331d4d909d92fab9552b429adde5379b2ae69", "question": "How significant is the performance compared to LSTM model?", "answer": ["0.03 absolute / 2.22% relative BPC ... 11.29 absolute / 10.42% relative PPL", "GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN. ... From our experiment on the PennTreeBank corpus, our proposed models outperformed the baseline models with a similar number of parameters in character-level language modeling and word-level language modeling tasks. In a character-level language modeling task, GRURNTN obtained 0.06 absolute (4.32% relative) BPC reduction over GRURNN, and LSTMRNTN obtained 0.03 absolute (2.22% relative) BPC reduction over LSTMRNN. In a word-level language modeling task, GRURNTN obtained 10.4 absolute (10.63% relative) PPL reduction over GRURNN, and LSTMRNTN obtained 11.29 absolute (10.42% relative PPL) reduction over LSTMRNN."], "num_words": 5000, "gpt4_answer": "FAILED"}
{"id": "1706.02222.516b691ef192f136bb037c12c3c9365ef5a6604c", "question": "How does the introduced model combine the both factors?", "answer": ["For the former limitation, the RecNN performance can be improved by adding more interaction between the two input vectors. Therefore, a new architecture called a Recursive Neural Tensor Network (RecNTN) tried to overcome the previous problem by adding interaction between two vectors using a tensor product, which is connected by tensor weight parameters. Each slice of the tensor weight can be used to capture the specific pattern between the left and right child vectors. For RecNTN, value $p_1$ from Eq. 13 and is defined by:\n\n$$p_1 &=& f\\left( \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} W_{tsr}^{[1:d]} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} W + b \\right) \\\\ p_2 &=& f\\left( \\begin{bmatrix} p_1 & x_3 \\end{bmatrix} W_{tsr}^{[1:d]} \\begin{bmatrix} p_1 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} p_1 & x_3 \\end{bmatrix} W + b \\right)$$ (Eq. 15)\n\nwhere $W_{tsr}^{[1:d]} \\in \\mathbb {R}^{2d \\times 2d \\times d}$ is the tensor weight to map the tensor product between two children vectors. Each slice $W_{tsr}^{[i]}$ is a matrix $\\mathbb {R}^{2d \\times 2d}$ . ", "in this paper we proposed a Gated Recurrent Neural Tensor Network (GRURNTN) to combine these two advantages into an RNN architecture. In this architecture, the tensor product operation is applied between the current input and previous hidden layer multiplied by the reset gates for calculating the current candidate hidden layer values. ... As with GRURNTN, we also applied the tensor product operation for the LSTM unit to improve its performance. In this architecture, the tensor product operation is applied between the current input and the previous hidden layers to calculate the current memory cell. The calculation is parameterized by the tensor weight. We call this architecture a Long Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN). "], "num_words": 5000, "gpt4_answer": "FAILED"}
{"id": "1706.02222.c53b036eff430a9d0449fb50b8d2dc9d2679d9fe", "question": "How much improvement do the introduced model achieve compared to the previous models?", "answer": ["GRURNTN, character: 0.06 absolute / 4.32% relative bits-per-character.\nLSTMRNTN, character: 0.03 absolute / 2.22% relative bits-per-character.\nGRURNTN, word: 10.4 absolute / 10.63% relative perplexity.\nLSTMRNTN, word: 11.29 absolute / 10.42% relative perplexity.", "we report our experiment results on PTB character-level language modeling using our baseline models GRURNN and LSTMRNN as well as our proposed models GRURNTN and LSTMRNTN.  ... In this experiment, GRURNN made faster progress than LSTMRNN, but eventually LSTMRNN converged into a better BPC based on the development set. Our proposed model GRURNTN made faster and quicker progress than LSTMRNTN and converged into a similar BPC in the last epoch. Both proposed models produced lower BPC than our baseline models from the first epoch to the last epoch. ... Our proposed model GRURNTN and LSTMRNTN outperformed both baseline models. GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN. Overall, GRURNTN slightly outperformed LSTMRNTN, and both proposed models outperformed all of the baseline models on the character-level language modeling task. ... we report our experiment results on PTB word-level language modeling using our baseline models GRURNN and LSTMRNN and our proposed models GRURNTN and LSTMRNTN.  ... In this experiment, GRURNN made faster progress than LSTMRNN. Our proposed GRURNTN's progress was also better than LSTMRNTN. The best model in this task was GRURNTN, which had a consistently lower PPL than the other models. ... GRURNTN reduced the perplexity from 97.78 to 87.38 (10.4 absolute / 10.63% relative PPL) over the baseline GRURNN and LSTMRNTN reduced the perplexity from 108.26 to 96.97 (11.29 absolute / 10.42% relative PPL) over the baseline LSTMRNN. Overall, LSTMRNTN improved the LSTMRNN model and its performance closely resembles the baseline GRURNN. However, GRURNTN outperformed all the baseline models as well as the other models by a large margin. ... In a character-level language modeling task, GRURNTN obtained 0.06 absolute (4.32% relative) BPC reduction over GRURNN, and LSTMRNTN obtained 0.03 absolute (2.22% relative) BPC reduction over LSTMRNN. In a word-level language modeling task, GRURNTN obtained 10.4 absolute (10.63% relative) PPL reduction over GRURNN, and LSTMRNTN obtained 11.29 absolute (10.42% relative PPL) reduction over LSTMRNN. "], "num_words": 5000, "gpt4_answer": "FAILED"}
{"id": "1804.00982.5da9e2eef741bd7efccec8e441b8e52e906b2d2d", "question": "do they compare their system with other systems?", "answer": ["Yes"], "num_words": 1668, "gpt4_answer": "The paper mentions that the stance detection model significantly outperforms baselines such as a bag-of-n-grams."}
{"id": "1804.00982.77bc886478925c8e9fb369b1ba5d05c42b3cd79a", "question": "what is the architecture of their model?", "answer": ["a Bidirectional Encoding model BIBREF2", "bidirectional LSTM"], "num_words": 1668, "gpt4_answer": "The architecture of their model is a Bidirectional Encoding model, which uses a bidirectional LSTM (BiLSTM) to encode the entity and initialize another BiLSTM that encodes the article and produces a prediction."}
{"id": "1804.00982.f15bc40960bd3f81bc791f43ab5c94c52378692d", "question": "what dataset did they use for this tool?", "answer": ["They collect data using the AYLIEN News API,  which provides search capabilities for news articles enriched with extracted entities and other metadata and take a step to compile a curated list of topics. The final dataset consists of 32,227 pairs of news articles and topics annotated with their stance. ", "dataset consists of 32,227 pairs of news articles and topics annotated with their stance"], "num_words": 1668, "gpt4_answer": "They collected and annotated a novel dataset, which associates news articles with a stance towards a specified topic."}
{"id": "1809.06083.80d6b9123a10358f57f259b8996a792cac08cb88", "question": "Did they build a dataset?", "answer": ["Yes"], "num_words": 1603, "gpt4_answer": "Yes, they built datasets of news articles in English and German language from the news agency Reuters."}
{"id": "1809.06083.5181aefb8a7272b4c83a1f7cb61f864ead6a1f1f", "question": "Do they compare to other methods?", "answer": ["No"], "num_words": 1603, "gpt4_answer": "The paper discusses other methods like word2vec, GloVe, and unsupervised relation discovery but does not directly compare their method to these approaches."}
{"id": "1809.06083.f010f9aa4ba1b4360a78c00aa0747d7730a61805", "question": "How large is the dataset?", "answer": ["70287", "English corpus has a dictionary of length 106.848 ... German version has a dictionary of length 163.788"], "num_words": 1603, "gpt4_answer": "The English corpus has a dictionary of length 106,848, and the German version has a dictionary of length 163,788."}
{"id": "1709.04005.1e582319df1739dcd07ba0ba39e8f70187fba049", "question": "what is the average number of speakers in the dataset?", "answer": ["26.8"], "num_words": 3571, "gpt4_answer": "The paper does not provide the average number of speakers in the dataset."}
{"id": "1709.04005.aaf2445e78348dba66d7208b7430d25364e11e46", "question": "by how much is accuracy improved?", "answer": ["In addressee selection around 12% in RES-CAND = 2 and 10% in RES-CAND = 10, in candidate responses around 2% in RES-CAND = 2 and 4% in RES-CAND = 10", "The accuracy of addressee selection is improved by 11.025 percent points on average, the accuracy of response selection is improved by 3.09 percent points on average."], "num_words": 3571, "gpt4_answer": "SI-RNN significantly improves addressee selection accuracy by around 12% in RES-CAND 2 and more than 10% in RES-CAND 10, and response selection accuracy by 2% in RES-CAND 2 and 4% in RES-CAND 10 compared to the previous state-of-the-art."}
{"id": "1709.04005.d98148f65d893101fa9e18aaf549058712485436", "question": "what are the previous state of the art systems?", "answer": ["Dynamic-RNN model", "Dynamic-RNN model from BIBREF4"], "num_words": 3571, "gpt4_answer": "The previous state-of-the-art system is the Dynamic-RNN model."}
{"id": "1909.02855.34e9e54fa79e89ecacac35f97b33ef3ca3a00f85", "question": "What are the three SOTA models evaluated?", "answer": ["BIBREF3 ... BIBREF4 ... BIBREF5", "BIBREF3, BIBREF4, BIBREF5 "], "num_words": 3922, "gpt4_answer": "The three state-of-the-art models evaluated are from BIBREF3, BIBREF4, and BIBREF5."}
{"id": "1909.02855.6e63db22a2a34c20ad341eb33f3422f40d0001d3", "question": "What is the morphological constraint added?", "answer": ["Aligned words must share the same morphosyntactic category", "each iteration they can align two words only if they share the same morphosyntactic category"], "num_words": 3922, "gpt4_answer": "The morphological constraint added is that during training, the self-learning models can align two words only if they share the same morphosyntactic category."}
{"id": "2003.04707.58259f2e22363aab20c448e5dd7b6f432556b32d", "question": "How do they interpret the model?", "answer": ["They find relations that connect questions to the answer-options.", "Unanswerable"], "num_words": 5946, "gpt4_answer": "The explainability of AI algorithms is related to how context is processed, computationally, based on the machine's perceptual capabilities and on the external knowledge resources that are available."}
{"id": "2003.04707.b9e0b1940805a5056f71c66d176cc87829e314d4", "question": "Do they compare their approach to data-driven only methods?", "answer": ["No", "Yes"], "num_words": 5946, "gpt4_answer": "Yes, they compare their hybrid approach to data-driven methods for context understanding in the experiments."}
{"id": "2003.04707.b54525a0057aa82b73773fa4dacfd115d8f86f1c", "question": "What are the two applications of neuro-symbolism?", "answer": ["Application I: Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes ... Application II: Neural Question-Answering using Commonsense Knowledge Bases", "Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes ... Neural Question-Answering using Commonsense Knowledge Bases"], "num_words": 5946, "gpt4_answer": "The two applications of neuro-symbolism discussed in the paper are context understanding for autonomous vehicles and context understanding for language tasks."}
{"id": "1605.05166.f264612db9096caf938bd8ee4085848143b34f81", "question": "what elements of each profile did they use?", "answer": ["No profile elements", "time and the linguistic content of posts by the users"], "num_words": 4466, "gpt4_answer": "The models used the time and the linguistic content of posts by the users, not using any profile information such as screen name, birthday, or location."}
{"id": "1605.05166.da0a2195bbf6736119ff32493898d2aadffcbcb8", "question": "Does this paper discuss the potential these techniques have for invading user privacy?", "answer": ["Yes"], "num_words": 4466, "gpt4_answer": "Yes, this paper aims to shed light on the relative ease with which seemingly innocuous information can be used to track users across social networks, even when signing up on different services using completely different account and profile information."}
{"id": "1605.05166.f5513f9314b9d7b41518f98c6bc6d42b8555258d", "question": "How is the gold standard defined?", "answer": ["We used a third party social media site (i.e., Google Plus)", "We used a third party social media site (i.e., Google Plus), one that was not used in our analysis to compile our ground truth ... We discarded all users who did not link to an account for both Twitter and Facebook"], "num_words": 4466, "gpt4_answer": "The gold standard is defined as the precise matching between Twitter and Facebook accounts for the ground truth, collected by crawling publicly available, English-language Google Plus accounts and scraping links to users' other social media profiles."}
{"id": "1607.00167.d97843afec733410d2c580b4ec98ebca5abf2631", "question": "What is the timeframe of the current events?", "answer": ["January 2014 to December 2015", "from January 2014 to December 2015"], "num_words": 1051, "gpt4_answer": "The timeframe of the current events in this study is from January 2014 to December 2015."}
{"id": "1607.00167.813a8156f9ed8ead53dda60ef54601f6ca8076e9", "question": "What model was used for sentiment analysis?", "answer": ["A word-level sentiment analysis was made, using Sentilex-PT BIBREF7 - a sentiment lexicon for the portuguese language, which can be used to determine the sentiment polarity of each word, i.e. a value of -1 for negative words, 0 for neutral words and 1 for positive words", "Lexicon based word-level  SA."], "num_words": 1051, "gpt4_answer": "Sentilex-PT was used for sentiment analysis."}
{"id": "1607.00167.dd807195d10c492da2b0da8b2c56b8f7b75db20e", "question": "How many tweets did they look at?", "answer": ["Unanswerable"], "num_words": 1051, "gpt4_answer": "The paper does not provide the exact number of tweets analyzed."}
{"id": "1607.00167.aa287673534fc05d8126c8e3486ca28821827034", "question": "What language are the tweets in?", "answer": ["Portuguese ", "portuguese and english"], "num_words": 1051, "gpt4_answer": "The tweets are in Portuguese and English."}
{"id": "1903.04329.8b8adb1d5a1824c8995b3eba668745c44f61c9c6", "question": "Is this analysis performed only on English data?", "answer": ["No", "Yes"], "num_words": 3225, "gpt4_answer": "The analysis is primarily focused on English data, but it also includes some Spanish words that could not be separated from the English text due to the shortness of tweets."}
{"id": "1903.04329.88d1bd21b53b8be4f9d3cb26ecc3cbcacffcd63e", "question": "Do they authors offer any hypothesis for why the parameters of Zipf's law and Heaps' law differ on Twitter?", "answer": ["No", "Yes"], "num_words": 3225, "gpt4_answer": "The authors suggest that the nature of communication and the carrying medium (Twitter) itself have a huge impact on the exact values of the parameters of linguistic laws."}
{"id": "1903.04329.74cef0205e0f31d0ab28d0e4d96c1e8ef62d4cce", "question": "What explanation do the authors offer for the super or sublinear urban scaling?", "answer": ["abundance or lack of the elements of urban lifestyle"], "num_words": 3225, "gpt4_answer": "The authors explain that the super or sublinear urban scaling is related to the meaning of the words and their sensitivity to city size, with some words being more prevalent in larger cities and others in smaller cities."}
{"id": "1903.04329.200c37060d037dee33f3b7c8b1a2aaa58376566e", "question": "Do the authors give examples of the core vocabulary which follows the scaling relationship of the bulk text?", "answer": ["Yes"], "num_words": 3225, "gpt4_answer": "Yes, the authors mention that the core vocabulary has a lot in common with the most common words of the English language, but also shows some Twitter-specific elements."}
{"id": "1605.05156.415014a5bcd83df52c9307ad16fab1f03d80f705", "question": "What syntactic and semantic features are proposed?", "answer": ["Opinion Words ... Vulgar Words ... Emoticons ... Speech Act Verbs ... N-grams ... Punctuations ... Twitter-specific Characters ... Abbreviations ... Dependency Sub-trees ... Part-of-speech", "Semantic Features : Opinion Words, Vulgar Words, Emoticons,  Speech Act Verbs,  N-grams.\nSyntactic Features: Punctuations, Twitter-specific Characters, Abbreviations, Dependency Sub-trees,  Part-of-speech."], "num_words": 2483, "gpt4_answer": "The proposed syntactic and semantic features include opinion words, vulgar words, emoticons, speech act verbs, n-grams, punctuations, Twitter-specific characters, abbreviations, dependency sub-trees, and part-of-speech."}
{"id": "1605.05156.b79c85fa84712d3028cb5be2af873c634e51140e", "question": "Which six speech acts are included in the taxonomy?", "answer": ["Assertion ... Recommendation  ... Expression ... Question ... Request ... Miscellaneous", "Assertion, Recommendation Expression, Question, Request, and Miscellaneous"], "num_words": 2483, "gpt4_answer": "The six speech acts included in the taxonomy are Assertion, Recommendation, Expression, Question, Request, and Miscellaneous."}
{"id": "1605.05156.dc473819b196c0ea922773e173a6b283fa778791", "question": "what classifier had better performance?", "answer": ["logistic regression", "topic-specific classifier"], "num_words": 2483, "gpt4_answer": "The logistic regression classifier had better performance."}
{"id": "1605.05156.9207f19e65422bdf28f20e270ede6c725a38e5f9", "question": "how many tweets were labeled?", "answer": ["7,563"], "num_words": 2483, "gpt4_answer": "7,563 total tweets were labeled."}
{"id": "1605.05156.8ddf78dbdc6ac964a7102ae84df18582841f2e3c", "question": "how many annotators were there?", "answer": ["three"], "num_words": 2483, "gpt4_answer": "There were three undergraduate annotators."}
{"id": "1605.05156.079e654c97508c521c07ab4d24cdaaede5602c61", "question": "who labelled the tweets?", "answer": ["three undergraduate annotators", "three undergraduate annotators "], "num_words": 2483, "gpt4_answer": "Three undergraduate annotators independently labeled the tweets."}
{"id": "1605.05156.7efbd9adbc403de4be6b1fb1999dd5bed9d6262c", "question": "what are the proposed semantic features?", "answer": ["Binary features indicating opinion words, vulgar words, emoticons, speech act verbs and unigram, bigram and trigram that appear at least five times in the dataset", "Opinion Words ... Vulgar Words ... Emoticons ... Speech Act Verbs ... N-grams"], "num_words": 2483, "gpt4_answer": "The proposed semantic features are Opinion Words, Vulgar Words, Emoticons, Speech Act Verbs, and N-grams."}
{"id": "1605.05156.95bbd91badbfe979899cca6655afc945ea8a6926", "question": "what syntactic features are proposed?", "answer": ["Binary features indicating appeance of punctuations, twitter-specific characters - @, #, and RT, abbreviations, length one and two sub-trees extracted from dependency sub-tree and Part-of-speech - adjectives and interjections.", "Punctuations ... Twitter-specific Characters ... Abbreviations ... Dependency Sub-trees ... Part-of-speech"], "num_words": 2483, "gpt4_answer": "The syntactic features proposed include punctuations, Twitter-specific characters, abbreviations, dependency sub-trees, and part-of-speech (usage of adjectives and interjections)."}
{"id": "1605.05156.76ae794ced3b5ae565f361451813f2f3bc85b214", "question": "what datasets were used?", "answer": ["A dataset they annotated, \"Harvard General Inquirer\" Lexicon for opinion words, a collection of vulgar words, an online collection of text-based emoticons, and Wierzbicka's collection of English speech act verbs", "Twitter data"], "num_words": 2483, "gpt4_answer": "The dataset used was a manually annotated collection of tweets for six selected topics, with a total of 7,563 tweets."}
{"id": "1804.05306.2a9c7243744b42f1e9fed9ff2ab17c6f156b1ba4", "question": "What was the baseline?", "answer": ["Model A, which was trained on LibriSpeech data with SAT, together with the language model also trained with LibriSpeech.", "a model trained on LibriSpeech data with SAT and a with a LM also trained with LibriSpeech"], "num_words": 2831, "gpt4_answer": "The baseline was Model A in Fig. FIGREF5, trained on LibriSpeech data with SAT, together with the language model also trained with LibriSpeech. It had an extremely high WER (96.21%), indicating the wide mismatch between speech and song audio, and the high difficulties in transcribing song audio."}
{"id": "1804.05306.f8f64da7172e72e684f0e024a19411b43629ff55", "question": "How many songs were collected?", "answer": ["110 pieces of music-removed version of commercial English popular songs", "130 "], "num_words": 2831, "gpt4_answer": "130 music-removed English songs were collected."}
{"id": "1802.02614.8da8c4651979a4b1d1d3008c1f77bc7e9397183b", "question": "how does end of utterance and token tags affect the performance", "answer": ["Performance degrades if the tags are not used.", "The performance is significantly degraded without two special tags (0,025 in MRR)"], "num_words": 3027, "gpt4_answer": "The performance is significantly degraded without the end-of-utterance (__eou__) and end-of-turn (__eot__) tags, as they carry useful information and help capture utterance and turn boundary structure information."}
{"id": "1802.02614.8cf52ba480d372fc15024b3db704952f10fdca27", "question": "what are the baselines?", "answer": ["ESIM"], "num_words": 3027, "gpt4_answer": "The baselines include RNN, CNN, LSTM, BiLSTM, Dual-Encoder, and ESIM models."}
{"id": "1802.02614.d8ae36ae1b4d3af5b59ebd24efe94796101c1c12", "question": "what kind of conversations are in the douban conversation corpus?", "answer": ["Conversations from popular social networking service in China", "Conversations that are typical for a social networking service."], "num_words": 3027, "gpt4_answer": "The Douban conversation corpus consists of conversations constructed from Douban group, a popular social networking service in China."}
{"id": "1802.02614.2bd702174e915d97884d1571539fb1b5b0b7123a", "question": "what pretrained word embeddings are used?", "answer": ["300-dimensional GloVe vectors", "GloVe ... FastText "], "num_words": 3027, "gpt4_answer": "The pretrained word embeddings used are GloVe vectors."}
{"id": "1910.12203.0c247a04f235a4375dd3b0fd0ce8d0ec72ef2256", "question": "What other evaluation metrics are reported?", "answer": ["Macro-averaged F1-score, macro-averaged precision, macro-averaged recall", "Precision and recall for 2-way classification and F1 for 4-way classification."], "num_words": 2054, "gpt4_answer": "The paper reports macro-averaged scores for the experiments."}
{"id": "1910.12203.66dfcdab1db6a8fcdf392157a478b4cca0d87961", "question": "What out of domain scenarios did they evaluate on?", "answer": ["In 2-way classification they used LUN-train for training, LUN-test for development and the entire SLN dataset for testing. In 4-way classification they used LUN-train for training and development and LUN-test for testing.", "entire SLN dataset ...  LUN-test as our out of domain test set"], "num_words": 2054, "gpt4_answer": "The out-of-domain scenarios were evaluated on the entire SLN dataset for 2-way classification between satirical and trusted news articles, and on the LUN-test set for 4-way classification between satire, propaganda, hoax, and trusted articles."}
{"id": "1910.12203.7ef34b4996ada33a4965f164a8f96e20af7470c0", "question": "What was their state of the art accuracy score?", "answer": ["In 2-way classification precision score was 88% and recall 82%. In 4-way classification on LUN-dev F1-score was 91% and on LUN-test F1-score was 65%.", "accuracy of 87% on the entire RPN dataset when used as an out-of-domain test set"], "num_words": 2054, "gpt4_answer": "The state-of-the-art accuracy score is not explicitly mentioned in the provided text."}
{"id": "1910.12203.6e80386b33fbfba8bc1ab811a597d844ae67c578", "question": "Which datasets did they use?", "answer": ["Satirical and Legitimate News Database ... Random Political News Dataset ... Labeled Unreliable News Dataset", "Satirical and Legitimate News Database BIBREF2 ... RPN: Random Political News Dataset BIBREF10 ... LUN: Labeled Unreliable News Dataset BIBREF0"], "num_words": 2054, "gpt4_answer": "They used SLN: Satirical and Legitimate News Database, RPN: Random Political News Dataset, and LUN: Labeled Unreliable News Dataset for their experiments."}
{"id": "1910.12203.1c182b4805b336bd6e1a3f43dc84b07db3908d4a", "question": "What are the neural baselines mentioned?", "answer": ["CNN ... LSTM ... BERT"], "num_words": 2054, "gpt4_answer": "The neural baselines mentioned are CNN (Convolutional Neural Network), LSTM (Long Short-Term Memory), and BERT (Bidirectional Encoder Representations from Transformers)."}
{"id": "1911.07620.f71b95001dce46ee35cdbd8d177676de19ca2611", "question": "What regularization methods are used?", "answer": ["dropout ... DropBlock", "dropout ... embedding dropout ... DropBlock"], "num_words": 3639, "gpt4_answer": "Dropout on the fully-connected layer, embedding dropout on the inputs, and DropBlock on the activations of the convolutional layers are used as regularization methods."}
{"id": "1911.07620.5aa6556ffd7142933f820a015f1294d38e8cd96c", "question": "What metrics are used?", "answer": ["Accuracy, Precision, Recall, F1-score", "Accuracy, precision, recall and F1 score."], "num_words": 3639, "gpt4_answer": "The metrics used are accuracy and F1 score."}
{"id": "1911.07620.10edfb9428b8a4652274c13962917662fdf84f8a", "question": "How long is the dataset?", "answer": ["2022", "almost doubles the number of commits in the training split to 1493 ... validation, and test splits containing 808, 265, and 264 commits"], "num_words": 3639, "gpt4_answer": "The dataset contains 808 training commits, 265 validation commits, and 264 test commits."}
{"id": "1911.07620.a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763", "question": "What dataset do they use?", "answer": ["Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github", "manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them"], "num_words": 3639, "gpt4_answer": "They use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23."}
{"id": "1911.03353.0b5a7ccf09810ff5a86162d502697d16b3536249", "question": "What simplification of the architecture is performed that resulted in same performance?", "answer": [" we simplify the origin network architecture and extract span representation by a simple pooling layer", "randomly sampling them rather than enumerate them all ... simple max-pooling to extract span representation because those features are implicitly included in self-attention layers of transformers"], "num_words": 1790, "gpt4_answer": "The simplification of the architecture involves removing the pruner, using under-sampling to balance positive and negative samples, and replacing the attention or LSTM network in span extractors with a simple pooling layer."}
{"id": "1911.03353.8f00859f74fc77832fa7d38c22f23f74ba13a07e", "question": "How much better is performance of SEPT compared to previous state-of-the-art?", "answer": ["In ELMo model, SCIIE achieves almost 3.0% F1 higher than BiLSTM ...  in SciBERT, the performance becomes similar, which is only a 0.5% gap", "SEPT have improvement for Recall  3.9%  and F1 1.3%  over the best performing baseline (SCIIE(SciBERT))"], "num_words": 1790, "gpt4_answer": "SEPT achieves a new state of the art result in scientific named entity recognition, outperforming previous transformer-based models, especially in recall."}
{"id": "1906.04236.bda21bfb2dd74085cbc355c70dab5984ef41dba7", "question": "How many actions are present in the dataset?", "answer": ["14,769", "14,769 actions"], "num_words": 4342, "gpt4_answer": "The dataset includes a total of 14,769 actions, 4,340 of which are visible."}
{"id": "1906.04236.c2497552cf26671f6634b02814e63bb94ec7b273", "question": "How many videos did they use?", "answer": ["1,268", "177"], "num_words": 4342, "gpt4_answer": "The dataset consists of 1,268 short video clips."}
{"id": "1906.04236.441a2b80e82266c2cc2b306c0069f2b564813fed", "question": "What unimodal algorithms do they compare with?", "answer": ["Concreteness ... Feature-based Classifier ... LSTM and ELMo ... Yolo Object Detection", "SVM ... LSTM ... ELMo ... Yolo Object Detection"], "num_words": 4342, "gpt4_answer": "They compare their multimodal model with several unimodal baselines, including Concreteness, Feature-based Classifier, LSTM and ELMo, and Yolo Object Detection."}
{"id": "1906.04236.e462efb58c71f186cd6b315a2d861cbb7171f65b", "question": "What platform was used for crowdsourcing?", "answer": ["Amazon Mechanical Turk ", "Amazon Mechanical Turk (AMT)"], "num_words": 4342, "gpt4_answer": "Amazon Mechanical Turk (AMT) was used for crowdsourcing."}
{"id": "1906.04236.84f9952814d6995bc99bbb3abb372d90ef2f28b4", "question": "What language are the videos in?", "answer": ["Unanswerable"], "num_words": 4342, "gpt4_answer": "The paper does not explicitly mention the language of the videos."}
{"id": "1906.04236.5364fe5f256f1263a939e0a199c3708727ad856a", "question": "How long are the videos?", "answer": ["On average videos are 16.36 minutes long", "length of our collected videos varies from two minutes to twenty minutes"], "num_words": 4342, "gpt4_answer": "The length of the collected videos varies from two minutes to twenty minutes."}
{"id": "1812.05813.e500948fa01c74e5cb3e6774f66aaa9ad4b3e435", "question": "What was the inter-annotator agreement between the expert annotators?", "answer": ["Unanswerable"], "num_words": 2879, "gpt4_answer": "The paper does not provide a specific inter-annotator agreement value between the expert annotators."}
{"id": "1812.05813.b8b79a6123716cb9fabf751b31dff424235a2ee2", "question": "How were missing hypotheses discovered?", "answer": ["More precisely, the experts were instructed to re-consider each problem and be especially wary of missing hypotheses. If they considered the entailment to hold, we still gave the instruction to optionally mention any additional implicit hypothesis that they would be using. ", "The problems were then re-rated by experts in logic and/or linguistics.  ... If they considered the entailment to hold, we still gave the instruction to optionally mention any additional implicit hypothesis that they would be using."], "num_words": 2879, "gpt4_answer": "Missing hypotheses were discovered by consulting a crowd of experts in logic and/or linguistics, who re-rated the problems and were instructed to be especially wary of missing hypotheses."}
{"id": "1806.07042.00f507053c47e55d7e72bebdbd8a75b3ca88cf85", "question": "Which aspects of response generation do they evaluate on?", "answer": ["fluency ... relevance ... diversity  ... originality"], "num_words": 4765, "gpt4_answer": "They evaluate response generation on four criteria: fluency, relevance, diversity, and originality."}
{"id": "1806.07042.e14e3e0944ec3290d1985e9a3da82a7df17575cd", "question": "Which dataset do they evaluate on?", "answer": [" a large scale Chinese conversation corpus", "Chinese conversation corpus comprised of 20 million context-response pairs", "Chinese dataset containing human-human context response pairs collected from Douban Group "], "num_words": 4765, "gpt4_answer": "They evaluate on a Chinese dataset collected from Douban Group with over 20 million human-human context-response pairs."}
{"id": "1806.07042.f637bba86cfb94ca8ac4b058faf839c257d5eaa0", "question": "What model architecture do they use for the decoder?", "answer": ["GRU", "a GRU language model"], "num_words": 4765, "gpt4_answer": "The decoder uses a GRU language model with attention."}
{"id": "1806.07042.0b5bf00d2788c534c4c6c007b72290c48be21e16", "question": "Do they ensure the edited response is grammatical?", "answer": ["Yes"], "num_words": 4765, "gpt4_answer": "The paper aims to generate grammatical responses by editing prototype responses, which are human-written and already grammatical, thus alleviating the \"safe response\" problem."}
{"id": "1806.07042.86c867b393db0ec4ad09abb48cc1353cac47ea4c", "question": "What do they use as the pre-defined index of prototype responses?", "answer": ["similar context INLINEFORM1 and its associated response INLINEFORM2", "to compute the context similarity."], "num_words": 4765, "gpt4_answer": "They use Lucene to construct the pre-defined index of prototype responses."}
{"id": "1911.09241.8f6b11413a19fe4639b3fba88fc6b3678286fa0c", "question": "what are all the datasets they experiment with?", "answer": ["CoQA  ... DuoRC  ... HotpotQA  ... SQuAD v1.1  ... SQuAD v2.0 ... ARC (Challenge) ... MCTest  ... MultiRC  ... RACE  ... SWAG", "CoQA BIBREF17 ... DuoRC BIBREF18 ... HotpotQA (distractor) BIBREF1 ... SQuAD v1.1 BIBREF0 ... SQuAD v2.0 BIBREF20 ... ARC (Challenge) BIBREF21 ... MCTest BIBREF22 ...  MultiRC BIBREF23 ... RACE BIBREF24 ...  SWAG BIBREF25"], "num_words": 5676, "gpt4_answer": "CoQA, DuoRC, HotpotQA (distractor), SQuAD v1.1, SQuAD v2.0, ARC (Challenge), MCTest, MultiRC, RACE, and SWAG."}
{"id": "1911.09241.141f23e87c10c2d54d559881e641c983e3ec8ef3", "question": "what was the baseline model?", "answer": [" BERT-large", "BERT-large BIBREF3"], "num_words": 5676, "gpt4_answer": "The baseline model used in the paper was BERT-large."}
{"id": "1912.11637.45e6532ac06a59cb6a90624513242b06d7391501", "question": "What do they mean by explicit selection of most relevant segments?", "answer": ["It is meant that only most contributive k elements are reserved, while other elements are removed.", "focusing on the top-k segments that contribute the most in terms of correlation to the query"], "num_words": 3742, "gpt4_answer": "Explicit selection of the most relevant segments refers to the process of choosing the top-k most contributive elements in the attention mechanism, focusing the attention on these elements and removing irrelevant information."}
{"id": "1912.11637.a98ae529b47362f917a398015c8525af3646abf0", "question": "What datasets they used for evaluation?", "answer": ["For En-De translation the newstest 2014 set from WMT 2014 En-De translation dataset, for En-Vi translation the tst2013 from IWSLT 2015 dataset. and for De-En translation the teste set from IWSLT 2014.", "newstest 2014 ... tst2013 ... Following BIBREF21, we used the same test set with around 7K sentences. ... MSCOCO 2014 test set ... Enwiki8"], "num_words": 3742, "gpt4_answer": "The authors used three NMT tasks (WMT 2014 English-to-German, IWSLT 2015 English-to-Vietnamese, and IWSLT 2014 German-to-English), the Microsoft COCO 2014 dataset for image captioning, and the enwiki8 dataset for character-level language modeling for evaluation."}
{"id": "1910.09942.58df55002fbcba76b9aeb2181d78378b8c01a827", "question": "Which part of their architecture provides the most speedup in comparison to existing approaches?", "answer": ["Unanswerable"], "num_words": 3918, "gpt4_answer": "The G-SAT model provides the most speedup in comparison to existing approaches by using a single shared encoder for all slots and slot-specific classifier modules, which significantly reduces the prediction time and the number of parameters."}
{"id": "1910.09942.7a60f29e28063f50c2a7afd1c2a7668fb615cd53", "question": "Do they consistently outperform existing systems in terms of accuracy?", "answer": ["No"], "num_words": 3918, "gpt4_answer": "The G-SAT model is competitive with existing systems in terms of accuracy but does not consistently outperform them in all cases. However, it significantly reduces prediction time while maintaining high-level performance."}
{"id": "2002.04326.6371c6863fe9a14bf67560e754ce531d70de10ab", "question": "How big is this dataset?", "answer": ["6,138 logical reasoning questions", "6,138 pieces of logical reasoning questions"], "num_words": 3609, "gpt4_answer": "The ReClor dataset contains 6,138 logical reasoning questions."}
{"id": "2002.04326.28a8a1542b45f67674a2f1d54fff7a1e45bfad66", "question": "How are biases identified in the dataset?", "answer": ["They identify biases as lexical choice and sentence length for right and wrong answer options in an isolated context, without the question and paragraph context that typically precedes answer options. Lexical choice was identified by calculating per-token correlation scores with \"right\" and \"wrong labels. They calculated the average sentence length for \"right\" and \"wrong\" sentences.", "we feed the five strong baseline models (GPT, GPT-2, BERT$_{\\small \\textsc {BASE}}$, XLNet$_{\\small \\textsc {BASE}}$ and RoBERTa$_{\\small \\textsc {BASE}}$) with ONLY THE ANSWER OPTIONS for each problem ...  identify those problems that can be answered correctly by merely exploiting the biases in answer options without knowing the relevant context and question"], "num_words": 3609, "gpt4_answer": "Biases in the dataset are identified by feeding strong baseline models with only the answer options for each problem, without context and question. Data points that are consistently predicted correctly by each model across different random seeds are considered biased and grouped into the EASY set."}
{"id": "1911.04128.539f5c27e1a2d240e52b711d0a50a3a6ddfa5cb2", "question": "How do they deal with imbalanced datasets?", "answer": ["data expansion using oversampling ... add loss control", "data expansion using oversampling ... add loss control in the model"], "num_words": 2371, "gpt4_answer": "They deal with imbalanced datasets by using data expansion through oversampling and by adding loss control in the model with a modified loss function."}
{"id": "1911.04128.aa7c5386aedfb13a361a2629b67cb54277e208d2", "question": "What models do they compare to?", "answer": ["rule-based TN model", "six different variations of their multi-head attention model"], "num_words": 2371, "gpt4_answer": "They compare 7 different neural model setups with varying configurations such as replacing word-to-vector with BERT, changing padding values, altering context window length, using different loss functions, removing mask, and applying data expansion."}
{"id": "1911.04128.9b3371dcd855f1d3342edb212efa39dfc9142ae3", "question": "What text preprocessing tasks do they focus on?", "answer": ["Unanswerable", "normalize unreadable numbers, symbols or characters"], "num_words": 2371, "gpt4_answer": "The paper focuses on text normalization tasks, specifically transforming non-standard words (NSW) into spoken-form words (SFW) for disambiguation in Mandarin."}
{"id": "1911.04128.b02a6f59270b8c55fa4df3751bcb66fca2371451", "question": "What news sources did they get the dataset from?", "answer": ["Unanswerable"], "num_words": 2371, "gpt4_answer": "Unanswerable"}
{"id": "1911.04128.3a3c372b6d73995adbdfa26103c85b32d071ff10", "question": "Did they collect their own corpus?", "answer": ["Unanswerable"], "num_words": 2371, "gpt4_answer": "The training dataset is from their internal news corpus."}
{"id": "1912.07940.952fe4fbf4e0bcfcf44fab2dbd3ed85dd961eff3", "question": "Do the tweets fall under a specific domain?", "answer": ["No", "Unanswerable"], "num_words": 2183, "gpt4_answer": "Unanswerable"}
{"id": "1912.07940.1dc5bf9dca7de2ba21db10e9056d3906267ef5d5", "question": "How many tweets are in the dataset?", "answer": ["670 tweets ", "These 980 PLOs were annotated within a total of 670 tweets."], "num_words": 2183, "gpt4_answer": "There are 670 tweets in the dataset."}
{"id": "1912.07940.8faec509406d33444bd620afc829adc9eae97644", "question": "What categories do they look at?", "answer": ["PERSON, LOCATION, ORGANIZATION", "PERSON, LOCATION, and ORGANIZATION"], "num_words": 2183, "gpt4_answer": "They look at eight name variant categories: WELL-FORMED, ABBREVIATION, CAPITALIZATION, DIACRITICS, HASHTAG-LIKE, CONTRACTED, HYPOCORISM, and ERROR."}
{"id": "1606.07947.e3c2b6fcf77a7b1c76add2e6e1420d07c29996ea", "question": "Which knowledge destilation methods do they introduce?", "answer": ["Word-Level Knowledge Distillation ... Sequence-Level Knowledge Distillation ... Sequence-Level Interpolation", "standard knowledge distillation for NMT "], "num_words": 3613, "gpt4_answer": "They introduce two sequence-level variants of knowledge distillation: Sequence-Level Knowledge Distillation and Sequence-Level Interpolation."}
{"id": "1606.07947.ee2c2fb01d67f4c58855bf23186cbd45cecbfa56", "question": "What type of weight pruning do they use?", "answer": ["Prune %x of the parameters by removing the weights with the lowest absolute values.", "pruning parameters by removing the weights with the lowest absolute values"], "num_words": 3613, "gpt4_answer": "They use magnitude-based pruning, removing the weights with the lowest absolute values."}
{"id": "1606.07947.f77d7cddef3e021d70e16b9e16cecfd4b8ee80d3", "question": "Which dataset do they train on?", "answer": ["IWSLT 2015 ...  WMT 2014", "WMT 2014 ... IWSLT 2015"], "num_words": 3613, "gpt4_answer": "The authors train on two datasets: a high-resource dataset (English-German) from WMT 2014 and a low-resource dataset (Thai-English) from IWSLT 2015."}
{"id": "1606.07947.a0197894ee94b01766fa2051f50f84e16b5c9370", "question": "Do they reason why greedy decoding works better then beam search?", "answer": ["No", "Yes"], "num_words": 3613, "gpt4_answer": "The paper suggests that greedy decoding works better than beam search for sequence-level knowledge distillation models because these models only model relevant parts of the teacher distribution (around the teacher's mode), making the student's distribution more peaked and the mode easier to find."}
{"id": "1603.00957.55bafa0f7394163f4afd1d73340aac94c2d9f36c", "question": "Are experiments conducted on multiple datasets?", "answer": ["No"], "num_words": 4954, "gpt4_answer": "The paper presents a method for question answering that infers both structured and unstructured data. The method uses a Multi-Channel Convolutional Neural Network (MCCNN) for relation extraction and combines joint entity linking and relation extraction with unstructured inference. The method achieves a state-of-the-art F1 score of 53.3% on the WebQuestions dataset."}
{"id": "1603.00957.cbb4eba59434d596749408be5b923efda7560890", "question": "What baselines is the neural relation extractor compared to?", "answer": ["BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 ... BIBREF11 , BIBREF12 ... BIBREF7 , BIBREF13 , BIBREF14 ...  BIBREF16", "Berant et al. (2013), Yao and Van Durme (2014), Xu et al. (2014), Berant and Liang (2014), Bao et al. (2014), Border et al. (2014), Dong et al. (2015), Yao (2015), Bast and Haussmann (2015), Berant and Liang (2015), Reddy et al. (2016), Yih et al. (2015)"], "num_words": 4954, "gpt4_answer": "The neural relation extractor is compared to pipelined entity linking and relation extraction, joint entity linking and relation extraction, and pipelined entity linking and relation extraction with inference on unstructured data."}
{"id": "1603.00957.1d9d7c96c5e826ac06741eb40e89fca6b4b022bd", "question": "What additional evidence they use?", "answer": ["Wikipedia sentences that validate or support KB facts", "by applying an answer refinement model which takes the Wikipedia page of the topic entity into consideration to filter out the wrong answers and pick the correct ones"], "num_words": 4954, "gpt4_answer": "The method uses additional evidence from Wikipedia to validate candidate answers retrieved from Freebase, refining the results and selecting the correct answers."}
{"id": "1603.00957.d1d37dec9053d465c8b6f0470e06316bccf344b3", "question": "How much improvement they get from the previous state-of-the-art?", "answer": ["0.8 point improvement", "0.8 point on average (question-wise) F1 measure "], "num_words": 4954, "gpt4_answer": "Our method achieves an F_1 of 53.3%, a substantial improvement over the state-of-the-art."}
{"id": "1603.00957.90eeb1b27f84c83ffcc8a88bc914a947c01a0c8b", "question": "What is the previous state-of-the-art?", "answer": ["F1 score of 39.9 for semantic-based parsing methods. For information extraction methods, 49.4 using relation extraction, 40.8 using distributed representations, and 52.5 using neural networks models", "yih-EtAl:2015:ACL-IJCNLP"], "num_words": 4954, "gpt4_answer": "Our method achieves an F1 of 53.3%, a substantial improvement over the state-of-the-art."}
{"id": "1804.08000.e057fa254ea7a4335de22fd97a0f08814b88aea4", "question": "What is the architecture of the model?", "answer": ["Document-level context encoder, entity and sentence-level context encoders with common attention, then logistic regression, followed by adaptive thresholds.", "logistic regression"], "num_words": 1804, "gpt4_answer": "The architecture of the model consists of three encoders (entity encoder, sentence-level context encoder, and document-level context encoder) that encode entity and its context into feature vectors, and it employs a logistic regression with adaptive thresholds for classification."}
{"id": "1804.08000.134a66580c363287ec079f353ead8f770ac6d17b", "question": "What fine-grained semantic types are considered?", "answer": ["/other/event/accident, /person/artist/music, /other/product/mobile phone, /other/event/sports event, /other/product/car", "Unanswerable"], "num_words": 1804, "gpt4_answer": "The paper does not explicitly list the fine-grained semantic types considered, but it mentions using 89 types from OntoNotes, 93 types from BBN, and 113 types from FIGER datasets."}
{"id": "1804.08000.610fc593638c5e9809ea9839912d0b282541d42d", "question": "What hand-crafted features do other approaches use?", "answer": ["e.g., lexical and syntactic features", "lexical and syntactic features"], "num_words": 1804, "gpt4_answer": "Other approaches use hand-crafted features such as lexical and syntactic features."}
{"id": "1908.05803.ab895ed198374f598e13d6d61df88142019d13b8", "question": "What is the strong baseline model used?", "answer": ["Passage-only heuristic baseline, QANet, QANet+BERT, BERT QA", "an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0"], "num_words": 1170, "gpt4_answer": "The strong baseline model used is uncased base BERT trained on SQuAD 1.1."}
{"id": "1908.05803.8795bb1f874e5f3337710d8c3d5be49e672ab43a", "question": "What crowdsourcing platform did they obtain the data from?", "answer": ["Mechanical Turk"], "num_words": 1170, "gpt4_answer": "They obtained the data from Mechanical Turk."}
{"id": "1910.02677.c30b0d6b23f0f01573eea315176c5ffe4e0c6b5c", "question": "How large is the test set?", "answer": ["359 samples"], "num_words": 3230, "gpt4_answer": "The test set contains 359 samples."}
{"id": "1910.02677.311f9971d61b91c7d76bba1ad6f038390977a8be", "question": "What does SARI measure?", "answer": ["SARI compares the predicted simplification with both the source and the target references", "the predicted simplification with both the source and the target references"], "num_words": 3230, "gpt4_answer": "SARI measures the quality of text simplification by comparing the predicted simplification with both the source and the target references, averaging F1 scores for three n-gram operations: additions, keeps, and deletions."}
{"id": "1910.02677.23cbf6ab365c1eb760b565d8ba51fb3f06257d62", "question": "What are the baseline models?", "answer": ["BIBREF12 ... BIBREF33 ... BIBREF9 ... BIBREF10 ... BIBREF17 ... BIBREF15 ... BIBREF35 ... BIBREF16", "PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS"], "num_words": 3230, "gpt4_answer": "The baseline models include Phrase-Based MT system with candidate reranking, Deep semantics sentence representation fed to a monolingual MT system, Syntax-based MT model augmented using the PPDB paraphrase database, Seq2Seq trained with reinforcement learning, Seq2Seq model based on the pointer-copy mechanism, Standard Seq2Seq model, Seq2Seq with a memory-augmented Neural Semantic Encoder, and Seq2Seq integrating the simple PPDB simplification database as a dynamic memory."}
{"id": "2002.04374.6ec267f66a1c5f996519aed8aa0befb5e5aec205", "question": "Is dataset balanced in terms of available data per language?", "answer": ["No", "Yes"], "num_words": 2752, "gpt4_answer": "The dataset is balanced in terms of the number of PD patients and healthy control speakers for each language, with 50 PD patients and 50 healthy controls for Spanish and Czech, and 88 PD patients and 88 healthy controls for German."}
{"id": "2002.04374.f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e", "question": "What datasets are used?", "answer": ["PC-GITA corpus ... 88 PD patients and 88 HC speakers from Germany ... 100 native Czech speakers (50 PD, 50 HC) ", "the PC-GITA corpus BIBREF5 ... BIBREF17 ... BIBREF18"], "num_words": 2752, "gpt4_answer": "The datasets used are speech recordings of patients in three different languages: Spanish (PC-GITA corpus), German, and Czech."}
{"id": "1909.08211.a9d5f83f4b32c52105f2ae1c570f1c590ac52487", "question": "How do they split the dataset when training and evaluating their models?", "answer": ["SemEval-2017 task 8  dataset includes 325 rumorous conversation threads, and has been split into training, development and test sets. \nThe PHEME dataset provides 2,402 conversations covering nine events - in each fold, one event's conversations are used for testing, and all the rest events are used for training. ", "SemEval-2017 task 8 dataset is split into train, development and test sets. Two events go into test set and eight events go to train and development sets for every thread in the dataset. PHEME dataset is split as leave-one-event-out cross-validation. One event goes to test and the rest of events go to training set for each conversation. Nine folds are created"], "num_words": 4088, "gpt4_answer": "They split the SemEval-2017 task 8 dataset into training, development, and test sets. For the PHEME dataset, they conduct leave-one-event-out cross-validation."}
{"id": "1909.08211.288f0c003cad82b3db5e7231c189c0108ae7423e", "question": "Do they demonstrate the relationship between veracity and stance over time in the Twitter dataset?", "answer": ["No", "Yes"], "num_words": 4088, "gpt4_answer": "Yes, the authors observe that the temporal dynamics of public stances can indicate rumor veracity, and they propose to model the temporal dynamics of stance evolution with a recurrent neural network (RNN)."}
{"id": "1909.08211.562a995dfc8d95777aa2a3c6353ee5cd4a9aeb08", "question": "How much improvement does their model yield over previous methods?", "answer": ["For single-task, proposed method show\noutperform by  0.031 and 0.053 Macro-F1 for SemEval and PHEME dataset respectively.\nFor multi-task, proposed method show\noutperform by 0.049 and 0.036 Macro-F1 for SemEval and PHEME dataset respectively.", "Their model improves macro-averaged F1 by 0.017 over previous best model in Rumor Stance Classification and improves macro-averaged F1 by 0.03 and 0.015 on Multi-task Rumor Veracity Prediction on SemEval and PHEME datasets respectively"], "num_words": 4088, "gpt4_answer": "Our hierarchical framework performs much better than existing methods."}
{"id": "1908.09355.71e1f06daf6310609d00850340e64a846fbe2dfb", "question": "How many GPUs do they train their models on?", "answer": ["Unanswerable"], "num_words": 4112, "gpt4_answer": "The paper does not specify the exact number of GPUs used for training their models."}
{"id": "1908.09355.ebb4db9c24aa36db9954dd65ea079a798df80558", "question": "What of the two strategies works best?", "answer": ["PKD-Skip"], "num_words": 4112, "gpt4_answer": "PKD-Skip performs slightly better than PKD-Last, as it captures more diverse representations of richer semantics from low-level to high-level layers."}
{"id": "1908.09355.7a212a34e9dbb0ba52c40471842b2e0e3e14f276", "question": "What downstream tasks are tested?", "answer": ["Sentiment Classification ... Paraphrase Similarity Matching ... Natural Language Inference ... Machine Reading Comprehension"], "num_words": 4112, "gpt4_answer": "The downstream tasks tested are Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, and Machine Reading Comprehension."}
{"id": "2001.05540.ed15a593d64a5ba58f63c021ae9fd8f50051a667", "question": "Is this model trained in unsuperized manner?", "answer": ["No"], "num_words": 2055, "gpt4_answer": "No, the Insertion-Deletion Transformer is not trained in an unsupervised manner."}
{"id": "2001.05540.e86fb784011de5fda6ff8ccbe4ee4deadd7ee7d6", "question": "How much is BELU score difference between proposed approach and insertion-only method?", "answer": [" deletion model again increases the BLEU score over just the insertion model, by around 2 BLEU points", "Learning shifted alphabetic sequences: 21.34\nCaesar's Cipher: 2.02"], "num_words": 2055, "gpt4_answer": "The Insertion-Deletion Transformer significantly outperforms the Insertion Transformer on the alphabetic sequence task, and it increases the BLEU score by around 2 points on the Caesar's Cipher task."}
{"id": "1805.00195.d206f2cbcc3d2a6bd0ccaa3b57fece396159f609", "question": "are the protocols manually annotated?", "answer": ["Yes"], "num_words": 2924, "gpt4_answer": "Yes, the protocols were manually annotated by a team of 10 annotators."}
{"id": "1805.00195.633e2210c740b4558b1eea3f041b3ae8e0813293", "question": "what ML approaches did they experiment with?", "answer": ["MaxEnt, BiLSTM, BiLSTM+CRF", "maximum entropy ... neural network tagging model"], "num_words": 2924, "gpt4_answer": "The paper experimented with two machine learning approaches for extracting actions and entities: a maximum entropy model and a neural network tagging model."}
{"id": "1612.02695.bb7c80ab28c2aebfdd0bd90b22a55dbdf3a8ed5b", "question": "What type of attention is used in the recognition system?", "answer": ["Unanswerable", "find a set of relevant activations of the listener INLINEFORM0 and summarize them into a context"], "num_words": 2991, "gpt4_answer": "The recognition system uses an attention mechanism that employs convolutional filters over the previous attention weights."}
{"id": "1612.02695.6c4e1a1ccc0c5c48115864a6928385c248f4d8ad", "question": "What are the solutions proposed for the seq2seq shortcomings?", "answer": ["forbids emitting the EOS token ... beam search criterion can be extended to promote long transcripts ... coverage criterion prevents looping over the utterance ... ground-truth label distribution is smoothed", "label smoothing, use of coverage"], "num_words": 2991, "gpt4_answer": "The solutions proposed for the seq2seq shortcomings include label smoothing to prevent overconfidence and using a coverage term to address the problem of incomplete transcripts."}
{"id": "2002.04745.55bde89fc5822572f794614df3130d23537f7cf2", "question": "How much is training speeded up?", "answer": ["40%", "40% speed-up rate"], "num_words": 7111, "gpt4_answer": "FAILED"}
{"id": "2002.04745.523bc4e3482e1c9a8e0cb92cfe51eea92c20e8fd", "question": "What experiments do they perform?", "answer": [" experiments on two widely used tasks: the IWSLT14 German-to-English (De-En) task and the WMT14 English-to-German (En-De) task ... we train the Pre-LN BERT without the warm-up stage and compare it with the Post-LN BERT", "whether the learning rate warm-up stage is essential, whether the final model performance is sensitive to the value of Twarmup."], "num_words": 7111, "gpt4_answer": "FAILED"}
{"id": "2002.04745.6073be8b88f0378cd0c4ffcad87e1327bc98b991", "question": "What is mean field theory?", "answer": ["Unanswerable"], "num_words": 7111, "gpt4_answer": "FAILED"}
{"id": "1907.12984.f3b4e52ba962a0004064132d123fd9b78d9e12e2", "question": "Which datasets do they evaluate on?", "answer": ["2008 (NIST08) datasets ... Baidu Speech Translation Corpus (BSTC)", "NIST02 ... NIST03 ... NIST04 ... NIST05 ... NIST08"], "num_words": 6886, "gpt4_answer": "They evaluate on the NIST OpenMT08 task dataset and the Baidu Speech Translation Corpus (BSTC)."}
{"id": "1907.12984.ea6edf45f094586caf4684463287254d44b00e95", "question": "Do they compare against a system that does not use streaming text, but has the entire text at disposal?", "answer": ["Yes"], "num_words": 6886, "gpt4_answer": "Yes, they compare their model with a baseline system, which is a standard Transformer-based model that has the entire text at disposal."}
{"id": "1907.12984.ba406e07c33a9161e29c75d292c82a15503beae5", "question": "Does larger granularity lead to better translation quality?", "answer": ["It depends on the model used.", "No"], "num_words": 6886, "gpt4_answer": "Larger granularity does not necessarily lead to better translation quality, as the performance of context-aware models varies depending on the granularity of information units."}
{"id": "1911.09247.3d662fb442d5fc332194770aac835f401c2148d9", "question": "Do they report results only on English data?", "answer": ["No", "Yes"], "num_words": 3728, "gpt4_answer": "Yes, they report results only on English data."}
{"id": "1911.09247.2280ed1e2b3e99921e2bca21231af43b58ca04f0", "question": "What is the baseline method?", "answer": ["evaluate the original ill-formed question using the automatic metrics", "we also evaluate the original ill-formed question using the automatic metrics"], "num_words": 3728, "gpt4_answer": "The baseline method is evaluating the original ill-formed question using automatic metrics like BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR."}
{"id": "1911.09247.961a97149127e1123c94fbf7e2021eb1aa580ecb", "question": "What aspects are used to judge question quality?", "answer": ["Is the question grammatically correct? ... Is the spelling correct? ... Is the question an explicit question", "Is the question grammatically correct? ... Is the spelling correct? ... Is the question an explicit question, rather than a search query, a command, or a statement?"], "num_words": 3728, "gpt4_answer": "The aspects used to judge question quality are grammatical correctness, spelling correctness, and whether the question is explicit."}
{"id": "1911.09247.1e4f45c956dfb40fadb8e10d4c1bfafa8968be4d", "question": "What did the human annotations consist of?", "answer": ["Is the question grammatically correct? ... Is the spelling correct? ... Is the question an explicit question ...  annotators were asked to annotate each aspect with a binary (0/1) answer", "annotate each aspect with a binary (0/1) answer. Examples of questions provided to the annotators are in Table TABREF13"], "num_words": 3728, "gpt4_answer": "The human annotations consisted of judging the quality of questions in the DEV and TEST splits based on three aspects: grammatical correctness, spelling correctness, and whether the question is an explicit question rather than a search query, command, or statement. They also evaluated semantic equivalence for question pairs."}
{"id": "1911.09247.627ce8a1db08a732d5a8f7e1f8a72e3de89847e6", "question": "What characterizes the 303 domains? e.g. is this different subject tags?", "answer": ["The domains represent different subfields related to the topic of the questions.  ", "sub areas from Stack Exchange data dumps"], "num_words": 3728, "gpt4_answer": "The 303 domains in the MQR dataset come from 303 sub-areas of Stack Exchange, covering a broad range of topics, excluding Stack Overflow and language-specific sub-areas."}
{"id": "2003.12660.80bb07e553449bde9ac0ff35fcc718d7c161f2d4", "question": "How long is their dataset?", "answer": ["21214", "Data used has total of 23315 sentences."], "num_words": 1001, "gpt4_answer": "The dataset used for the supervised models contained 20,214 sentence pairs in the train set and 1,000 sentence pairs in the validation set."}
{"id": "2003.12660.c8f8ecac23a991bceb8387e68b3b3f2a5d8cf029", "question": "What metrics are used?", "answer": ["BLEU", "BLEU score"], "num_words": 1001, "gpt4_answer": "The metric used is the BLEU score."}
{"id": "2003.12660.28847b20ca63dc56f2545e6f6ec3082d9dbe1b3f", "question": "What is the best performing system?", "answer": ["A supervised model with byte pair encoding was the best for English to Pidgin, while a supervised model with word-level encoding was the best for Pidgin to English.", "In English to Pidgin best was byte pair encoding tokenization superised model, while in Pidgin to English word-level tokenization supervised model was the best."], "num_words": 1001, "gpt4_answer": "The best performing system is the supervised model with byte pair encoding tokenization for English to Pidgin, achieving a BLEU score of 24.29."}
{"id": "2003.12660.2d5d0b0c54105717bf48559b914fefd0c94964a6", "question": "What tokenization methods are used?", "answer": ["word-level  ... subword-level", "word-level ... Byte Pair Encoding (BPE) subword-level"], "num_words": 1001, "gpt4_answer": "The paper uses word-level and Byte Pair Encoding (BPE) subword-level tokenization methods."}
{"id": "2003.12660.dd81f58c782169886235c48b8f9a08e0954dd3ae", "question": "What baselines do they propose?", "answer": ["Transformer architecture of BIBREF7", "supervised translation models"], "num_words": 1001, "gpt4_answer": "The authors propose supervised and unsupervised neural machine translation baselines between English and Nigerian Pidgin using word-level and subword-level tokenization methods."}
{"id": "1912.08084.c138a45301713c1a9f6edafeef338ba2f99220ce", "question": "What is the size of the dataset?", "answer": ["5,415 sentences"], "num_words": 4322, "gpt4_answer": "The dataset consists of four debates with a total of 5,415 sentences."}
{"id": "1912.08084.56d788af4694c1cd1eebee0b83c585836d1f5f99", "question": "What models are trained?", "answer": ["SVM classifier with an RBF kernel ... deep feed-forward neural network (FNN) with two hidden layers (with 200 and 50 neurons, respectively) and a softmax output unit for the binary classification", "Support Vector Machines (SVM) and Feed-forward Neural Networks (FNN) "], "num_words": 4322, "gpt4_answer": "The paper trains Support Vector Machines (SVM) and Feed-forward Neural Networks (FNN) models to predict check-worthiness of claims."}
{"id": "1912.08084.34b434825f0ca3225dc8914f9da865d2b4674f08", "question": "Does the baseline use any contextual information?", "answer": ["No"], "num_words": 4322, "gpt4_answer": "No, the baseline does not use any contextual information."}
{"id": "1912.08084.61a2599acfbd3d75de58e97ecdba2d9cf0978324", "question": "What is the strong rivaling system?", "answer": ["ClaimBuster", "ClaimBuster "], "num_words": 4322, "gpt4_answer": "The strong rivaling system is the ClaimBuster system."}
{"id": "1912.08084.cf58d25bfa2561a359fdd7b6b20aef0b41dc634e", "question": "Where are the debates from?", "answer": ["four transcripts of the 2016 US election: one vice-presidential and three presidential debates", "the 2016 US presidential and vice-presidential debates"], "num_words": 4322, "gpt4_answer": "The debates are from the 2016 US election, including one vice-presidential and three presidential debates."}
{"id": "1910.12477.e86b9633dc691976dd00ed57d1675e1460f7167b", "question": "What is the state-of-the-art model in this task?", "answer": ["Unanswerable"], "num_words": 2395, "gpt4_answer": "Our method obtained an F1 score of 70.45% on the test data."}
{"id": "1910.12477.b0edb9023f35a5a02eb8fb968e880e36233e66b3", "question": "How does this result compare to other methods KB QA in CCKS2019?", "answer": ["Unanswerable"], "num_words": 2395, "gpt4_answer": "The paper does not provide a direct comparison of the proposed method's results with other methods in CCKS2019."}
{"id": "1610.00479.8c872236e4475d5d0969fb90d2df94589c7ab1c4", "question": "Do they have an elementary unit of text?", "answer": ["No"], "num_words": 7562, "gpt4_answer": "The elementary units of text in this paper are character ngrams, which are subsequences of characters in the text."}
{"id": "1610.00479.f6ba0a5cfd5b35219efe5e52b0a5b86ae85c5abd", "question": "By how much do they outpeform existing text denoising models?", "answer": ["Answer with content missing: (Table 4) Mean reciprocal rank of proposed model is 0.76 compared to 0.64 of bag-of-ngrams.", "Their F1 score outperforms an existing model by 0.017 on average for the random segmentation experiment."], "num_words": 7562, "gpt4_answer": "The paper does not provide a direct comparison to existing text denoising models in terms of performance improvement."}
{"id": "1610.00479.b21f61c0f95fefdb1bdb90d51cbba4655cd59896", "question": "In their nonsymbolic representation can they represent two same string differently depending on the context?", "answer": ["No", "Unanswerable"], "num_words": 7562, "gpt4_answer": "No, position embeddings are context-free and represent the same string with the same embedding, independent of the context in which it occurs."}
{"id": "1610.00479.0dbb5309d2be97f6eda29d7ae220aa16cafbabb7", "question": "On which datasets do they evaluate their models?", "answer": ["3 gigabyte English Wikipedia corpus", "entity dataset released by xie16entitydesc2"], "num_words": 7562, "gpt4_answer": "They evaluate their models on a 3-gigabyte English Wikipedia corpus and an entity dataset released by xie16entitydesc2."}
{"id": "1905.01347.c27b885b1e38542244f52056abf288b2389b9fc6", "question": "How do they determine demographics on an image?", "answer": [" a model-driven demographic annotation pipeline for apparent age and gender, analysis of said annotation models and the presentation of annotations for each image in the training set of the ILSVRC 2012 subset of ImageNet", "using model driven face detection, apparent age annotation and gender annotation"], "num_words": 1852, "gpt4_answer": "They determine demographics on an image by using a model-driven framework for the automatic annotation of apparent age and gender attributes in large-scale image datasets."}
{"id": "1905.01347.1ce6c09cf886df41a3d3c52ce82f370c5a30334a", "question": "Do they assume binary gender?", "answer": ["No"], "num_words": 1852, "gpt4_answer": "Yes, they assume binary gender, with continuous values between 0 and 1, and thresholding at 0.5 to define male and female gender classes."}
{"id": "1905.01347.5429add4f166a3a66bec2ba22232821d2cbafd62", "question": "What is the most underrepresented person group in ILSVRC?", "answer": ["Females and males with age 75+", "people over the age of 60"], "num_words": 1852, "gpt4_answer": "The most underrepresented person group in ILSVRC is individuals above the age of 60, accounting for only 1.71% of the dataset."}
{"id": "1710.01789.d3d6a4a721b8bc9776f62759b8d9be1a19c6b0d2", "question": "How long did the training take?", "answer": ["Unanswerable"], "num_words": 2737, "gpt4_answer": "Unanswerable"}
{"id": "1710.01789.cc8f495cac0af12054c746a5b796e989ff0e5d5f", "question": "Is the proposed model smaller or bigger than the conventional NMT system?", "answer": ["Unanswerable"], "num_words": 2737, "gpt4_answer": "The proposed double-attention model is bigger than the conventional NMT system."}
{"id": "1710.01789.64c45fdb536ae294cf06716ac20d08b5fdb7944d", "question": "Do they compare to state-of-the-art models?", "answer": ["Yes"], "num_words": 2737, "gpt4_answer": "The paper compares the two-stage NMT system with two baseline systems: a conventional SMT system (Moses) and an attention-based NMT system (reproduced from Bahdanau et al.)."}
{"id": "1711.03759.bab4ae97afd598a11d1fc7c05c6fdb98c30cafe0", "question": "how many sentences did they annotate?", "answer": ["100 sentences"], "num_words": 2206, "gpt4_answer": "They annotated 100 sentences."}
{"id": "1711.03759.f5913e37039b9517a323ec700b712e898316161b", "question": "what dataset was used in their experiment?", "answer": ["CoNLL 2003 English NER", "CoNLL 2003 English NER BIBREF8"], "num_words": 2206, "gpt4_answer": "The dataset used in their experiment is extracted from CoNLL 2003 English NER training data."}
{"id": "1711.03759.a064d01d45a33814947161ff208abb88d4353b26", "question": "what are the existing annotation tools?", "answer": ["BIBREF2 ... BIBREF3 ... BIBREF4 ... BIBREF5 ... BIBREF6 ... BIBREF7", "existing annotation tools BIBREF6 , BIBREF7"], "num_words": 2206, "gpt4_answer": "Existing annotation tools include Stanford manual annotation tool, Knowtator, WordFreak, Gate, Brat, Anafora, Atomic, and WebAnno."}
{"id": "1810.13414.3d5b4aa1ce99903b1fcd257c1e394f7990431d13", "question": "what ontologies did they use?", "answer": ["Wine Ontology ... m-piro ontology ... Disease Ontology"], "num_words": 20958, "gpt4_answer": "The paper does not provide specific details about the ontologies used in the experiments."}
{"id": "1711.04964.8d3f79620592d040f9f055b4fce0f73cc45aab63", "question": "How much improvement is given on RACE by their introduced approach?", "answer": ["1.6%", "7.3% on RACE-M and 1.5% on RACE-H"], "num_words": 5374, "gpt4_answer": "FAILED"}
{"id": "1803.04579.65e30c842e4c140a6cb8b2f9498fcc6223ed49c0", "question": "what pruning did they perform?", "answer": ["eliminate spurious training data entries", "separate algorithm for pruning out spurious logical forms using fictitious tables"], "num_words": 1278, "gpt4_answer": "They performed pruning of spurious logical forms using a separate algorithm that relies on fictitious tables."}
{"id": "1611.01116.65e26b15e087bedb6e8782d91596b35e7454b16b", "question": "Do they evaluate binary paragraph vectors on a downstream task?", "answer": ["Yes"], "num_words": 3612, "gpt4_answer": "Yes, they evaluate binary paragraph vectors on information retrieval tasks using three datasets: 20 Newsgroups, Reuters Corpus Volume 1 (RCV1), and English Wikipedia."}
{"id": "1611.01116.a8f189fad8b72f8b2b4d2da4ed8475d31642d9e7", "question": "How do they show that binary paragraph vectors capture semantics?", "answer": [" trained Binary PV-DBOW with bigrams on the English Wikipedia, and then inferred binary codes for the test parts of the 20 Newsgroups and RCV1 datasets", "They perform information-retrieval tasks on popular benchmarks"], "num_words": 3612, "gpt4_answer": "Results from the experiments indicate that binary paragraph vectors can capture semantics relevant for various domain-specific documents."}
{"id": "1611.01116.eafea4a24d103fdecf8f347c7d84daff6ef828a3", "question": "Which training dataset do they use?", "answer": [" 20 Newsgroups ... RCV1 ... English Wikipedia", "20 Newsgroups ... Reuters Corpus Volume ... English Wikipedia"], "num_words": 3612, "gpt4_answer": "They use three training datasets: 20 Newsgroups, a cleansed version of Reuters Corpus Volume 1 (RCV1), and English Wikipedia."}
{"id": "1611.01116.e099a37db801718ab341ac9a380a146c7452fd21", "question": "Do they analyze the produced binary codes?", "answer": ["No", "Yes"], "num_words": 3612, "gpt4_answer": "Yes, they analyze the produced binary codes and compare their retrieval precision with semantic hashing codes on two popular information retrieval benchmarks."}
{"id": "1807.03674.ead7704a64447dccd504951618d3be463eba86bf", "question": "How long is the dataset?", "answer": ["125383", "125383 death certificates"], "num_words": 1351, "gpt4_answer": "The dataset contains 125383 death certificates."}
{"id": "1807.03674.8476d0bf5962f4ed619a7b87415ebe28c38ce296", "question": "Do they use machine learning?", "answer": ["No", "Unanswerable"], "num_words": 1351, "gpt4_answer": "No, they used a dictionary-based approach for this task, but they plan to combine machine learning techniques with a dictionary-based approach in the future."}
{"id": "1807.03674.bbfe7e131ed776c85f2359b748db1325386c1af5", "question": "What are the ICD-10 codes?", "answer": ["International Classification of Diseases", "International Classification of Diseases, 10th revision (ICD-10) BIBREF1"], "num_words": 1351, "gpt4_answer": "The ICD-10 codes are the International Classification of Diseases, 10th revision, used for coding death certificates and medical records."}
{"id": "1811.00127.b6dae03d56dff0db8ad2a1bff9c7dd3f87551cd1", "question": "Do they release their code?", "answer": ["No"], "num_words": 2484, "gpt4_answer": "Unanswerable"}
{"id": "1811.00127.f93bad406e004014618dd64f6c604b1a9ee6a371", "question": "What media sources do they use?", "answer": ["Swedish online data from 2018 crawled by Trendiction ... manually scraped party platforms from the eight parties in parliament and Feministiskt Initiativ (Feminist Initiative)", "party platforms from the eight parties in parliament and Feministiskt Initiativ (Feminist Initiative)"], "num_words": 2484, "gpt4_answer": "The media sources used in the study include Aftonbladet, Arbetarbladet, Dala-Demokraten, Folkbladet, ETC, Arbetaren, Flamman, Bang, Offensiv, Proletren, Dagens Industri, Dalabygden, Hallands Nyheter, Axess, Svensk Tidskrift, Hemmets Vn, Dagens Nyheter, Gteborgs-Posten, Helsingborgs Dagblad, Nerikes Allehanda, Sydsvenskan, Upsala Nya Tidning"}
{"id": "1602.07563.c5ea4da3c760ba89194ad807bc1ef60e1761429f", "question": "What evidence is presented that humans perceive the sentiment classes as ordered?", "answer": ["average agreement is 18% higher with INLINEFORM7 than with INLINEFORM8", "the average agreement is 18% higher with INLINEFORM7 than with INLINEFORM8"], "num_words": 9318, "gpt4_answer": "The paper presents strong evidence that sentiment values (negative, neutral, and positive) are perceived as ordered by human annotators, based on the analysis of over 1.6 million sentiment annotated Twitter posts and the computation of annotator self-agreement and inter-annotator agreement."}
{"id": "1602.07563.4a093a9af4903a59057a4372ac1b01603467ca58", "question": "What size of dataset is sufficiently large for the model performance to approach the inter-annotator agreement?", "answer": ["150,000 labeled tweets", "around 100,000 annotations"], "num_words": 9318, "gpt4_answer": "A sufficiently large dataset for the model performance to approach the inter-annotator agreement depends on the specific dataset and domain, but in some cases, the peak performance was reached at around 150,000 labeled tweets for Polish, 70,000 tweets for Slovenian, and 50,000 tweets for Croatian."}
{"id": "1602.07563.f4e16b185b506713ff99acc4dbd9ec3208e4997b", "question": "Which measures of inter-annotator agreement are used?", "answer": ["Krippendorff's Alpha-reliability", "Krippendorff's Alpha-reliability ( INLINEFORM0 ) BIBREF6 ... F score ( INLINEFORM0 ) ... Accuracy ( INLINEFORM0 ) ... Accuracy within 1 ( INLINEFORM0 )"], "num_words": 9318, "gpt4_answer": "Various annotator agreement measures are used to quantify the quality of training data in the paper."}
{"id": "1602.07563.4683812cba21c92319be68c03260b5a8175bbb6e", "question": "What statistical test(s) is used to compare the top classification models?", "answer": [" Friedman-Nemenyi test", " Friedman-Nemenyi test BIBREF14 , BIBREF15"], "num_words": 9318, "gpt4_answer": "The Friedman-Nemenyi test is used to compare the top classification models."}
{"id": "1908.10001.c25014b7e57bb2949138d64d49f356d69838bc25", "question": "What is the baseline?", "answer": ["a rule-based unigram matching baseline, which takes the entry with highest unigram overlap with the query string to be the top match", "rule-based unigram matching baseline"], "num_words": 2424, "gpt4_answer": "The baseline is a rule-based unigram matching model, which takes the entry with the highest unigram overlap with the query string to be the top match."}
{"id": "1908.10001.25a8d432bf94af1662837877bc6c284e2fc3fbe2", "question": "How is their NER model trained?", "answer": ["Trained using SpaCy and fine-tuned with their data of hotel and location entities", "Using SpaCy"], "num_words": 2424, "gpt4_answer": "The NER model is trained using SpaCy, initialized with SpaCy's English NER model, and fine-tuned using their data, consisting of 21K messages labeled with hotel and location entities."}
{"id": "1908.10001.be632f0246c2e5f049d12e796812f496e083c33e", "question": "Do they use pretrained word embeddings such as BERT?", "answer": ["Yes"], "num_words": 2424, "gpt4_answer": "Yes, they use pretrained word embeddings such as BERT for their information retrieval model."}
{"id": "1908.10001.415b42ef6ff92553d04bd44ed0cbf6b3d6c83e51", "question": "How well does the system perform?", "answer": ["F1 score of 0.96 on recognizing both hotel and location entities and Top-1 recall of 0.895 with the IR BERT model", "For NER,  combined entity model achieves the best performance (F1 0.96). For IR, BERT+fine-tuning model achieves TOP-1 Recall 0.895 and Top-3 Recall 0.961."], "num_words": 2424, "gpt4_answer": "The BERT with fine-tuning model retrieves the correct match for nearly 90% of queries in the information retrieval task."}
{"id": "1908.10001.9da181ac8f2600eb19364c1b1e3cdeb569811a11", "question": "Where does their information come from?", "answer": ["From conversions between users and customer support agents through their partners, and professional annotators creating data.", "Information  from users and information  from database of approximately 100,000 cities and 300,000 hotels, populated using data from their partners."], "num_words": 2424, "gpt4_answer": "The hotel search information comes from a database of approximately 100,000 cities and 300,000 hotels, populated using data from their partners."}
{"id": "1908.10001.67f1b8a9f72e62cd74ec42e9631ef763a9b098c7", "question": "What intents do they have?", "answer": ["The most common intents are thanks, cancel, stop, search, and unknown", "thanks ... cancel ... stop ... search ... unknown "], "num_words": 2424, "gpt4_answer": "The most common intents are thanks, cancel, stop, search, and unknown."}
{"id": "1911.06919.9a6bf1d481e6896eef9f8fed835d9d29658ede36", "question": "Is proposed approach compared to some baselines?", "answer": ["Yes"], "num_words": 3619, "gpt4_answer": "Yes, the proposed approach is compared to baselines in both abstractive summarization and popularity prediction of online petitions."}
{"id": "1911.06919.4999da863ecbd40378505bfb1f4e395061a3f559", "question": "What datasets are used for this tasks?", "answer": [" US Petition dataset ... CNN/DailyMail corpus ... 385 documents from the Wall Street Journal", "CNN/DailyMail corpus ... US Petition dataset"], "num_words": 3619, "gpt4_answer": "The datasets used for the tasks are the anonymized CNN/DailyMail corpus for abstractive summarization and the US Petition dataset for popularity prediction of online petitions."}
{"id": "1911.06919.3098793595252039f363ee1150d4ea956f2504b8", "question": "How big are improvements on these tasks?", "answer": ["Observing that our model generally has better recall (Table TABREF17) and its summaries tend to be more verbose (e.g. second example in Figure FIGREF20) ...  We are able to reproduce the performance of the baseline model (CNN w/ GloVe), and find that once again, adding the shallow discourse features improves results.", "Unanswerable"], "num_words": 3619, "gpt4_answer": "The improvements in the tasks are generally consistent but task-dependent, with the best method of incorporation and feature type (shallow or latent) varying between tasks."}
{"id": "1710.10380.99f898eb91538cb82bc9a00892d54ae2a740961e", "question": "Which downstream tasks are considered?", "answer": ["SICK ... MSRP ... TREC ... MR ... SST ... CR ... SUBJ ... MPQA ... STS14 ... SNLI", "semantic relatedness (SICK, BIBREF17 ), paraphrase detection (MSRP, BIBREF19 ), question-type classification (TREC, BIBREF20 ), and five benchmark sentiment and subjective datasets, which include movie review sentiment (MR, BIBREF21 , SST, BIBREF22 ), customer product reviews (CR, BIBREF23 ), subjectivity/objectivity classification (SUBJ, BIBREF24 ), opinion polarity (MPQA, BIBREF25 ), semantic textual similarity (STS14, BIBREF18 ), and SNLI BIBREF13"], "num_words": 4585, "gpt4_answer": "The downstream tasks considered include semantic relatedness (SICK), paraphrase detection (MSRP), question-type classification (TREC), movie review sentiment (MR, SST), customer product reviews (CR), subjectivity/objectivity classification (SUBJ), opinion polarity (MPQA), semantic textual similarity (STS14), and SNLI."}
{"id": "1710.10380.cf68906b7d96ca0c13952a6597d1f23e5184c304", "question": "How long are the two unlabelled corpora?", "answer": ["71000000, 142000000", "Amazon Review dataset BIBREF26 with 142 million sentences, about twice as large as BookCorpus"], "num_words": 4585, "gpt4_answer": "The paper mentions two different large unlabelled corpora but does not provide their exact length."}
{"id": "1911.11025.3e5162e6399c7d03ecc7007efd21d06c04cf2843", "question": "Do the authors report only on English data?", "answer": ["Yes"], "num_words": 2934, "gpt4_answer": "Yes, the authors limit the streaming to English as their text analysis models are trained on English language corpora."}
{"id": "1911.11025.bd255aadf099854541d06997f83a0e478f526120", "question": "How is the impact of ParityBOT analyzed?", "answer": [" interviewing individuals involved in government ($n=5$)", "by interviewing individuals involved in government"], "num_words": 2934, "gpt4_answer": "The impact of ParityBOT is analyzed through qualitative assessment, where interviews are conducted with individuals involved in government to evaluate the social impact of the system."}
{"id": "1911.11025.a9ff35f77615b3a4e7fd7b3a53d0b288a46f06ce", "question": "What public online harassment datasets was the system validated on?", "answer": [" unique tweets identified as either hateful and not hateful from previous research BIBREF22", "20194 cleaned, unique tweets identified as either hateful and not hateful from previous research BIBREF22"], "num_words": 2934, "gpt4_answer": "The paper does not specify the exact public online harassment datasets used for validation."}
{"id": "1911.11025.69a46a227269c3aac9bf9d7c3d698c787642f806", "question": "Where do the supportive tweets about women come from? Are they automatically or manually generated?", "answer": ["Manualy (volunteers composed them)", "Volunteers submitted many of these positivitweets through an online form"], "num_words": 2934, "gpt4_answer": "The supportive tweets, or \"positivitweets,\" are composed by volunteers. These tweets express encouragement, state facts about women in politics, and aim to inspire and uplift the community. Volunteers submit many of these positivitweets through an online form, and they are reviewed, copy-edited, and fact-checked before being used by ParityBOT."}
{"id": "1911.11025.ebe6b8ec141172f7fea66f0a896b3124276d4884", "question": "How are the hateful tweets aimed at women detected/classified?", "answer": ["The text analysis models classify a tweet by using, as features, the outputs from Perspective API from Jigsaw BIBREF10, HateSonar BIBREF11, and VADER sentiment models BIBREF12", "classify a tweet by using, as features, the outputs from Perspective API from Jigsaw BIBREF10, HateSonar BIBREF11, and VADER sentiment models BIBREF12"], "num_words": 2934, "gpt4_answer": "ParityBOT uses text analysis models to classify tweets, including outputs from Perspective API, HateSonar, and VADER sentiment models. The most consistently predictive feature for classifying hateful tweets is the TOXICITY probability from Perspective API."}
{"id": "1911.01940.946d7c877d363f549f84e9500c852dce70ae5d36", "question": "How many GPUs do they use for this task?", "answer": ["Unanswerable"], "num_words": 2902, "gpt4_answer": "Unanswerable"}
{"id": "1911.01940.26e32f24fe0c31ef25de78935daa479534b9dd58", "question": "Do they use all the hidden layer representations?", "answer": ["Yes"], "num_words": 2902, "gpt4_answer": "Yes, they use all the hidden layer representations in the HIdden Representation Extractor (HIRE) to dynamically generate complementary information."}
{"id": "1708.07252.22375aac4cbafd252436b756bdf492a05f97eed8", "question": "What languages are used for the experiments?", "answer": ["English ... French", "Unanswerable"], "num_words": 6264, "gpt4_answer": "The experiments are performed on the Brown Corpus, which is an English language text corpus."}
{"id": "1708.07252.d2f91303cec132750a416192f67c8ac1d3cf6fc0", "question": "What is the caching mechanism?", "answer": ["The cache language models are based on the assumption that the word in recent history are more likely to appear again ... conditional probability of a word is calculated by interpolating the output of standard language model and the probability evaluated by caching", "store the outputs and states of language models for future prediction given the same contextual history"], "num_words": 6264, "gpt4_answer": "The caching mechanism in language models is based on the assumption that words in recent history are more likely to appear again. It calculates the conditional probability of a word by interpolating the output of the standard language model and the probability evaluated by caching."}
{"id": "1708.07252.9f065e787a0d40bb4550be1e0d64796925459005", "question": "What language model architectures are examined?", "answer": ["FNNLM ... RNNLM ... BiRNN ... LSTM", "RNNLM ... LSTM-RNN ... FNNLM"], "num_words": 6264, "gpt4_answer": "The paper examines feed-forward neural network language model (FNNLM), recurrent neural network language model (RNNLM), and long-short term memory (LSTM) RNNLM."}
{"id": "1708.07252.e6f5444b7c08d79d4349e35d5298a63bb30e7004", "question": "What directions are suggested to improve language models?", "answer": ["Improved architecture for ANN, use of linguistical properties of words or sentences as features.", "gate recurrent unit (GRU) RNNLM, dropout strategy for addressing overfitting, character level neural network language model and ect."], "num_words": 6264, "gpt4_answer": "Some directions for improving neural network language modeling include exploring different model architectures, incorporating bidirectional recurrent neural networks, using word classes, caching, and importance sampling."}
{"id": "1808.07733.59f41306383dd6e201bded0f1c7c959ec4f61c5a", "question": "What logic rules can be learned using ELMo?", "answer": ["1).But   2).Eng  3). A-But-B", "A-but-B and negation"], "num_words": 1833, "gpt4_answer": "ELMo implicitly learns logic rules for sentiment classification of complex sentences like A-but-B sentences."}
{"id": "1808.07733.b3432f52af0b95929e6723dd1f01ce029d90a268", "question": "Does Elmo learn all possible logic rules?", "answer": ["No", "Yes"], "num_words": 1833, "gpt4_answer": "The paper provides evidence that ELMo implicitly learns logic rules for sentiment classification of complex sentences like A-but-B sentences, but it does not claim that ELMo learns all possible logic rules."}
{"id": "1902.04094.6b1a6517b343fdb79f246955091ff25e440b9511", "question": "Which metrics are used for evaluating the quality?", "answer": ["BLEU ...  perplexity", "BLEU ... perplexity ...  self-BLEU ... percentage of $n$ -grams that are unique"], "num_words": 1064, "gpt4_answer": "The evaluation metrics used are BLEU, perplexity of a trained language model, self-BLEU, and the percentage of unique n-grams."}
{"id": "1912.04979.5f25b57a1765682331e90a46c592a4cea9e3a336", "question": "Are face tracking, identification, localization etc multimodal inputs in some ML model or system is programmed by hand?", "answer": ["Face tracking is performed in an automatic tracklet module, face identification is performed by creating a face embedding from the output of a CNN, the embedding is then compared to a gallery of each person's face using a discriminative classifier (SVM) and localization is modelled with a complex angular central Gaussian model. All are merged in a statistical model. ", "Input in ML model"], "num_words": 5610, "gpt4_answer": "Face tracking, identification, and localization are integrated using a speaker diarization module that combines face tracking, face identification, speaker identification, sound source localization (SSL), and the time-frequency (TF) masks generated by the continuous speech separation (CSS) module. The system is not solely programmed by hand, as it utilizes machine learning models for speaker identification and face identification."}
{"id": "1912.04979.2ba2ff6c21a16bd295b07af1ef635b3b4c5bd17e", "question": "What are baselines used?", "answer": ["A diarization system using only face identification and SSL", "The baseline system was a conventional speech recognition approach using single-output beamforming."], "num_words": 5610, "gpt4_answer": "Unanswerable"}
{"id": "1712.00733.74acaa205a5998af4ad7edbed66837a6f2b5c58b", "question": "What are the baselines for this paper?", "answer": ["Ablated versions of the full model (without external knowledge, without memory network); alternative VQA methods: LSTM-Att, MemAUG, MCB+Att, MLAN", "LSTM with attention, memory augmented model, ", "LSTM-Att BIBREF7 , a LSTM model with spatial attention ... MemAUG BIBREF33 : a memory-augmented model for VQA ... MCB+Att BIBREF6 : a model combining multi-modal features by Multimodal Compact Bilinear pooling ... MLAN BIBREF11 : an advanced multi-level attention model"], "num_words": 5452, "gpt4_answer": "The baselines for this paper are LSTM-Att, MemAUG, MCB+Att, and MLAN."}
{"id": "1712.00733.cfcf94b81589e7da215b4f743a3f8de92a6dda7a", "question": "What VQA datasets are used for evaluating this task? ", "answer": ["Visual7W ... a collection of open-domain visual question-answer pairs", "Visual7W and an automatically constructed open-domain VQA dataset"], "num_words": 5452, "gpt4_answer": "The Visual7W dataset is used for evaluating this task."}
{"id": "1712.00733.d147117ef24217c43252d917d45dff6e66ff807c", "question": "How do they model external knowledge? ", "answer": ["Word embeddings from knowledge triples (subject, rel, object) from ConceptNet are fed to an RNN"], "num_words": 5452, "gpt4_answer": "They model external knowledge by embedding the entities and relations of a Knowledge Graph (KG) into a continuous vector space, treating each knowledge triple as a three-word SVO (subject, verb, object) phrase, and embedding it into a feature space using an RNN architecture."}
{"id": "1712.00733.1a2b69dfa81dfeadd67b133229476086f2cc74a8", "question": "What type of external knowledge has been used for this paper? ", "answer": ["ConceptNet, which contains common-sense relationships between daily words", "an open multilingual knowledge graph containing common-sense relationships between daily words"], "num_words": 5452, "gpt4_answer": "The paper uses external knowledge mined from ConceptNet, an open multilingual knowledge graph containing common-sense relationships between daily words, to aid the reasoning of open-domain VQA."}
{"id": "1905.07894.6d6a9b855ec70f170b854baab6d8f7e94d3b5614", "question": "What is the proposed algorithm or model architecture?", "answer": ["Hybrid Fusion ... Late Fusion ... Early Fusion", "They combine content- and graph-based methods in new ways."], "num_words": 4168, "gpt4_answer": "The proposed method combines content- and graph-based features for abusive language detection in online conversations. It uses Support Vector Machine (SVM) classifiers and explores three fusion strategies: Early Fusion, Late Fusion, and Hybrid Fusion."}
{"id": "1905.07894.870358f28a520cb4f01e7f5f780d599dfec510b4", "question": "Do they attain state-of-the-art performance?", "answer": ["No"], "num_words": 4168, "gpt4_answer": "Yes, the fusion methods combining content- and graph-based features improve classification performance, with the Late Fusion strategy achieving a final F-measure of 93.26%."}
{"id": "1905.07894.98aa86ee948096d6fe16c02c1e49920da00e32d4", "question": "What fusion methods are applied?", "answer": ["Early Fusion ... Late Fusion ... Hybrid Fusion", "Early fusion, late fusion, hybrid fusion."], "num_words": 4168, "gpt4_answer": "The fusion methods applied are Early Fusion, Late Fusion, and Hybrid Fusion."}
{"id": "1905.07894.c463136ba9a312a096034c872b5c74b9d58cef95", "question": "What graph-based features are considered?", "answer": ["Coreness Score, PageRank Centrality, Strength Centrality, Vertex Count, Closeness Centrality, Authority Score, Hub Score, Reciprocity, Closeness Centrality", "Top graph based features are: Coreness Score, PageRank Centrality, Strength Centrality, Vertex Count, Closeness Centrality, Closeness Centrality, Authority Score, Hub Score, Reciprocity and Closeness Centrality."], "num_words": 4168, "gpt4_answer": "The graph-based features considered include Coreness, Strength, Closeness, Reciprocity, and Eccentricity, which help detect changes in the direct neighborhood of the targeted author, average node centrality at the level of the whole graph in terms of distance, and general reciprocity of exchanges between users."}
{"id": "1909.05372.81e101b2c803257492d67a00e8a1d9a07cbab136", "question": "How does Overton handles contradictory or incomplete supervision data?", "answer": ["Overton learns the accuracy of these sources using ideas from the Snorkel project", "Overton uses techniques from Snorkel BIBREF1 and Google's Snorkel DryBell BIBREF0"], "num_words": 5287, "gpt4_answer": "Overton handles contradictory or incomplete supervision data by using techniques from Snorkel and Google's Snorkel DryBell, which help in combining supervision by estimating the accuracy of data sources and computing the probability that each training point is correct. This information is incorporated into the loss function for a task, allowing Overton to automatically handle common issues like rebalancing classes."}
{"id": "1909.05372.b942d94e4187e4fdc706cfdf92e3a869fc294911", "question": "What are high level declarative abstractions Overton provides?", "answer": ["Code-free Deep Learning  ... Multitask Learning ...  Weak Supervision", "data payloads ... model tasks"], "num_words": 5287, "gpt4_answer": "Overton provides high-level declarative abstractions that automate the life cycle of model construction, deployment, and monitoring, allowing engineers to build, maintain, and monitor their applications by manipulating data files instead of custom code."}
{"id": "1909.05372.8ffae517bc0efa453b7e316d41bd9f1b6679b158", "question": "How are applications presented in Overton?", "answer": ["Applications are customized by providing supervision in a data file that conforms to the schema", "by manipulating data filesnot custom code"], "num_words": 5287, "gpt4_answer": "In Overton, applications are presented by providing a schema and a data file. The schema specifies tasks and payloads, while the data file contains supervision data that conforms to the schema. Overton compiles these inputs into a multitask deep model."}
{"id": "1909.05372.0fd2854dd8d8191f00c8d12483b5a81a04de859f", "question": "Does Overton support customizing deep learning models without writing any code?", "answer": ["No", "Yes"], "num_words": 5287, "gpt4_answer": "Yes, Overton supports building deep-learning-based applications without writing any code in frameworks like TensorFlow."}
{"id": "1610.09722.742d5e182b57bfa5f589fde645717ed0ac3f49c2", "question": "what metrics are used to evaluate the models?", "answer": ["modified version of standard precision, recall, and F INLINEFORM0 , as proposed by reschke2014 ... mean reciprocal rank (MRR)", "precision ... recall ... mean reciprocal rank ... F INLINEFORM0"], "num_words": 4054, "gpt4_answer": "The models are evaluated using modified precision, recall, and F1 score, standard precision, recall, and F1 score for null values, and mean reciprocal rank (MRR)."}
{"id": "1610.09722.726c5c1b6951287f4bae22978f9a91d22d9bef61", "question": "what are the baselines?", "answer": ["Reschke CRF ... Reschke Noisy-OR ... Reschke Best"], "num_words": 4054, "gpt4_answer": "The baselines are Reschke CRF, Reschke Noisy-OR, and Reschke Best, which are three notable examples from previous work in event extraction."}
{"id": "1610.09722.dfdd309e56b71589b25412ba85b0a5d79a467ceb", "question": "what is the size of the dataset?", "answer": ["80 plane crash events", "80 plane crash events, each paired with a set of related news articles"], "num_words": 4054, "gpt4_answer": "The dataset consists of 80 plane crash events, with 40 reserved for training and 40 for testing."}
{"id": "1610.09722.7ae95716977d39d96e871e552c35ca0753115229", "question": "what dataset did they use?", "answer": ["Event dataset with news articles", "Stanford Plane Crash Dataset BIBREF15"], "num_words": 4054, "gpt4_answer": "The authors used the Stanford Plane Crash Dataset for their experiments."}
{"id": "1908.05925.ff3e93b9b5f08775ebd1a7408d7f0ed2f6942dde", "question": "How is the quality of the translation evaluated?", "answer": ["The performances of our final model and other baseline models are illustrated in Table TABREF34.", "They report the scores of several evaluation methods for every step of their approach."], "num_words": 2822, "gpt4_answer": "The quality of the translation is evaluated using the BLEU (cased) score."}
{"id": "1908.05925.59a3d4cdd1c3797962bf8d72c226c847e06e1d44", "question": "What are the post-processing approaches applied to the output?", "answer": ["Special Token Replacement ... Quotes Fixing ... Recaser ...  Patch-up", "unknown words replacement"], "num_words": 2822, "gpt4_answer": "The post-processing approaches applied to the output include special token replacement, quotes fixing, recasing, and patch-up for name entities."}
{"id": "1908.05925.49474a3047fa3f35e1bcd63991e6f15e012ac10b", "question": "Is the MUSE alignment independently evaluated?", "answer": ["No"], "num_words": 2822, "gpt4_answer": "Unanswerable"}
{"id": "1908.05925.63279ecb2ba4e51c1225e63b81cb021abc10d0d1", "question": "How does byte-pair encoding work?", "answer": ["Unanswerable"], "num_words": 2822, "gpt4_answer": "Byte-pair encoding (BPE) works by iteratively merging the most frequent pairs of characters or symbols in a text to create new subword units, which helps in reducing vocabulary size and tackling the out-of-vocabulary problem."}
{"id": "1909.03553.f1a50f88898556ecdba8e9cac13ae54c11835945", "question": "How many general qualitative statements are in dataset?", "answer": ["400 qualitative knowledge sentences", "background corpus of 400 qualitative knowledge sentences"], "num_words": 2026, "gpt4_answer": "The dataset contains 400 general qualitative statements."}
{"id": "1909.03553.ef6304512652ba56bd13dbe282a5ce1d41a4f171", "question": "What are state-of-the-art models on this dataset?", "answer": [" BERT (IR) ... BERT (IR upper bound) ... BERT-PFT (no knowledge) ... BERT-PFT (IR)", "BERT-PFT (IR)"], "num_words": 2026, "gpt4_answer": "State-of-the-art models on the QuaRTz dataset are BERT-based models, specifically BERT-Sci, BERT (IR), BERT (IR upper bound), BERT-PFT (no knowledge), and BERT-PFT (IR)."}
{"id": "1909.03553.72dbdd11b655b25b2b254e39689a7d912f334b71", "question": "How are properties being compared annotated?", "answer": ["crowdworkers were shown a seed sentence $K_i$, and asked to annotate the two properties being compared using the template", "qualitative relations were manually extracted by the authors from a large corpus ... asked to annotate the two properties being compared ... asked to author a situated, 2-way multiple-choice (MC) question that tested this relationship ... asked to validate its answer and quality ... asked to generate a new question by flipping the original so the answer switched"], "num_words": 2026, "gpt4_answer": "Properties being compared are annotated using templates, where crowdworkers fill in the property expressions from the seed sentence $K_i$ and the question $Q_i$."}
{"id": "2003.11531.9b6339e24f58b576143d2adf599cfc4a31fd3b0c", "question": "What state-of-the-art tagging model did they use?", "answer": ["Span-Attribute Tagging (SAT) model", "the Span-Attribute Tagging (SAT) model"], "num_words": 5568, "gpt4_answer": "The paper used the Span-Attribute Tagging (SAT) model for their tasks."}
{"id": "1709.06136.55e3daecaf8030ed627e037992402dd0a7dd67ff", "question": "By how much do they improve upon supervised traning methods?", "answer": ["A2C and REINFORCE-joint for joint policy optimization achieve the improvement over SL baseline of 29.4% and  25.7 susses rate,  1.21  AND 1.28 AvgRevard and  0.25 and  -1.34 AvgSucccess Turn Size, respectively.", "agent-update-only models using REINFORCE and A2C achieve similar results, outperforming the baseline model by 14.9% and 15.3% respectively ...  jointly optimized models improved the performance further"], "num_words": 3887, "gpt4_answer": "The jointly optimized models using deep RL improve the task success rate by up to 15.3% over the supervised training baseline model."}
{"id": "1709.06136.5522a9eeb06221722052e3c38f9b0d0dbe7c13e6", "question": "Do they jointly optimize both agents?", "answer": ["Yes"], "num_words": 3887, "gpt4_answer": "Yes, they propose an iterative policy learning method that jointly optimizes the dialog agent and the user simulator with deep RL"}
{"id": "1709.06136.30870a962cf88ac8c8e6b7b795936fd62214f507", "question": "Which neural network architecture do they use for the dialog agent and user simulator?", "answer": ["LSTM", "Similar to the design of the dialog agent, state of the user simulator is maintained in the state of an LSTM. ... State of the dialog agent is maintained in the LSTM BIBREF35"], "num_words": 3887, "gpt4_answer": "They use LSTM (Long Short-Term Memory) neural networks for the dialog agent and user simulator."}
{"id": "1709.06136.7ece07a84635269bb19796497847e4517d1e3e61", "question": "Do they create the basic dialog agent and basic user simulator separately?", "answer": ["Yes"], "num_words": 3887, "gpt4_answer": "Yes, they first bootstrap a basic dialog agent and a basic user simulator separately by learning directly from dialog corpora with supervised training."}
{"id": "2003.08769.f94cea545f745994800c1fb4654d64d1384f2c26", "question": "Is this done in form of unsupervised (clustering) or suppervised learning?", "answer": ["Supervised methods are used to identify the dish and ingredients in the image, and an unsupervised method (KNN) is used to create the food profile.", "Unsupervised"], "num_words": 2013, "gpt4_answer": "This is done in the form of supervised learning, using the K Nearest Neighbors Model."}
{"id": "2003.08769.f3b851c9063192c86a3cc33b2328c02efa41b668", "question": "Does this study perform experiments to prove their claim that indeed personalized profiles will have inclination towards particular cuisines?", "answer": ["The study features a radar chart describing inclinations toward particular cuisines, but they do not perform any experiments", "Yes"], "num_words": 2013, "gpt4_answer": "Yes, the study performs experiments using a data pipeline, a rudimentary method of classification, and a KNN model for classification to determine a user's preference towards particular cuisines based on their food images."}
{"id": "1909.13466.54b25223ab32bf8d9205eaa8a570e99c683f0077", "question": "What baselines do they compare to?", "answer": ["A neural encoder-decoder architecture with attention using LSTMs or Transformers", "a encoder-decoder architecture with attention incorporating LSTMs and transformers"], "num_words": 6115, "gpt4_answer": "They compare to two baselines: the LSTM and the transformer models."}
{"id": "1909.13466.e5be900e70ea86c019efb06438ba200e11773a7c", "question": "What training set sizes do they use?", "answer": ["219,777 ... 114,243 ... 89,413 ... over 5M ", "89k, 114k, 291k, 5M"], "num_words": 6115, "gpt4_answer": "The training set sizes for the experiments vary from tens of thousands to millions of sentences."}
{"id": "1909.13466.b36a8a73b3457a94203eed43f063cb684a8366b7", "question": "What languages do they experiment with?", "answer": ["German ... English ... French ... Czech ... Basque", "German-English, English-French, Czech-English, Basque-English pairs"], "num_words": 6115, "gpt4_answer": "The paper experiments with four language pairs: German-English (de-en), English-French (en-fr), Czech-English (cs-en), and Basque-English (eu-en)."}
{"id": "1910.09916.3d73cb92d866448ec72a571331967da5d34dfbb1", "question": "What language model is trained?", "answer": ["ULMFiT", "ULMFiT BIBREF21"], "num_words": 4105, "gpt4_answer": "The language model trained in this study is ULMFiT."}
{"id": "1910.09916.708f5f83a3c356b23b27a9175f5c35ac00cdf5db", "question": "What machine learning models are considered?", "answer": ["RandomForestRegressor ... LinearSVR ... KNeighborsRegressor", "RandomForestRegressor ... LinearSVR ... KNeighborsRegressor ... Support Vector Machine Regression"], "num_words": 4105, "gpt4_answer": "The paper considers Support Vector Regression (SVR) and pre-trained Language Model (LM) with transfer learning as the machine learning models."}
{"id": "1910.09916.9240ee584d4354349601aeca333f1bc92de2165e", "question": "What is the agreement of the dataset?", "answer": ["Answer with content missing: (Table 2): Krippednorff's alpha coefficient for dataset is: Stability -0.26, Extraversion 0.07, Openness 0.36, Agreeableness 0.51, Conscientiousness 0.31", "Unanswerable"], "num_words": 4105, "gpt4_answer": "The intra-annotator reliability of both datasets $D_\\textrm {\\textit {LR}}$ and $D_\\textrm {\\textit {HR}}$ is calculated using the Krippendorff's alpha coefficient."}
{"id": "1803.05160.9133a85730c4090fe8b8d08eb3d9146efe7d7037", "question": "Do the authors offer any potential reasons why cross-validation variants tend to overestimate the performance, while the sequential methods tend to underestimate it?", "answer": ["No", "Yes"], "num_words": 5638, "gpt4_answer": "The authors do not provide explicit reasons for why cross-validation variants tend to overestimate the performance, while the sequential methods tend to underestimate it."}
{"id": "1803.05160.42279c3a202a93cfb4aef49212ccaf401a3f8761", "question": "Which three variants of sequential validation are examined?", "answer": ["9:1 training:test ratio, 20 equidistant samples ... 9:1 training:test ratio, 10 equidistant samples ... 2:1 training:test ratio, 10 samples randomly selected out of 20 equidistant points", "seq(9:1, 20, equi) - 9:1 training:test ratio, 20 equidistant samples,\n\n ... seq(9:1, 10, equi) - 9:1 training:test ratio, 10 equidistant samples,\n\n ... seq(2:1, 10, semi-equi) - 2:1 training:test ratio, 10 samples randomly selected out of 20 equidistant points.\n\n"], "num_words": 5638, "gpt4_answer": "seq(9:1, 20, equi) - 9:1 training:test ratio, 20 equidistant samples,\nseq(9:1, 10, equi) - 9:1 training:test ratio, 10 equidistant samples,\nseq(2:1, 10, semi-equi) - 2:1 training:test ratio, 10 samples randomly selected out of 20 equidistant points."}
{"id": "1803.05160.9ca85242ebeeafa88a0246986aa760014f6094f2", "question": "Which three variants of cross-validation are examined?", "answer": ["10-fold, stratified, blocked; ... 10-fold, not stratified, blocked; ... 10-fold, stratified, random selection of examples.", "xval(9:1, strat, block) - 10-fold, stratified, blocked;\n\n ... xval(9:1, no-strat, block) - 10-fold, not stratified, blocked;\n\n ... xval(9:1, strat, rand) - 10-fold, stratified, random selection of examples.\n\n"], "num_words": 5638, "gpt4_answer": "The three variants of cross-validation examined are: 1) xval(9:1, strat, block) - 10-fold, stratified, blocked; 2) xval(9:1, no-strat, block) - 10-fold, not stratified, blocked; and 3) xval(9:1, strat, rand) - 10-fold, stratified, random selection of examples."}
{"id": "1803.05160.8641156c4d67e143ebbabbd79860349242a11451", "question": "Which European languages are targeted?", "answer": ["Albanian\nBulgarian\nEnglish\nGerman\nHungarian\nPolish\nPortuguese\nRussian\nSer/Cro/Bos\nSlovak\nSlovenian\nSpanish\nSwedish", "Albanian, Bulgarian, English, German, Hungarian, Polish, Portughese, Russian, Serbian, Slovak, Slovenian, Spanish, Swedish"], "num_words": 5638, "gpt4_answer": "The paper targets 13 European languages for sentiment classification of Twitter data."}
{"id": "1803.05160.2a120f358f50c377b5b63fb32633223fa4ee2149", "question": "In what way are sentiment classes ordered?", "answer": ["negative ... neutral ... positive", "time-ordered"], "num_words": 5638, "gpt4_answer": "Sentiment classes are ordered as negative, neutral, and positive."}
