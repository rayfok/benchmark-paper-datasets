{"id": "1909.00694", "title": "Minimally Supervised Learning of Affective Events Using Discourse Relations", "abstract": "Recognizing affective events that trigger positive or negative sentiment has a wide range of natural language processing applications but remains a challenging problem mainly because the polarity of an event is not necessarily predictable from its constituent words. In this paper, we propose to propagate affective polarity using discourse relations. Our method is simple and only requires a very small seed lexicon and a large raw corpus. Our experiments using Japanese data show that our method learns affective events effectively without manually labeled data. It also improves supervised learning results when labeled data are small.", "questions": "1. What are the natural language processing applications of recognizing affective events?\n   - Related phrase: \"natural language processing applications\"\n\n2. Why is the polarity of an event not necessarily predictable from constituent words?\n   - Related phrase: \"not necessarily predictable\"\n\n3. How does the method propagate affective polarity using discourse relations?\n   - Related phrase: \"propagate affective polarity\"\n\n4. What is the composition of the small seed lexicon used in the method?\n   - Related phrase: \"small seed lexicon\"\n\n5. What are the characteristics of the large raw corpus used in the study?\n   - Related phrase: \"large raw corpus\"\n\n6. How does the method perform on Japanese data when compared to other approaches?\n   - Related phrase: \"Japanese data\"\n\n7. In what ways does the method improve supervised learning results with small labeled data?\n   - Related phrase: \"improves supervised learning\""}
{"id": "2003.07723", "title": "PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry", "abstract": "Most approaches to emotion analysis regarding social media, literature, news, and other domains focus exclusively on basic emotion categories as defined by Ekman or Plutchik. However, art (such as literature) enables engagement in a broader range of more complex and subtle emotions that have been shown to also include mixed emotional responses. We consider emotions as they are elicited in the reader, rather than what is expressed in the text or intended by the author. Thus, we conceptualize a set of aesthetic emotions that are predictive of aesthetic appreciation in the reader, and allow the annotation of multiple labels per line to capture mixed emotions within context. We evaluate this novel setting in an annotation experiment both with carefully trained experts and via crowdsourcing. Our annotation with experts leads to an acceptable agreement of kappa=.70, resulting in a consistent dataset for future large scale analysis. Finally, we conduct first emotion classification experiments based on BERT, showing that identifying aesthetic emotions is challenging in our data, with up to .52 F1-micro on the German subset. Data and resources are available at https://github.com/tnhaider/poetry-emotion", "questions": "1. What are the basic emotion categories defined by Ekman or Plutchik?\n   - Related phrase: \"basic emotion categories\"\n\n2. How are aesthetic emotions conceptualized in this study?\n   - Related phrase: \"conceptualize a set\"\n\n3. What methodology was used for annotation of multiple labels per line?\n   - Related phrase: \"allow the annotation\"\n\n4. How were the trained experts and crowdsourcing participants selected for the annotation experiment?\n   - Related phrase: \"trained experts and via crowdsourcing\"\n\n5. What was the specific annotation process followed in the experiment?\n   - Related phrase: \"annotation experiment\"\n\n6. Can you provide details on the BERT-based emotion classification experiments?\n   - Related phrase: \"emotion classification experiments\"\n\n7. What were the main challenges in identifying aesthetic emotions in the data?\n   - Related phrase: \"identifying aesthetic emotions\"\n\n8. How was the performance of the classification model measured?\n   - Related phrase: \".52 F1-micro\"\n\n9. Are there any specific findings or insights about the German and English poetry in relation to aesthetic emotions?\n   - Related phrase: \"German and English poetry\"\n\n10. Does the paper discuss potential applications or future work related to the analysis of aesthetic emotions in literature?\n    - Related phrase: \"future large scale analysis\""}
{"id": "1705.09665", "title": "Community Identity and User Engagement in a Multi-Community Landscape", "abstract": "A community's identity defines and shapes its internal dynamics. Our current understanding of this interplay is mostly limited to glimpses gathered from isolated studies of individual communities. In this work we provide a systematic exploration of the nature of this relation across a wide variety of online communities. To this end we introduce a quantitative, language-based typology reflecting two key aspects of a community's identity: how distinctive, and how temporally dynamic it is. By mapping almost 300 Reddit communities into the landscape induced by this typology, we reveal regularities in how patterns of user engagement vary with the characteristics of a community. Our results suggest that the way new and existing users engage with a community depends strongly and systematically on the nature of the collective identity it fosters, in ways that are highly consequential to community maintainers. For example, communities with distinctive and highly dynamic identities are more likely to retain their users. However, such niche communities also exhibit much larger acculturation gaps between existing users and newcomers, which potentially hinder the integration of the latter. More generally, our methodology reveals differences in how various social phenomena manifest across communities, and shows that structuring the multi-community landscape can lead to a better understanding of the systematic nature of this diversity.", "questions": "1. What is the quantitative, language-based typology introduced in this study?\n   - Phrase: \"language-based typology\"\n\n2. How were the 300 Reddit communities chosen for this study?\n   - Phrase: \"300 Reddit communities\"\n\n3. What are the specific characteristics of a community's identity that were analyzed?\n   - Phrase: \"distinctive, and temporally dynamic\"\n\n4. How do user engagement patterns differ in communities with varying characteristics?\n   - Phrase: \"patterns of user engagement\"\n\n5. How is the collective identity of a community related to user engagement and retention?\n   - Phrase: \"collective identity\"\n\n6. What challenges do niche communities face when integrating newcomers?\n   - Phrase: \"acculturation gaps\"\n\n7. What are the implications of the study's findings for community maintainers?\n   - Phrase: \"community maintainers\"\n\n8. How can the methodology used in this study help to understand social phenomena across communities?\n   - Phrase: \"our methodology reveals\"\n\n9. Are there any specific examples or case studies of communities in this research?\n   - Phrase: \"individual communities\""}
{"id": "1908.06606", "title": "Question Answering based Clinical Text Structuring Using Pre-trained Language Model", "abstract": "Clinical text structuring is a critical and fundamental task for clinical research. Traditional methods such as taskspecific end-to-end models and pipeline models usually suffer from the lack of dataset and error propagation. In this paper, we present a question answering based clinical text structuring (QA-CTS) task to unify different specific tasks and make dataset shareable. A novel model that aims to introduce domain-specific features (e.g., clinical named entity information) into pre-trained language model is also proposed for QA-CTS task. Experimental results on Chinese pathology reports collected from Ruijing Hospital demonstrate our presented QA-CTS task is very effective to improve the performance on specific tasks. Our proposed model also competes favorably with strong baseline models in specific tasks.", "questions": "1. What are the limitations of traditional methods in clinical text structuring?\n   Related phrase: \"lack of dataset and error propagation\"\n\n2. How does the question answering based clinical text structuring (QA-CTS) task unify different specific tasks?\n   Related phrase: \"unify different specific tasks\"\n\n3. What is the novel model proposed for the QA-CTS task?\n   Related phrase: \"novel model\"\n\n4. How are domain-specific features integrated into the pre-trained language model?\n   Related phrase: \"domain-specific features\"\n\n5. What are the specific tasks the proposed model competes favorably with strong baseline models in?\n   Related phrase: \"strong baseline models\"\n\n6. How does the QA-CTS task improve performance on specific tasks?\n   Related phrase: \"improve the performance\"\n\n7. What type of dataset was used in the experiments of this paper?\n   Related phrase: \"Chinese pathology reports\"\n\n8. What is the source of the pathological reports used in the experiments?\n   Related phrase: \"Ruijing Hospital\""}
{"id": "1811.00942", "title": "Progress and Tradeoffs in Neural Language Models", "abstract": "In recent years, we have witnessed a dramatic shift towards techniques driven by neural networks for a variety of NLP tasks. Undoubtedly, neural language models (NLMs) have reduced perplexity by impressive amounts. This progress, however, comes at a substantial cost in performance, in terms of inference latency and energy consumption, which is particularly of concern in deployments on mobile devices. This paper, which examines the quality-performance tradeoff of various language modeling techniques, represents to our knowledge the first to make this observation. We compare state-of-the-art NLMs with\"classic\"Kneser-Ney (KN) LMs in terms of energy usage, latency, perplexity, and prediction accuracy using two standard benchmarks. On a Raspberry Pi, we find that orders of increase in latency and energy usage correspond to less change in perplexity, while the difference is much less pronounced on a desktop.", "questions": "1. What are the specific techniques driven by neural networks for NLP tasks mentioned in the paper? (\"neural networks\")\n\n2. How much has perplexity been reduced by neural language models? (\"reduced perplexity\")\n\n3. What are the performance tradeoffs when using neural language models on mobile devices? (\"performance tradeoff\")\n\n4. What are the differences in energy usage, latency, perplexity, and prediction accuracy between NLMs and Kneser-Ney LMs? (\"energy usage, latency\")\n\n5. How do the benchmarks compare NLMs and KN LMs on Raspberry Pi and desktop environments? (\"Raspberry Pi\")\n\n6. What are the orders of increase in latency and energy usage for NLMs on Raspberry Pi and desktops? (\"orders of increase\")\n\n7. How significant is the difference in perplexity between NLMs and classic Kneser-Ney LMs? (\"less change in perplexity\")\n\n8. What are the two standard benchmarks used to compare NLMs and KN LMs? (\"two standard benchmarks\")"}
{"id": "1805.02400", "title": "Stay On-Topic: Generating Context-specific Fake Restaurant Reviews", "abstract": "Automatically generated fake restaurant reviews are a threat to online review systems. Recent research has shown that users have difficulties in detecting machine-generated fake reviews hiding among real restaurant reviews. The method used in this work (char-LSTM ) has one drawback: it has difficulties staying in context, i.e. when it generates a review for specific target entity, the resulting review may contain phrases that are unrelated to the target, thus increasing its detectability. In this work, we present and evaluate a more sophisticated technique based on neural machine translation (NMT) with which we can generate reviews that stay on-topic. We test multiple variants of our technique using native English speakers on Amazon Mechanical Turk. We demonstrate that reviews generated by the best variant have almost optimal undetectability (class-averaged F-score 47%). We conduct a user study with skeptical users and show that our method evades detection more frequently compared to the state-of-the-art (average evasion 3.2/4 vs 1.5/4) with statistical significance, at level {\\alpha} = 1% (Section 4.3). We develop very effective detection tools and reach average F-score of 97% in classifying these. Although fake reviews are very effective in fooling people, effective automatic detection is still feasible.", "questions": "1. What is char-LSTM?\n   Related phrase: \"char-LSTM\"\n\n2. How does the neural machine translation technique improve context-specificity?\n   Related phrase: \"neural machine translation\"\n\n3. What are the different variants of the NMT technique tested in this study?\n   Related phrase: \"multiple variants\"\n\n4. How was the undetectability of generated reviews assessed?\n   Related phrase: \"class-averaged F-score\"\n\n5. What was the user study conducted with skeptical users?\n   Related phrase: \"user study\"\n\n6. How does the state-of-the-art method compare to the proposed technique in terms of evasion?\n   Related phrase: \"average evasion\"\n\n7. What are the developed detection tools that achieved an average F-score of 97%?\n   Related phrase: \"effective detection tools\"\n\n8. What are the implications of these findings for the online review systems?\n   Related phrase: \"fake restaurant reviews\""}
{"id": "1907.05664", "title": "Saliency Maps Generation for Automatic Text Summarization", "abstract": "Saliency map generation techniques are at the forefront of explainable AI literature for a broad range of machine learning applications. Our goal is to question the limits of these approaches on more complex tasks. In this paper we apply Layer-Wise Relevance Propagation (LRP) to a sequence-to-sequence attention model trained on a text summarization dataset. We obtain unexpected saliency maps and discuss the rightfulness of these\"explanations\". We argue that we need a quantitative way of testing the counterfactual case to judge the truthfulness of the saliency maps. We suggest a protocol to check the validity of the importance attributed to the input and show that the saliency maps obtained sometimes capture the real use of the input features by the network, and sometimes do not. We use this example to discuss how careful we need to be when accepting them as explanation.", "questions": "1. What are the applications of saliency map generation techniques in explainable AI?\n   - Related phrase: explainable AI literature\n\n2. How does Layer-Wise Relevance Propagation (LRP) work?\n   - Related phrase: Layer-Wise Relevance Propagation\n\n3. What is the sequence-to-sequence attention model used for the text summarization dataset?\n   - Related phrase: sequence-to-sequence attention model\n\n4. What are some examples of unexpected saliency maps generated?\n   - Related phrase: unexpected saliency maps\n\n5. How can the counterfactual case be tested quantitatively?\n   - Related phrase: quantitative way\n\n6. What is the proposed protocol for checking the validity of importance attributed to input?\n   - Related phrase: suggest a protocol\n\n7. In what cases do the saliency maps capture the real use of input features by the network?\n   - Related phrase: capture the real use\n\n8. How can one be more cautious when accepting saliency maps as explanations?\n   - Related phrase: how careful we need"}
{"id": "1910.14497", "title": "Probabilistic Bias Mitigation in Word Embeddings", "abstract": "It has been shown that word embeddings derived from large corpora tend to incorporate biases present in their training data. Various methods for mitigating these biases have been proposed, but recent work has demonstrated that these methods hide but fail to truly remove the biases, which can still be observed in word nearest-neighbor statistics. In this work we propose a probabilistic view of word embedding bias. We leverage this framework to present a novel method for mitigating bias which relies on probabilistic observations to yield a more robust bias mitigation algorithm. We demonstrate that this method effectively reduces bias according to three separate measures of bias while maintaining embedding quality across various popular benchmark semantic tasks", "questions": "1. What are the biases present in word embeddings derived from large corpora?\n   Phrase: incorporate biases\n\n2. What are the existing methods for mitigating biases in word embeddings?\n   Phrase: Various methods\n\n3. How do recent works show that existing methods fail to truly remove biases?\n   Phrase: recent work\n\n4. What is the proposed probabilistic view of word embedding bias?\n   Phrase: probabilistic view\n\n5. How does the novel method for mitigating bias work?\n   Phrase: novel method\n\n6. What are the three separate measures of bias used for evaluating the effectiveness of the proposed method?\n   Phrase: three separate measures\n\n7. How does the proposed method maintain embedding quality across popular benchmark semantic tasks?\n   Phrase: maintaining embedding quality"}
{"id": "1912.02481", "title": "Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yor\\`ub\\'a and Twi", "abstract": "The success of several architectures to learn semantic representations from unannotated text and the availability of these kind of texts in online multilingual resources such as Wikipedia has facilitated the massive and automatic creation of resources for multiple languages. The evaluation of such resources is usually done for the high-resourced languages, where one has a smorgasbord of tasks and test sets to evaluate on. For low-resourced languages, the evaluation is more difficult and normally ignored, with the hope that the impressive capability of deep learning architectures to learn (multilingual) representations in the high-resourced setting holds in the low-resourced setting too. In this paper we focus on two African languages, Yor\\`ub\\'a and Twi, and compare the word embeddings obtained in this way, with word embeddings obtained from curated corpora and a language-dependent processing. We analyse the noise in the publicly available corpora, collect high quality and noisy data for the two languages and quantify the improvements that depend not only on the amount of data but on the quality too. We also use different architectures that learn word representations both from surface forms and characters to further exploit all the available information which showed to be important for these languages. For the evaluation, we manually translate the wordsim-353 word pairs dataset from English into Yor\\`ub\\'a and Twi. As output of the work, we provide corpora, embeddings and the test suits for both languages.", "questions": "1. What are the several architectures used for learning semantic representations from unannotated text?\n   - \"several architectures\"\n\n2. How is the evaluation done for high-resourced languages compared to low-resourced languages?\n   - \"evaluation of such resources\"\n\n3. How do deep learning architectures perform in low-resourced settings?\n   - \"deep learning architectures\"\n\n4. What are the specific differences in word embeddings obtained from curated corpora and language-dependent processing?\n   - \"word embeddings obtained\"\n\n5. How was the noise in publicly available corpora analyzed?\n   - \"analyse the noise\"\n\n6. What kind of high quality and noisy data were collected for Yorùbá and Twi?\n   - \"collect high quality and noisy data\"\n\n7. What improvements were seen based on the amount and quality of the data?\n   - \"quantify the improvements\"\n\n8. What were the different architectures that learned word representations from surface forms and characters?\n   - \"different architectures\"\n\n9. How was the wordsim-353 dataset translated into Yorùbá and Twi for evaluation?\n   - \"manually translate\"\n\n10. What are the details of the provided corpora, embeddings, and test suits for Yorùbá and Twi?\n    - \"provide corpora, embeddings and the test suits\""}
{"id": "1810.04528", "title": "Is there Gender bias and stereotype in Portuguese Word Embeddings?", "abstract": "In this work, we propose an analysis of the presence of gender bias associated with professions in Portuguese word embeddings. The objective of this work is to study gender implications related to stereotyped professions for women and men in the context of the Portuguese language.", "questions": "1. What are word embeddings?\n   - \"Portuguese word embeddings\"\n\n2. How is gender bias analyzed?\n   - \"analysis of...gender bias\"\n\n3. What professions are considered stereotyped?\n   - \"stereotyped professions\"\n\n4. What are the specific gender implications examined?\n   - \"gender implications\"\n\n5. What methodology is used for studying gender bias in word embeddings?\n   - \"study gender implications\"\n\n6. Are there any previous studies or works related to this topic?\n   - \"propose an analysis\"\n\n7. How does the Portuguese language context differ from other languages in terms of gender bias?\n   - \"context of the Portuguese language\"\n\n8. What are the main findings of the research?\n   - \"presence of gender bias\"\n\n9. Are there any recommendations to mitigate the identified biases?\n   - \"objective of this work\""}
{"id": "2002.02224", "title": "Citation Data of Czech Apex Courts", "abstract": "In this paper, we introduce the citation data of the Czech apex courts (Supreme Court, Supreme Administrative Court and Constitutional Court). This dataset was automatically extracted from the corpus of texts of Czech court decisions - CzCDC 1.0. We obtained the citation data by building the natural language processing pipeline for extraction of the court decision identifiers. The pipeline included the (i) document segmentation model and the (ii) reference recognition model. Furthermore, the dataset was manually processed to achieve high-quality citation data as a base for subsequent qualitative and quantitative analyses. The dataset will be made available to the general public.", "questions": "1. What is the purpose of extracting citation data from Czech apex courts? (citation data)\n2. How were the court decisions organized in the CzCDC 1.0 corpus? (corpus of texts)\n3. What are the specific components of the natural language processing pipeline? (natural language processing)\n4. How did the document segmentation model work in the extraction process? (document segmentation)\n5. What was the role of the reference recognition model in the pipeline? (reference recognition)\n6. What manual processing steps were taken to ensure high-quality citation data? (manually processed)\n7. What are some examples of qualitative and quantitative analyses that can be performed using this dataset? (qualitative and quantitative analyses)\n8. How can the general public access the dataset? (general public)"}
{"id": "2003.07433", "title": "LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment", "abstract": "Veteran mental health is a significant national problem as large number of veterans are returning from the recent war in Iraq and continued military presence in Afghanistan. While significant existing works have investigated twitter posts-based Post Traumatic Stress Disorder (PTSD) assessment using blackbox machine learning techniques, these frameworks cannot be trusted by the clinicians due to the lack of clinical explainability. To obtain the trust of clinicians, we explore the big question, can twitter posts provide enough information to fill up clinical PTSD assessment surveys that have been traditionally trusted by clinicians? To answer the above question, we propose, LAXARY (Linguistic Analysis-based Exaplainable Inquiry) model, a novel Explainable Artificial Intelligent (XAI) model to detect and represent PTSD assessment of twitter users using a modified Linguistic Inquiry and Word Count (LIWC) analysis. First, we employ clinically validated survey tools for collecting clinical PTSD assessment data from real twitter users and develop a PTSD Linguistic Dictionary using the PTSD assessment survey results. Then, we use the PTSD Linguistic Dictionary along with machine learning model to fill up the survey tools towards detecting PTSD status and its intensity of corresponding twitter users. Our experimental evaluation on 210 clinically validated veteran twitter users provides promising accuracies of both PTSD classification and its intensity estimation. We also evaluate our developed PTSD Linguistic Dictionary's reliability and validity.", "questions": "1. What is the main issue with existing PTSD assessment frameworks for veterans?\n   Related phrase: \"lack of clinical explainability\"\n\n2. How does LAXARY aim to address the trust issue with clinicians?\n   Related phrase: \"Explainable Artificial Intelligent\"\n\n3. What is the modified Linguistic Inquiry and Word Count (LIWC) analysis used for in the LAXARY model?\n   Related phrase: \"LIWC analysis\"\n\n4. How is the PTSD Linguistic Dictionary developed?\n   Related phrase: \"PTSD assessment survey results\"\n\n5. How is the PTSD Linguistic Dictionary used in the machine learning model?\n   Related phrase: \"fill up the survey tools\"\n\n6. What is the sample size and source for the experimental evaluation?\n   Related phrase: \"210 clinically validated veteran twitter users\"\n\n7. What were the results of the experimental evaluation on PTSD classification and intensity estimation?\n   Related phrase: \"promising accuracies\"\n\n8. How was the reliability and validity of the PTSD Linguistic Dictionary evaluated?\n   Related phrase: \"reliability and validity\""}
{"id": "2003.12218", "title": "Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak Supervision", "abstract": "We created this CORD-19-NER dataset with comprehensive named entity recognition (NER) on the COVID-19 Open Research Dataset Challenge (CORD-19) corpus (2020- 03-13). This CORD-19-NER dataset covers 74 fine-grained named entity types. It is automatically generated by combining the annotation results from four sources: (1) pre-trained NER model on 18 general entity types from Spacy, (2) pre-trained NER model on 18 biomedical entity types from SciSpacy, (3) knowledge base (KB)-guided NER model on 127 biomedical entity types with our distantly-supervised NER method, and (4) seed-guided NER model on 8 new entity types (specifically related to the COVID-19 studies) with our weakly-supervised NER method. We hope this dataset can help the text mining community build downstream applications. We also hope this dataset can bring insights for the COVID- 19 studies, both on the biomedical side and on the social side.", "questions": "1. What is the CORD-19-NER dataset?\n   - \"CORD-19-NER dataset\"\n\n2. How many fine-grained named entity types does the dataset cover?\n   - \"74 fine-grained\"\n\n3. What are the four sources used to generate the dataset?\n   - \"four sources\"\n\n4. What are the 18 general entity types from Spacy?\n   - \"18 general entity\"\n\n5. What are the 18 biomedical entity types from SciSpacy?\n   - \"18 biomedical entity\"\n\n6. What is the distantly-supervised NER method?\n   - \"distantly-supervised NER\"\n\n7. What are the 127 biomedical entity types from the knowledge base-guided NER model?\n   - \"127 biomedical entity\"\n\n8. What is the weakly-supervised NER method?\n   - \"weakly-supervised NER\"\n\n9. What are the 8 new entity types related to COVID-19 studies?\n   - \"8 new entity types\"\n\n10. What are the potential downstream applications of this dataset?\n    - \"downstream applications\"\n\n11. How can this dataset contribute to the understanding of COVID-19 studies?\n    - \"COVID-19 studies\""}
{"id": "1904.09678", "title": "UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages", "abstract": "In this paper, we introduce UniSent a universal sentiment lexica for 1000 languages created using an English sentiment lexicon and a massively parallel corpus in the Bible domain. To the best of our knowledge, UniSent is the largest sentiment resource to date in terms of number of covered languages, including many low resource languages. To create UniSent, we propose Adapted Sentiment Pivot, a novel method that combines annotation projection, vocabulary expansion, and unsupervised domain adaptation. We evaluate the quality of UniSent for Macedonian, Czech, German, Spanish, and French and show that its quality is comparable to manually or semi-manually created sentiment resources. With the publication of this paper, we release UniSent lexica as well as Adapted Sentiment Pivot related codes. method.", "questions": "1. What is UniSent?\n   - Related phrase: \"universal sentiment lexica\"\n\n2. How many languages does UniSent cover?\n   - Related phrase: \"1000 languages\"\n\n3. What domain is the parallel corpus from?\n   - Related phrase: \"Bible domain\"\n\n4. What is the Adapted Sentiment Pivot method?\n   - Related phrase: \"Adapted Sentiment Pivot\"\n\n5. How does Adapted Sentiment Pivot combine annotation projection, vocabulary expansion, and unsupervised domain adaptation?\n   - Related phrase: \"combines annotation projection\"\n\n6. Which languages were used for evaluation?\n   - Related phrase: \"Macedonian, Czech, German, Spanish, and French\"\n\n7. How does UniSent's quality compare to other sentiment resources?\n   - Related phrase: \"comparable to manually\"\n\n8. What resources are being released along with this paper?\n   - Related phrase: \"release UniSent lexica\""}
{"id": "2003.06651", "title": "Word Sense Disambiguation for 158 Languages using Word Embeddings Only", "abstract": "Disambiguation of word senses in context is easy for humans, but is a major challenge for automatic approaches. Sophisticated supervised and knowledge-based models were developed to solve this task. However, (i) the inherent Zipfian distribution of supervised training instances for a given word and/or (ii) the quality of linguistic knowledge representations motivate the development of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al. (2018), enabling WSD in these languages. Models and system are available online.", "questions": "1. What are the limitations of supervised and knowledge-based models for word sense disambiguation?\n   - Phrase: \"supervised and knowledge-based models\"\n\n2. How does the Zipfian distribution affect supervised training instances for a given word?\n   - Phrase: \"inherent Zipfian distribution\"\n\n3. How does the quality of linguistic knowledge representations impact word sense disambiguation?\n   - Phrase: \"quality of linguistic knowledge representations\"\n\n4. What are the benefits of unsupervised and knowledge-free approaches to WSD, especially for under-resourced languages?\n   - Phrase: \"under-resourced languages\"\n\n5. How does the presented method induce a fully-fledged word sense inventory from a pre-trained word embedding model?\n   - Phrase: \"induces a fully-fledged word sense inventory\"\n\n6. What are the main features of the fastText word embeddings by Grave et al. (2018)?\n   - Phrase: \"fastText word embeddings\"\n\n7. How is the induced collection of sense inventories used for disambiguation in context across 158 languages?\n   - Phrase: \"disambiguation in context\"\n\n8. Are the induced sense inventories and models available online, and if so, where can they be accessed?\n   - Phrase: \"available online\""}
{"id": "1910.04269", "title": "Spoken Language Identification using ConvNets", "abstract": "Language Identification (LI) is an important first step in several speech processing systems. With a growing number of voice-based assistants, speech LI has emerged as a widely researched field. To approach the problem of identifying languages, we can either adopt an implicit approach where only the speech for a language is present or an explicit one where text is available with its corresponding transcript. This paper focuses on an implicit approach due to the absence of transcriptive data. This paper benchmarks existing models and proposes a new attention based model for language identification which uses log-Mel spectrogram images as input. We also present the effectiveness of raw waveforms as features to neural network models for LI tasks. For training and evaluation of models, we classified six languages (English, French, German, Spanish, Russian and Italian) with an accuracy of 95.4% and four languages (English, French, German, Spanish) with an accuracy of 96.3% obtained from the VoxForge dataset. This approach can further be scaled to incorporate more languages.", "questions": "1. What are the existing models in speech LI research?\n   - Phrase: \"benchmarks existing models\"\n\n2. How does the new attention-based model work?\n   - Phrase: \"attention based model\"\n\n3. Why are log-Mel spectrogram images used as input?\n   - Phrase: \"log-Mel spectrogram\"\n\n4. What is the effectiveness of raw waveforms as features for LI tasks?\n   - Phrase: \"raw waveforms\"\n\n5. What are the details of the VoxForge dataset used in this study?\n   - Phrase: \"VoxForge dataset\"\n\n6. How can this approach be scaled to include more languages?\n   - Phrase: \"scaled to incorporate\"\n\n7. What are the specific results and comparisons between the proposed model and the benchmarked models?\n   - Phrase: \"benchmarks existing models\"\n\n8. Were any challenges or limitations encountered during the experiment?\n   - Phrase: \"accuracy of 95.4%\" (as an indicator of potential challenges)\n\n9. What are the potential applications or improvements for voice-based assistants resulting from this research?\n   - Phrase: \"voice-based assistants\""}
{"id": "1906.00378", "title": "Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data", "abstract": "Bilingual lexicon induction, translating words from the source language to the target language, is a long-standing natural language processing task. Recent endeavors prove that it is promising to employ images as pivot to learn the lexicon induction without reliance on parallel corpora. However, these vision-based approaches simply associate words with entire images, which are constrained to translate concrete words and require object-centered images. We humans can understand words better when they are within a sentence with context. Therefore, in this paper, we propose to utilize images and their associated captions to address the limitations of previous approaches. We propose a multi-lingual caption model trained with different mono-lingual multimodal data to map words in different languages into joint spaces. Two types of word representation are induced from the multi-lingual caption model: linguistic features and localized visual features. The linguistic feature is learned from the sentence contexts with visual semantic constraints, which is beneficial to learn translation for words that are less visual-relevant. The localized visual feature is attended to the region in the image that correlates to the word, so that it alleviates the image restriction for salient visual representation. The two types of features are complementary for word translation. Experimental results on multiple language pairs demonstrate the effectiveness of our proposed method, which substantially outperforms previous vision-based approaches without using any parallel sentences or supervision of seed word pairs.", "questions": "1. What are the limitations of previous vision-based approaches for bilingual lexicon induction?\n   - Phrase: vision-based approaches\n\n2. How does the proposed multi-lingual caption model work?\n   - Phrase: multi-lingual caption model\n\n3. What are the two types of word representation induced from the multi-lingual caption model?\n   - Phrase: two types\n\n4. How do linguistic features help in learning translation for less visual-relevant words?\n   - Phrase: linguistic features\n\n5. How do localized visual features alleviate image restrictions for salient visual representation?\n   - Phrase: localized visual features\n\n6. What were the experimental results for the proposed method on multiple language pairs?\n   - Phrase: experimental results\n\n7. How does the proposed method compare to previous vision-based approaches in terms of performance?\n   - Phrase: substantially outperforms"}
{"id": "1912.13072", "title": "AraNet: A Deep Learning Toolkit for Arabic Social Media", "abstract": "We describe AraNet, a collection of deep learning Arabic social media processing tools. Namely, we exploit an extensive host of publicly available and novel social media datasets to train bidirectional encoders from transformer models (BERT) to predict age, dialect, gender, emotion, irony, and sentiment. AraNet delivers state-of-the-art performance on a number of the cited tasks and competitively on others. In addition, AraNet has the advantage of being exclusively based on a deep learning framework and hence feature engineering free. To the best of our knowledge, AraNet is the first to performs predictions across such a wide range of tasks for Arabic NLP and thus meets a critical needs. We publicly release AraNet to accelerate research and facilitate comparisons across the different tasks.", "questions": "1. What are the specific social media datasets used for training the bidirectional encoders?\n   - Phrase: \"social media datasets\"\n\n2. How does AraNet's performance compare to other existing methods on the mentioned tasks?\n   - Phrase: \"state-of-the-art performance\"\n\n3. What deep learning framework is AraNet based on?\n   - Phrase: \"deep learning framework\"\n\n4. How does the absence of feature engineering benefit AraNet?\n   - Phrase: \"feature engineering free\"\n\n5. What are the detailed results of AraNet on each of the tasks (age, dialect, gender, emotion, irony, and sentiment)?\n   - Phrase: \"predict age, dialect, gender, emotion, irony, and sentiment\"\n\n6. How can researchers access and use AraNet for their own work?\n   - Phrase: \"publicly release AraNet\"\n\n7. Are there any limitations or challenges faced while developing AraNet?\n   - Phrase: \"AraNet delivers\"\n\n8. What are some potential applications of AraNet in Arabic NLP research?\n   - Phrase: \"Arabic NLP\"\n\n9. Are there any plans for future improvements or additions to AraNet?\n   - Phrase: \"AraNet is the first\"\n\n10. How does AraNet handle variations in Arabic dialects and regional differences?\n    - Phrase: \"predict dialect\""}
{"id": "1712.09127", "title": "Generative Adversarial Nets for Multiple Text Corpora", "abstract": "Generative adversarial nets (GANs) have been successfully applied to the artificial generation of image data. In terms of text data, much has been done on the artificial generation of natural language from a single corpus. We consider multiple text corpora as the input data, for which there can be two applications of GANs: (1) the creation of consistent cross-corpus word embeddings given different word embeddings per corpus; (2) the generation of robust bag-of-words document embeddings for each corpora. We demonstrate our GAN models on real-world text data sets from different corpora, and show that embeddings from both models lead to improvements in supervised learning problems.", "questions": "1. What are generative adversarial nets (GANs)?\n- Phrase: Generative adversarial nets\n\n2. How have GANs been applied to image data?\n- Phrase: artificial generation of image data\n\n3. What progress has been made in generating natural language from a single corpus?\n- Phrase: artificial generation of natural language\n\n4. What are the two applications of GANs for multiple text corpora?\n- Phrase: two applications of GANs\n\n5. How do the GAN models create consistent cross-corpus word embeddings?\n- Phrase: consistent cross-corpus word embeddings\n\n6. How are robust bag-of-words document embeddings generated for each corpus?\n- Phrase: robust bag-of-words document embeddings\n\n7. What real-world text data sets were used for the demonstration of GAN models?\n- Phrase: real-world text data sets\n\n8. How do the embeddings from both models improve supervised learning problems?\n- Phrase: improvements in supervised learning problems"}
{"id": "2001.00137", "title": "Stacked DeBERT: All Attention in Incomplete Data for Text Classification", "abstract": "In this paper, we propose Stacked DeBERT, short for Stacked Denoising Bidirectional Encoder Representations from Transformers. This novel model improves robustness in incomplete data, when compared to existing systems, by designing a novel encoding scheme in BERT, a powerful language representation model solely based on attention mechanisms. Incomplete data in natural language processing refer to text with missing or incorrect words, and its presence can hinder the performance of current models that were not implemented to withstand such noises, but must still perform well even under duress. This is due to the fact that current approaches are built for and trained with clean and complete data, and thus are not able to extract features that can adequately represent incomplete data. Our proposed approach consists of obtaining intermediate input representations by applying an embedding layer to the input tokens followed by vanilla transformers. These intermediate features are given as input to novel denoising transformers which are responsible for obtaining richer input representations. The proposed approach takes advantage of stacks of multilayer perceptrons for the reconstruction of missing words' embeddings by extracting more abstract and meaningful hidden feature vectors, and bidirectional transformers for improved embedding representation. We consider two datasets for training and evaluation: the Chatbot Natural Language Understanding Evaluation Corpus and Kaggle's Twitter Sentiment Corpus. Our model shows improved F1-scores and better robustness in informal/incorrect texts present in tweets and in texts with Speech-to-Text error in the sentiment and intent classification tasks.", "questions": "1. What is Stacked DeBERT?\n   - Phrase: Stacked DeBERT\n\n2. How does Stacked DeBERT improve robustness in incomplete data?\n   - Phrase: novel encoding scheme\n\n3. How is incomplete data defined in this context?\n   - Phrase: missing or incorrect words\n\n4. Why do current models struggle with incomplete data?\n   - Phrase: clean and complete data\n\n5. What are the main components of the proposed approach?\n   - Phrase: denoising transformers\n\n6. How do multilayer perceptrons contribute to the reconstruction of missing words' embeddings?\n   - Phrase: multilayer perceptrons\n\n7. What is the role of bidirectional transformers in the proposed model?\n   - Phrase: bidirectional transformers\n\n8. Which datasets were used for training and evaluation?\n   - Phrase: Chatbot Natural Language Understanding Evaluation Corpus and Kaggle's Twitter Sentiment Corpus\n\n9. What performance metrics were used to evaluate the model?\n   - Phrase: improved F1-scores\n\n10. How does the model perform on informal/incorrect texts and texts with Speech-to-Text errors?\n    - Phrase: better robustness"}
{"id": "1910.03042", "title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations", "abstract": "Gunrock is the winner of the 2018 Amazon Alexa Prize, as evaluated by coherence and engagement from both real users and Amazon-selected expert conversationalists. We focus on understanding complex sentences and having in-depth conversations in open domains. In this paper, we introduce some innovative system designs and related validation analysis. Overall, we found that users produce longer sentences to Gunrock, which are directly related to users' engagement (e.g., ratings, number of turns). Additionally, users' backstory queries about Gunrock are positively correlated to user satisfaction. Finally, we found dialog flows that interleave facts and personal opinions and stories lead to better user satisfaction.", "questions": "FAILED"}
{"id": "2002.06644", "title": "Towards Detection of Subjective Bias using Contextualized Word Embeddings", "abstract": "Subjective bias detection is critical for applications like propaganda detection, content recommendation, sentiment analysis, and bias neutralization. This bias is introduced in natural language via inflammatory words and phrases, casting doubt over facts, and presupposing the truth. In this work, we perform comprehensive experiments for detecting subjective bias using BERT-based models on the Wiki Neutrality Corpus(WNC). The dataset consists of $360k$ labeled instances, from Wikipedia edits that remove various instances of the bias. We further propose BERT-based ensembles that outperform state-of-the-art methods like $BERT_{large}$ by a margin of $5.6$ F1 score.", "questions": "1. What are the applications of subjective bias detection?\n   - Related phrase: propaganda detection, content recommendation, sentiment analysis, and bias neutralization\n\n2. How is subjective bias introduced in natural language?\n   - Related phrase: inflammatory words and phrases\n\n3. What is the Wiki Neutrality Corpus (WNC)?\n   - Related phrase: Wiki Neutrality Corpus\n\n4. How many labeled instances are in the WNC dataset?\n   - Related phrase: 360k labeled instances\n\n5. What kind of Wikipedia edits does the WNC dataset consist of?\n   - Related phrase: Wikipedia edits\n\n6. What BERT-based models were used for subjective bias detection?\n   - Related phrase: BERT-based models\n\n7. How do the proposed BERT-based ensembles perform compared to state-of-the-art methods?\n   - Related phrase: outperform state-of-the-art methods\n\n8. What is the improvement in F1 score achieved by the BERT-based ensembles?\n   - Related phrase: 5.6 F1 score"}
{"id": "1809.08731", "title": "Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!", "abstract": "Motivated by recent findings on the probabilistic modeling of acceptability judgments, we propose syntactic log-odds ratio (SLOR), a normalized language model score, as a metric for referenceless fluency evaluation of natural language generation output at the sentence level. We further introduce WPSLOR, a novel WordPiece-based version, which harnesses a more compact language model. Even though word-overlap metrics like ROUGE are computed with the help of hand-written references, our referenceless methods obtain a significantly higher correlation with human fluency scores on a benchmark dataset of compressed sentences. Finally, we present ROUGE-LM, a reference-based metric which is a natural extension of WPSLOR to the case of available references. We show that ROUGE-LM yields a significantly higher correlation with human judgments than all baseline metrics, including WPSLOR on its own.", "questions": "1. What are the recent findings on probabilistic modeling of acceptability judgments? (recent findings)\n\n2. How does syntactic log-odds ratio (SLOR) work as a fluency evaluation metric? (syntactic log-odds ratio)\n\n3. What are the differences between SLOR and WPSLOR? (WordPiece-based version)\n\n4. How do referenceless methods perform compared to word-overlap metrics like ROUGE? (significantly higher correlation)\n\n5. What is the benchmark dataset used for evaluating compressed sentences? (benchmark dataset)\n\n6. How is ROUGE-LM a natural extension of WPSLOR? (ROUGE-LM)\n\n7. How does ROUGE-LM compare to other baseline metrics in terms of correlation with human judgments? (significantly higher correlation)\n\n8. What are the implications of these findings for natural language generation evaluation? (fluency evaluation)"}
{"id": "1707.00995", "title": "An empirical study on the effectiveness of images in Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multimodal translation task (English, image to German) and evaluate the ability of the model to make use of images to improve translation. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while translating.", "questions": "1. What is the attention mechanism in Neural Machine Translation?\n   - Related phrase: attention mechanism\n\n2. How does the decoder utilize the attention mechanism during translation?\n   - Related phrase: during decoding\n\n3. What are the different attention mechanisms compared in the study?\n   - Related phrase: several attention mechanism\n\n4. How does the model make use of images to improve translation?\n   - Related phrase: use of images\n\n5. What are the results of the comparison on the multimodal translation tasks?\n   - Related phrase: compare several attention\n\n6. What is the Multi30k dataset and how is it used in the study?\n   - Related phrase: Multi30k data set\n\n7. What are the specific misbehaviors observed in the machine during translation?\n   - Related phrase: different misbehavior"}
{"id": "1809.04960", "title": "Unsupervised Machine Commenting with Neural Variational Topic Model", "abstract": "Article comments can provide supplementary opinions and facts for readers, thereby increase the attraction and engagement of articles. Therefore, automatically commenting is helpful in improving the activeness of the community, such as online forums and news websites. Previous work shows that training an automatic commenting system requires large parallel corpora. Although part of articles are naturally paired with the comments on some websites, most articles and comments are unpaired on the Internet. To fully exploit the unpaired data, we completely remove the need for parallel data and propose a novel unsupervised approach to train an automatic article commenting model, relying on nothing but unpaired articles and comments. Our model is based on a retrieval-based commenting framework, which uses news to retrieve comments based on the similarity of their topics. The topic representation is obtained from a neural variational topic model, which is trained in an unsupervised manner. We evaluate our model on a news comment dataset. Experiments show that our proposed topic-based approach significantly outperforms previous lexicon-based models. The model also profits from paired corpora and achieves state-of-the-art performance under semi-supervised scenarios.", "questions": "1. What is the main goal of the unsupervised automatic article commenting model?\n   - \"automatically commenting\"\n\n2. How does the proposed model differ from previous models?\n   - \"completely remove\"\n\n3. What is the basis of the retrieval-based commenting framework?\n   - \"retrieval-based commenting\"\n\n4. How is the topic representation obtained in the neural variational topic model?\n   - \"topic representation\"\n\n5. How was the model evaluated?\n   - \"news comment dataset\"\n\n6. How does the proposed model compare to lexicon-based models?\n   - \"significantly outperforms\"\n\n7. How does the model perform under semi-supervised scenarios?\n   - \"state-of-the-art performance\""}
{"id": "1909.08402", "title": "Enriching BERT with Knowledge Graph Embeddings for Document Classification", "abstract": "In this paper, we focus on the classification of books using short descriptive texts (cover blurbs) and additional metadata. Building upon BERT, a deep neural language model, we demonstrate how to combine text representations with metadata and knowledge graph embeddings, which encode author information. Compared to the standard BERT approach we achieve considerably better results for the classification task. For a more coarse-grained classification using eight labels we achieve an F1- score of 87.20, while a detailed classification using 343 labels yields an F1-score of 64.70. We make the source code and trained models of our experiments publicly available", "questions": "1. What is the main goal of the research?\n   - \"classification of books\"\n\n2. How are cover blurbs and metadata utilized?\n   - \"combine text representations\"\n\n3. What is the role of knowledge graph embeddings in the study?\n   - \"encode author information\"\n\n4. How does the enriched BERT model compare to the standard BERT approach?\n   - \"achieve considerably better results\"\n\n5. What are the F1-scores for the coarse-grained and detailed classifications?\n   - \"F1-score of 87.20\" and \"F1-score of 64.70\"\n\n6. Are the source code and trained models accessible?\n   - \"publicly available\"\n\n7. What is the basis of the deep neural language model used in the research?\n   - \"Building upon BERT\"\n\n8. How many labels are used for the more coarse-grained classification?\n   - \"eight labels\"\n\n9. How many labels are used for the detailed classification?\n   - \"343 labels\""}
{"id": "1909.11189", "title": "Diachronic Topics in New High German Poetry", "abstract": "Statistical topic models are increasingly and popularly used by Digital Humanities scholars to perform distant reading tasks on literary data. It allows us to estimate what people talk about. Especially Latent Dirichlet Allocation (LDA) has shown its usefulness, as it is unsupervised, robust, easy to use, scalable, and it offers interpretable results. In a preliminary study, we apply LDA to a corpus of New High German poetry (textgrid, with 51k poems, 8m token), and use the distribution of topics over documents for a classification of poems into time periods and for authorship attribution.", "questions": "1. What are the benefits of using statistical topic models in Digital Humanities research?\n- \"Statistical topic models\"\n\n2. How does Latent Dirichlet Allocation (LDA) work, and what makes it a useful method for analyzing literary data?\n- \"Latent Dirichlet Allocation\"\n\n3. What is the size and scope of the New High German poetry corpus used in the study?\n- \"51k poems, 8m token\"\n\n4. How is the distribution of topics over documents used for classification purposes in this study?\n- \"distribution of topics\"\n\n5. What are the specific methods used for classifying poems into time periods and for authorship attribution?\n- \"classification of poems\"\n\n6. What were the main findings of the preliminary study, and how might they inform further research in the field?\n- \"preliminary study\"\n\n7. Are there any limitations or challenges faced in applying LDA to a corpus of New High German poetry?\n- \"apply LDA\"\n\n8. How does this research contribute to the understanding of diachronic topics in New High German poetry?\n- \"Diachronic Topics\""}
{"id": "1810.05320", "title": "Important Attribute Identification in Knowledge Graph", "abstract": "The knowledge graph(KG) composed of entities with their descriptions and attributes, and relationship between entities, is finding more and more application scenarios in various natural language processing tasks. In a typical knowledge graph like Wikidata, entities usually have a large number of attributes, but it is difficult to know which ones are important. The importance of attributes can be a valuable piece of information in various applications spanning from information retrieval to natural language generation. In this paper, we propose a general method of using external user generated text data to evaluate the relative importance of an entity's attributes. To be more specific, we use the word/sub-word embedding techniques to match the external textual data back to entities' attribute name and values and rank the attributes by their matching cohesiveness. To our best knowledge, this is the first work of applying vector based semantic matching to important attribute identification, and our method outperforms the previous traditional methods. We also apply the outcome of the detected important attributes to a language generation task; compared with previous generated text, the new method generates much more customized and informative messages.", "questions": "1. What are the various natural language processing tasks that KGs are applied to? (Phrase: \"application scenarios\")\n\n2. How does the proposed method evaluate the relative importance of an entity's attributes? (Phrase: \"external user generated\")\n\n3. What are the word/sub-word embedding techniques used for matching? (Phrase: \"embedding techniques\")\n\n4. How does the vector-based semantic matching approach compare to traditional methods? (Phrase: \"outperforms the previous\")\n\n5. What specific language generation task was used to test the outcome of detected important attributes? (Phrase: \"language generation task\")\n\n6. How do the generated messages differ from those produced by previous methods? (Phrase: \"more customized and informative\")"}
{"id": "2003.08529", "title": "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections", "abstract": "Summarizing data samples by quantitative measures has a long history, with descriptive statistics being a case in point. However, as natural language processing methods flourish, there are still insufficient characteristic metrics to describe a collection of texts in terms of the words, sentences, or paragraphs they comprise. In this work, we propose metrics of diversity, density, and homogeneity that quantitatively measure the dispersion, sparsity, and uniformity of a text collection. We conduct a series of simulations to verify that each metric holds desired properties and resonates with human intuitions. Experiments on real-world datasets demonstrate that the proposed characteristic metrics are highly correlated with text classification performance of a renowned model, BERT, which could inspire future applications.", "questions": "1. What are the limitations of current characteristic metrics for text collections?\n   - Phrase: insufficient characteristic metrics\n\n2. How are the proposed metrics of diversity, density, and homogeneity defined?\n   - Phrase: metrics of diversity, density, and homogeneity\n\n3. What are the desired properties of these metrics?\n   - Phrase: desired properties\n\n4. How do these metrics resonate with human intuitions?\n   - Phrase: human intuitions\n\n5. What kind of simulations were conducted to verify the metrics?\n   - Phrase: series of simulations\n\n6. Which real-world datasets were used for the experiments?\n   - Phrase: real-world datasets\n\n7. How were the correlations between the proposed metrics and BERT's performance measured?\n   - Phrase: highly correlated\n\n8. What are the potential future applications of these characteristic metrics?\n   - Phrase: future applications"}
{"id": "1708.05873", "title": "What Drives the International Development Agenda? An NLP Analysis of the United Nations General Debate 1970-2016", "abstract": "There is surprisingly little known about agenda setting for international development in the United Nations (UN) despite it having a significant influence on the process and outcomes of development efforts. This paper addresses this shortcoming using a novel approach that applies natural language processing techniques to countries' annual statements in the UN General Debate. Every year UN member states deliver statements during the General Debate on their governments' perspective on major issues in world politics. These speeches provide invaluable information on state preferences on a wide range of issues, including international development, but have largely been overlooked in the study of global politics. This paper identifies the main international development topics that states raise in these speeches between 1970 and 2016, and examine the country-specific drivers of international development rhetoric.", "questions": "1. What is the significance of agenda setting in the United Nations for international development?\n   - \"significant influence\"\n\n2. What are the natural language processing techniques used in this study?\n   - \"natural language processing\"\n\n3. How have the UN General Debate statements been overlooked in the study of global politics?\n   - \"largely been overlooked\"\n\n4. What are the main international development topics identified in the UN General Debate speeches between 1970 and 2016?\n   - \"main international development\"\n\n5. What country-specific drivers influence international development rhetoric?\n   - \"country-specific drivers\"\n\n6. How do the annual statements from UN member states provide insight into state preferences on international development?\n   - \"state preferences\"\n\n7. What is the overall aim of analyzing the UN General Debate speeches from 1970 to 2016?\n   - \"addresses this shortcoming\""}
{"id": "2003.08553", "title": "QnAMaker: Data to Bot in 2 Minutes", "abstract": "Having a bot for seamless conversations is a much-desired feature that products and services today seek for their websites and mobile apps. These bots help reduce traffic received by human support significantly by handling frequent and directly answerable known questions. Many such services have huge reference documents such as FAQ pages, which makes it hard for users to browse through this data. A conversation layer over such raw data can lower traffic to human support by a great margin. We demonstrate QnAMaker, a service that creates a conversational layer over semi-structured data such as FAQ pages, product manuals, and support documents. QnAMaker is the popular choice for Extraction and Question-Answering as a service and is used by over 15,000 bots in production. It is also used by search interfaces and not just bots.", "questions": "1. What is the main purpose of QnAMaker?\n   - Related phrase: \"creates a conversational layer\"\n\n2. How does QnAMaker help reduce traffic to human support?\n   - Related phrase: \"lower traffic to human support\"\n\n3. What types of data does QnAMaker work with?\n   - Related phrase: \"semi-structured data\"\n\n4. What are some examples of data sources for QnAMaker?\n   - Related phrase: \"FAQ pages, product manuals, and support documents\"\n\n5. How popular is QnAMaker among bots in production?\n   - Related phrase: \"used by over 15,000 bots\"\n\n6. What are other applications of QnAMaker besides bots?\n   - Related phrase: \"used by search interfaces\""}
{"id": "1909.09491", "title": "A simple discriminative training method for machine translation with large-scale features", "abstract": "Margin infused relaxed algorithms (MIRAs) dominate model tuning in statistical machine translation in the case of large scale features, but also they are famous for the complexity in implementation. We introduce a new method, which regards an N-best list as a permutation and minimizes the Plackett-Luce loss of ground-truth permutations. Experiments with large-scale features demonstrate that, the new method is more robust than MERT; though it is only matchable with MIRAs, it has a comparatively advantage, easier to implement.", "questions": "1. What are margin infused relaxed algorithms (MIRAs) used for in statistical machine translation?\n   Related phrase: MIRAs dominate\n\n2. What is the complexity associated with MIRAs in implementation?\n   Related phrase: complexity in implementation\n\n3. How does the new method treat an N-best list in the context of machine translation?\n   Related phrase: N-best list\n\n4. What is the Plackett-Luce loss and how is it used in the new method?\n   Related phrase: Plackett-Luce loss\n\n5. How does the new method compare to MERT in terms of robustness?\n   Related phrase: more robust than MERT\n\n6. In what way is the new method only matchable with MIRAs?\n   Related phrase: matchable with MIRAs\n\n7. What is the main advantage of the new method compared to MIRAs?\n   Related phrase: easier to implement\n\n8. What kind of experiments were conducted to test the new method with large-scale features?\n   Related phrase: Experiments with large-scale features"}
{"id": "2001.05284", "title": "Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses", "abstract": "In a modern spoken language understanding (SLU) system, the natural language understanding (NLU) module takes interpretations of a speech from the automatic speech recognition (ASR) module as the input. The NLU module usually uses the first best interpretation of a given speech in downstream tasks such as domain and intent classification. However, the ASR module might misrecognize some speeches and the first best interpretation could be erroneous and noisy. Solely relying on the first best interpretation could make the performance of downstream tasks non-optimal. To address this issue, we introduce a series of simple yet efficient models for improving the understanding of semantics of the input speeches by collectively exploiting the n-best speech interpretations from the ASR module.", "questions": "1. What is the main goal of the presented models?\n   - \"improving the understanding\"\n\n2. How does an SLU system typically process speech input?\n   - \"first best interpretation\"\n\n3. What is the main issue with relying on the first best interpretation?\n   - \"erroneous and noisy\"\n\n4. How do the proposed models address the issue of misrecognized speeches?\n   - \"collectively exploiting\"\n\n5. What specific downstream tasks are mentioned in the abstract?\n   - \"domain and intent classification\"\n\n6. What is the role of the ASR module in the SLU system?\n   - \"takes interpretations\"\n\n7. Are the introduced models complex or simple?\n   - \"simple yet efficient\""}
{"id": "1909.12140", "title": "DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German", "abstract": "We introduce DisSim, a discourse-aware sentence splitting framework for English and German whose goal is to transform syntactically complex sentences into an intermediate representation that presents a simple and more regular structure which is easier to process for downstream semantic applications. For this purpose, we turn input sentences into a two-layered semantic hierarchy in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them. In that way, we preserve the coherence structure of the input and, hence, its interpretability for downstream tasks.", "questions": "1. What is the purpose of DisSim?\n- \"transform syntactically complex sentences\"\n\n2. How does DisSim create an intermediate representation?\n- \"two-layered semantic hierarchy\"\n\n3. What are the two layers in the semantic hierarchy?\n- \"core facts and accompanying contexts\"\n\n4. How does DisSim identify rhetorical relations?\n- \"identifying the rhetorical relations\"\n\n5. How does DisSim preserve the input's coherence structure?\n- \"preserve the coherence structure\"\n\n6. What are the downstream semantic applications for DisSim?\n- \"downstream semantic applications\"\n\n7. What challenges does DisSim address in English and German languages?\n- \"English and German\"\n\n8. How does DisSim contribute to the interpretability of input for downstream tasks?\n- \"interpretability for downstream tasks\""}
{"id": "1709.00947", "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects", "abstract": "This paper describes a preliminary study for producing and distributing a large-scale database of embeddings from the Portuguese Twitter stream. We start by experimenting with a relatively small sample and focusing on three challenges: volume of training data, vocabulary size and intrinsic evaluation metrics. Using a single GPU, we were able to scale up vocabulary size from 2048 words embedded and 500K training examples to 32768 words over 10M training examples while keeping a stable validation loss and approximately linear trend on training time per epoch. We also observed that using less than 50\\% of the available training examples for each vocabulary size might result in overfitting. Results on intrinsic evaluation show promising performance for a vocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics suffer from over-sensitivity to their corresponding cosine similarity thresholds, indicating that a wider range of metrics need to be developed to track progress.", "questions": "1. What are the main challenges in producing and distributing a large-scale database of embeddings from the Portuguese Twitter stream? (challenges: volume, vocabulary)\n\n2. How was the experiment conducted with a relatively small sample? (small sample)\n\n3. What were the results of scaling up vocabulary size and training examples? (scale up vocabulary)\n\n4. How did the training time per epoch change when scaling up vocabulary size? (approximately linear trend)\n\n5. What is the impact of using less than 50% of the available training examples for each vocabulary size? (result in overfitting)\n\n6. What were the results of the intrinsic evaluation for a vocabulary size of 32768 words? (promising performance)\n\n7. What issues were observed with intrinsic evaluation metrics? (over-sensitivity)\n\n8. What suggestions might the authors have for developing a wider range of metrics for evaluation? (wider range of metrics)"}
{"id": "1909.08859", "title": "Procedural Reasoning Networks for Understanding Multimodal Procedures", "abstract": "This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.", "questions": "1. What is the main problem addressed in this research paper?\n   - Phrase: comprehending procedural commonsense knowledge\n\n2. What are the challenges in comprehending procedural commonsense knowledge?\n   - Phrase: identifying key entities\n\n3. How does this study differ from previous work?\n   - Phrase: do not rely on strong inductive bias\n\n4. How does the proposed model exploit multimodality?\n   - Phrase: multimodality can be exploited\n\n5. What is the structure of the introduced entity-aware neural comprehension model?\n   - Phrase: external relational memory units\n\n6. How does the model dynamically update entity states?\n   - Phrase: dynamically update entity states\n\n7. What is the RecipeQA dataset, and how is it used in this study?\n   - Phrase: RecipeQA dataset\n\n8. How does the proposed approach improve the accuracy of previously reported models?\n   - Phrase: improves the accuracy\n\n9. What kind of supervision is not used in the model?\n   - Phrase: supervision at the level of entity states\n\n10. What are the implications of the model's effective dynamic representations of entities?\n   - Phrase: effective dynamic representations"}
{"id": "1908.08419", "title": "Active Learning for Chinese Word Segmentation in Medical Text", "abstract": "Electronic health records (EHRs) stored in hospital information systems completely reflect the patients' diagnosis and treatment processes, which are essential to clinical data mining. Chinese word segmentation (CWS) is a fundamental and important task for Chinese natural language processing. Currently, most state-of-the-art CWS methods greatly depend on large-scale manually-annotated data, which is a very time-consuming and expensive work, specially for the annotation in medical field. In this paper, we present an active learning method for CWS in medical text. To effectively utilize complete segmentation history, a new scoring model in sampling strategy is proposed, which combines information entropy with neural network. Besides, to capture interactions between adjacent characters, K-means clustering features are additionally added in word segmenter. We experimentally evaluate our proposed CWS method in medical text, experimental results based on EHRs collected from the Shuguang Hospital Affiliated to Shanghai University of Traditional Chinese Medicine show that our proposed method outperforms other reference methods, which can effectively save the cost of manual annotation.", "questions": "1. What is the importance of Chinese word segmentation in medical texts?\n   - Related phrase: \"fundamental and important\"\n\n2. Why is manual annotation time-consuming and expensive in the medical field?\n   - Related phrase: \"time-consuming and expensive\"\n\n3. How does the active learning method for CWS work?\n   - Related phrase: \"active learning method\"\n\n4. What is the new scoring model in the sampling strategy?\n   - Related phrase: \"new scoring model\"\n\n5. How does the combination of information entropy and neural network contribute to the method's effectiveness?\n   - Related phrase: \"information entropy\"\n\n6. What role do K-means clustering features play in the word segmenter?\n   - Related phrase: \"K-means clustering\"\n\n7. How was the proposed method experimentally evaluated?\n   - Related phrase: \"experimentally evaluate\"\n\n8. What were the results of the experiments based on EHRs from Shuguang Hospital?\n   - Related phrase: \"experimental results\"\n\n9. How does the proposed method compare with other reference methods?\n   - Related phrase: \"outperforms other reference\"\n\n10. How much cost savings can the proposed method offer in terms of manual annotation?\n    - Related phrase: \"save the cost\""}
{"id": "1703.05260", "title": "InScript: Narrative texts annotated with script information", "abstract": "This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.", "questions": "1. What is the main purpose of the InScript corpus?\n   - \"unique resource\"\n\n2. How many stories are included in the InScript corpus?\n   - \"1,000 stories\"\n\n3. What are the different scenarios covered by the InScript corpus?\n   - \"10 different scenarios\"\n\n4. How are verbs and noun phrases annotated in the corpus?\n   - \"event and participant types\"\n\n5. What additional information is annotated in the text?\n   - \"coreference information\"\n\n6. What is the significance of the rich lexical variation in the InScript corpus?\n   - \"rich lexical variation\"\n\n7. How can the InScript corpus contribute to the study of script knowledge in natural language processing?\n   - \"role of script knowledge\""}
{"id": "1905.00563", "title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications", "abstract": "Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving accuracy and overlook other aspects such as robustness and interpretability. In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained. Using these single modifications of the graph, we identify the most influential fact for a predicted link and evaluate the sensitivity of the model to the addition of fake facts. We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes. To avoid the combinatorial search over all possible facts, we train a network to decode embeddings to their corresponding graph components, allowing the use of gradient-based optimization to identify the adversarial modification. We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base.", "questions": "1. What is the main goal of the proposed adversarial modifications for link prediction models?\n   - \"adversarial modifications\"\n\n2. How does the paper estimate the effect of modifications on the knowledge graph?\n   - \"efficient approach\"\n\n3. How does the proposed approach avoid combinatorial search over all possible facts?\n   - \"gradient-based optimization\"\n\n4. What are the main applications of the techniques presented in the paper?\n   - \"evaluate robustness\", \"study interpretability\", \"detect incorrect facts\"\n\n5. How do the authors measure the sensitivity of the models to additional fake facts?\n   - \"sensitivity to additional facts\"\n\n6. What is the method used to identify the most influential neighbors?\n   - \"most influential neighbors\"\n\n7. How do the authors decode embeddings to their corresponding graph components?\n   - \"train a network\""}
{"id": "1808.05902", "title": "Learning Supervised Topic Models for Classification and Regression from Crowds", "abstract": "The growing need to analyze large collections of documents has led to great developments in topic modeling. Since documents are frequently associated with other related variables, such as labels or ratings, much interest has been placed on supervised topic models. However, the nature of most annotation tasks, prone to ambiguity and noise, often with high volumes of documents, deem learning under a single-annotator assumption unrealistic or unpractical for most real-world applications. In this article, we propose two supervised topic models, one for classification and another for regression problems, which account for the heterogeneity and biases among different annotators that are encountered in practice when learning from crowds. We develop an efficient stochastic variational inference algorithm that is able to scale to very large datasets, and we empirically demonstrate the advantages of the proposed model over state-of-the-art approaches.", "questions": "1. What are the proposed supervised topic models for classification and regression problems?\n   - Phrase: \"supervised topic models\"\n\n2. How do these models account for heterogeneity and biases among annotators?\n   - Phrase: \"heterogeneity and biases\"\n\n3. What is the stochastic variational inference algorithm developed in the paper?\n   - Phrase: \"stochastic variational inference\"\n\n4. How does the proposed model scale to very large datasets?\n   - Phrase: \"scale to very large datasets\"\n\n5. In what ways do the proposed models outperform the state-of-the-art approaches?\n   - Phrase: \"advantages of the proposed model\"\n\n6. What are some real-world applications of these supervised topic models?\n   - Phrase: \"real-world applications\"\n\n7. How does the single-annotator assumption limit the effectiveness of other supervised topic models?\n   - Phrase: \"single-annotator assumption\"\n\n8. What are the specific challenges posed by annotation tasks with respect to ambiguity and noise?\n   - Phrase: \"ambiguity and noise\""}
{"id": "2002.11893", "title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset", "abstract": "To advance multi-domain (cross-domain) dialogue modeling as well as alleviate the shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts at both user and system sides. About 60% of the dialogues have cross-domain user goals that favor inter-domain dependency and encourage natural transition across domains in conversation. We also provide a user simulator and several benchmark models for pipelined task-oriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus. The large size and rich annotation of CrossWOZ make it suitable to investigate a variety of tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc.", "questions": "1. What is the purpose of creating CrossWOZ?\n   - Phrase: advance multi-domain\n\n2. How many dialogue sessions and utterances are included in CrossWOZ?\n   - Phrase: 6K dialogue sessions\n\n3. Which domains are covered in CrossWOZ?\n   - Phrase: 5 domains\n\n4. What type of annotations does CrossWOZ contain?\n   - Phrase: rich annotation\n\n5. What is the percentage of dialogues with cross-domain user goals?\n   - Phrase: 60% of the dialogues\n\n6. What tasks can be investigated using CrossWOZ?\n   - Phrase: cross-domain dialogue modeling\n\n7. What is the purpose of the provided user simulator and benchmark models?\n   - Phrase: compare and evaluate\n\n8. What specific dialogue modeling tasks are suitable for CrossWOZ?\n   - Phrase: dialogue state tracking, policy learning, user simulation"}
{"id": "1910.07181", "title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "abstract": "Pretraining deep contextualized representations using an unsupervised language modeling objective has led to large performance gains for a variety of NLP tasks. Despite this success, recent work by Schick and Schutze (2019) suggests that these architectures struggle to understand rare words. For context-independent word embeddings, this problem can be addressed by separately learning representations for infrequent words. In this work, we show that the same idea can also be applied to contextualized models and clearly improves their downstream task performance. Most approaches for inducing word embeddings into existing embedding spaces are based on simple bag-of-words models; hence they are not a suitable counterpart for deep neural network language models. To overcome this problem, we introduce BERTRAM, a powerful architecture based on a pretrained BERT language model and capable of inferring high-quality representations for rare words. In BERTRAM, surface form and contexts of a word directly interact with each other in a deep architecture. Both on a rare word probing task and on three downstream task datasets, BERTRAM considerably improves representations for rare and medium frequency words compared to both a standalone BERT model and previous work.", "questions": "1. What are the performance gains observed in NLP tasks due to pretraining deep contextualized representations?\n   Related phrase: \"large performance gains\"\n\n2. How do contextualized models struggle with rare words, as suggested by Schick and Schutze?\n   Related phrase: \"struggle to understand\"\n\n3. How does separately learning representations for infrequent words help context-independent word embeddings?\n   Related phrase: \"infrequent words\"\n\n4. How does applying the idea of learning representations for infrequent words improve the downstream task performance of contextualized models?\n   Related phrase: \"clearly improves\"\n\n5. Why are simple bag-of-words models not suitable for inducing word embeddings into existing embedding spaces?\n   Related phrase: \"not a suitable\"\n\n6. What are the components and structure of the BERTRAM architecture?\n   Related phrase: \"introduce BERTRAM\"\n\n7. How does BERTRAM enable high-quality representations for rare words?\n   Related phrase: \"inferring high-quality\"\n\n8. How do surface form and contexts of a word interact in the BERTRAM architecture?\n   Related phrase: \"directly interact\"\n\n9. What are the results of the rare word probing task and the three downstream task datasets when using BERTRAM?\n   Related phrase: \"considerably improves\"\n\n10. How does BERTRAM compare to both a standalone BERT model and previous work in terms of rare and medium frequency word representations?\n   Related phrase: \"compared to both"}
{"id": "1902.00330", "title": "Joint Entity Linking with Deep Reinforcement Learning", "abstract": "Entity linking is the task of aligning mentions to corresponding entities in a given knowledge base. Previous studies have highlighted the necessity for entity linking systems to capture the global coherence. However, there are two common weaknesses in previous global models. First, most of them calculate the pairwise scores between all candidate entities and select the most relevant group of entities as the final result. In this process, the consistency among wrong entities as well as that among right ones are involved, which may introduce noise data and increase the model complexity. Second, the cues of previously disambiguated entities, which could contribute to the disambiguation of the subsequent mentions, are usually ignored by previous models. To address these problems, we convert the global linking into a sequence decision problem and propose a reinforcement learning model which makes decisions from a global perspective. Our model makes full use of the previous referred entities and explores the long-term influence of current selection on subsequent decisions. We conduct experiments on different types of datasets, the results show that our model outperforms state-of-the-art systems and has better generalization performance.", "questions": "1. What is entity linking?\n- \"Entity linking\"\n\n2. What are the common weaknesses in previous global models?\n- \"common weaknesses\"\n\n3. How do these weaknesses introduce noise data and increase model complexity?\n- \"introduce noise data\"\n\n4. How does the proposed reinforcement learning model address the problems?\n- \"reinforcement learning model\"\n\n5. In what way does the model make use of previous referred entities?\n- \"previous referred entities\"\n\n6. How does the model explore the long-term influence of current selection on subsequent decisions?\n- \"long-term influence\"\n\n7. What types of datasets were used in the experiments?\n- \"different types of datasets\"\n\n8. How does the model's performance compare to state-of-the-art systems?\n- \"outperforms state-of-the-art\"\n\n9. What is the model's generalization performance like?\n- \"better generalization performance\""}
{"id": "1909.00542", "title": "Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b", "abstract": "Task B Phase B of the 2019 BioASQ challenge focuses on biomedical question answering. Macquarie University's participation applies query-based multi-document extractive summarisation techniques to generate a multi-sentence answer given the question and the set of relevant snippets. In past participation we explored the use of regression approaches using deep learning architectures and a simple policy gradient architecture. For the 2019 challenge we experiment with the use of classification approaches with and without reinforcement learning. In addition, we conduct a correlation analysis between various ROUGE metrics and the BioASQ human evaluation scores.", "questions": "1. What is the focus of Task B Phase B in the 2019 BioASQ challenge?\n   - Related phrase: biomedical question answering\n\n2. How does Macquarie University's participation differ from previous approaches in this challenge?\n   - Related phrase: classification approaches\n\n3. What are the deep learning architectures used in regression approaches for this task?\n   - Related phrase: deep learning architectures\n\n4. What are the advantages of using classification approaches over regression approaches in this context?\n   - Related phrase: betters regression\n\n5. How does reinforcement learning impact the performance of classification approaches?\n   - Related phrase: reinforcement learning\n\n6. What are the various ROUGE metrics used in the correlation analysis?\n   - Related phrase: ROUGE metrics\n\n7. How do the BioASQ human evaluation scores compare to the performance of the proposed methods?\n   - Related phrase: BioASQ human evaluation scores"}
{"id": "1810.06743", "title": "Marrying Universal Dependencies and Universal Morphology", "abstract": "The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects each present schemata for annotating the morphosyntactic details of language. Each project also provides corpora of annotated text in many languages - UD at the token level and UniMorph at the type level. As each corpus is built by different annotators, language-specific decisions hinder the goal of universal schemata. With compatibility of tags, each project's annotations could be used to validate the other's. Additionally, the availability of both type- and token-level resources would be a boon to tasks such as parsing and homograph disambiguation. To ease this interoperability, we present a deterministic mapping from Universal Dependencies v2 features into the UniMorph schema. We validate our approach by lookup in the UniMorph corpora and find a macro-average of 64.13% recall. We also note incompatibilities due to paucity of data on either side. Finally, we present a critical evaluation of the foundations, strengths, and weaknesses of the two annotation projects.", "questions": "1. What are the main goals of Universal Dependencies and Universal Morphology projects?\n   - \"morphosyntactic details\"\n\n2. How do the projects differ in terms of annotation levels?\n   - \"token level\", \"type level\"\n\n3. What issues do language-specific decisions cause in achieving universal schemata?\n   - \"language-specific decisions\"\n\n4. What are the potential benefits of compatibility between UD and UniMorph tags?\n   - \"validate the other's\"\n\n5. What is the deterministic mapping's purpose in this study?\n   - \"deterministic mapping\"\n\n6. How effective is the proposed mapping between UD v2 features and UniMorph schema?\n   - \"64.13% recall\"\n\n7. What are some incompatibilities observed due to data limitations?\n   - \"paucity of data\"\n\n8. What insights are provided by the critical evaluation of the two annotation projects?\n   - \"critical evaluation\""}
{"id": "1909.02764", "title": "Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning", "abstract": "The recognition of emotions by humans is a complex process which considers multiple interacting signals such as facial expressions and both prosody and semantic content of utterances. Commonly, research on automatic recognition of emotions is, with few exceptions, limited to one modality. We describe an in-car experiment for emotion recognition from speech interactions for three modalities: the audio signal of a spoken interaction, the visual signal of the driver's face, and the manually transcribed content of utterances of the driver. We use off-the-shelf tools for emotion detection in audio and face and compare that to a neural transfer learning approach for emotion recognition from text which utilizes existing resources from other domains. We see that transfer learning enables models based on out-of-domain corpora to perform well. This method contributes up to 10 percentage points in F1, with up to 76 micro-average F1 across the emotions joy, annoyance and insecurity. Our findings also indicate that off-the-shelf-tools analyzing face and audio are not ready yet for emotion detection in in-car speech interactions without further adjustments.", "questions": "1. What is the purpose of the in-car experiment for emotion recognition?\n   - Related phrase: \"in-car experiment\"\n\n2. How many modalities were studied for emotion recognition from speech interactions?\n   - Related phrase: \"three modalities\"\n\n3. What are the specific emotions analyzed in this study?\n   - Related phrase: \"emotions joy, annoyance and insecurity\"\n\n4. Which off-the-shelf tools were used for emotion detection in audio and face?\n   - Related phrase: \"off-the-shelf tools\"\n\n5. How does the neural transfer learning approach work for emotion recognition from text?\n   - Related phrase: \"neural transfer learning approach\"\n\n6. What are the performance improvements achieved by using transfer learning?\n   - Related phrase: \"10 percentage points\"\n\n7. What are the limitations of the off-the-shelf-tools for emotion detection in in-car speech interactions?\n   - Related phrase: \"off-the-shelf-tools analyzing face and audio are not ready yet\"\n\n8. What were the adjustments needed for off-the-shelf-tools to improve emotion detection in in-car speech interactions?\n   - Related phrase: \"without further adjustments\"\n\n9. Which existing resources from other domains were utilized in the transfer learning approach?\n   - Related phrase: \"existing resources from other domains\""}
{"id": "1905.11901", "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study", "abstract": "It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (PBSMT) and requiring large amounts of auxiliary data to achieve competitive results. In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource NMT systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource NMT. In our experiments on German--English with different amounts of IWSLT14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized NMT system can outperform PBSMT with far less data than previously claimed. We also apply these techniques to a low-resource Korean-English dataset, surpassing previously reported results by 4 BLEU.", "questions": "1. What causes the performance drop in low-resource NMT systems?\n   - Related phrase: \"low-resource conditions\"\n\n2. How does PBSMT perform in comparison to low-resource NMT systems?\n   - Related phrase: \"underperforming PBSMT\"\n\n3. What are some pitfalls when training low-resource NMT systems?\n   - Related phrase: \"lack of system adaptation\"\n\n4. What recent techniques have been helpful in low-resource settings?\n   - Related phrase: \"recent techniques\"\n\n5. What best practices are recommended for low-resource NMT?\n   - Related phrase: \"best practices\"\n\n6. How much IWSLT14 training data was used in the experiments?\n   - Related phrase: \"different amounts\"\n\n7. How does the optimized NMT system compare to PBSMT in terms of data requirements?\n   - Related phrase: \"far less data\"\n\n8. What were the results of applying these techniques to the Korean-English dataset?\n   - Related phrase: \"4 BLEU\""}
{"id": "1912.01252", "title": "Facilitating on-line opinion dynamics by mining expressions of causation. The case of climate change debates on The Guardian", "abstract": "News website comment sections are spaces where potentially conflicting opinions and beliefs are voiced. Addressing questions of how to study such cultural and societal conflicts through technological means, the present article critically examines possibilities and limitations of machine-guided exploration and potential facilitation of on-line opinion dynamics. These investigations are guided by a discussion of an experimental observatory for mining and analyzing opinions from climate change-related user comments on news articles from the this http URL. This observatory combines causal mapping methods with computational text analysis in order to mine beliefs and visualize opinion landscapes based on expressions of causation. By (1) introducing digital methods and open infrastructures for data exploration and analysis and (2) engaging in debates about the implications of such methods and infrastructures, notably in terms of the leap from opinion observation to debate facilitation, the article aims to make a practical and theoretical contribution to the study of opinion dynamics and conflict in new media environments.", "questions": "1. What are the challenges in studying cultural and societal conflicts through technological means?\n   - \"study such cultural\"\n\n2. How does the experimental observatory work for mining and analyzing opinions?\n   - \"experimental observatory\"\n\n3. What are the specific methods used in computational text analysis for opinion mining?\n   - \"computational text analysis\"\n\n4. How are causal mapping methods combined with computational text analysis?\n   - \"causal mapping methods\"\n\n5. What news website is the source of climate change-related user comments?\n   - \"this http URL\"\n\n6. How are beliefs and opinions visualized in the opinion landscape?\n   - \"visualize opinion landscapes\"\n\n7. What digital methods and open infrastructures are introduced for data exploration and analysis?\n   - \"introducing digital methods\"\n\n8. What are the implications of using these methods for observing and facilitating debates?\n   - \"debate facilitation\"\n\n9. How does the article contribute to the study of opinion dynamics and conflict in new media environments?\n   - \"opinion dynamics and conflict\""}
{"id": "1912.13109", "title": "\"Hinglish\"Language -- Modeling a Messy Code-Mixed Language", "abstract": "With a sharp rise in fluency and users of \"Hinglish\" in linguistically diverse country, India, it has increasingly become important to analyze social content written in this language in platforms such as Twitter, Reddit, Facebook. This project focuses on using deep learning techniques to tackle a classification problem in categorizing social content written in Hindi-English into Abusive, Hate-Inducing and Not offensive categories. We utilize bi-directional sequence models with easy text augmentation techniques such as synonym replacement, random insertion, random swap, and random deletion to produce a state of the art classifier that outperforms the previous work done on analyzing this dataset.", "questions": "1. What is the significance of the rise in Hinglish fluency and users in India?\n   - \"Hinglish\" in linguistically diverse country\n\n2. Why is it important to analyze social content written in Hinglish?\n   - analyze social content\n\n3. Which platforms are being studied for Hinglish content?\n   - Twitter, Reddit, Facebook\n\n4. What are the three categories for classifying Hinglish social content?\n   - Abusive, Hate-Inducing, Not offensive\n\n5. What deep learning techniques are used in this project?\n   - bi-directional sequence models\n\n6. What text augmentation techniques are employed in the study?\n   - synonym replacement, random insertion, random swap, random deletion\n\n7. How does the state of the art classifier compare to previous work?\n   - outperforms the previous work\n\n8. What dataset is used for the analysis?\n   - analyzing this dataset"}
{"id": "1911.03310", "title": "How Language-Neutral is Multilingual BERT?", "abstract": "Multilingual BERT (mBERT) provides sentence representations for 104 languages, which are useful for many multi-lingual tasks. Previous work probed the cross-linguality of mBERT using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the semantic properties of mBERT. We show that mBERT representations can be split into a language-specific component and a language-neutral component, and that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment and sentence retrieval but is not yet good enough for the more difficult task of MT quality estimation. Our work presents interesting challenges which must be solved to build better language-neutral representations, particularly for tasks requiring linguistic transfer of semantics.", "questions": "1. What is mBERT's purpose?\n   - Phrase: \"sentence representations\"\n\n2. How did previous work test mBERT's cross-linguality?\n   - Phrase: \"zero-shot transfer\"\n\n3. What is the focus of this research paper?\n   - Phrase: \"semantic properties\"\n\n4. How can mBERT representations be divided?\n   - Phrase: \"language-specific component\"\n\n5. What tasks can the language-neutral component handle with high accuracy?\n   - Phrase: \"word-alignment and sentence retrieval\"\n\n6. Which task is the language-neutral component not yet good enough for?\n   - Phrase: \"MT quality estimation\"\n\n7. What challenges do better language-neutral representations face?\n   - Phrase: \"linguistic transfer of semantics\""}
{"id": "1907.12108", "title": "CAiRE: An End-to-End Empathetic Chatbot", "abstract": "In this paper, we present an end-to-end empathetic conversation agent CAiRE. Our system adapts TransferTransfo (Wolf et al., 2019) learning approach that fine-tunes a large-scale pre-trained language model with multi-task objectives: response language modeling, response prediction and dialogue emotion detection. We evaluate our model on the recently proposed empathetic-dialogues dataset (Rashkin et al., 2019), the experiment results show that CAiRE achieves state-of-the-art performance on dialogue emotion detection and empathetic response generation.", "questions": "1. What is TransferTransfo learning approach? (TransferTransfo)\n2. How does CAiRE fine-tune the large-scale pre-trained language model? (fine-tunes)\n3. What are the multi-task objectives in CAiRE's training? (multi-task objectives)\n4. What is the structure of the empathetic-dialogues dataset? (empathetic-dialogues dataset)\n5. How does CAiRE perform on response language modeling? (response language modeling)\n6. What is the methodology behind response prediction? (response prediction)\n7. How is dialogue emotion detection achieved in CAiRE? (dialogue emotion detection)\n8. In what ways does CAiRE outperform previous state-of-the-art models? (state-of-the-art performance)\n9. Are there any limitations or challenges in CAiRE's empathetic response generation? (empathetic response generation)"}
{"id": "2004.03685", "title": "Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?", "abstract": "With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is\"defined\"by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.", "questions": "1. What is interpretability in NLP systems?\n   Related phrase: \"what is interpretability\"\n\n2. How is faithfulness criteria defined?\n   Related phrase: \"faithfulness criteria\"\n\n3. What are the different desired criteria for interpretation?\n   Related phrase: \"different desired criteria\"\n\n4. What are the current approaches to faithfulness evaluation?\n   Related phrase: \"faithfulness evaluation\"\n\n5. What are the three assumptions mentioned in the literature survey?\n   Related phrase: \"three assumptions\"\n\n6. What are the concrete guidelines for evaluating interpretation methods?\n   Related phrase: \"concrete guidelines\"\n\n7. Why is the current binary definition of faithfulness potentially unrealistic?\n   Related phrase: \"binary definition\"\n\n8. How can a more graded notion of faithfulness be beneficial?\n   Related phrase: \"graded one\""}
{"id": "1808.03894", "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference", "abstract": "Deep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these models are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals. We present several examples for which our methods are able to reveal interesting insights and identify the critical information contributing to the model decisions.", "questions": "1. What is the main goal of this study?\n- \"explaining such deep learning based models\"\n\n2. Which specific neural model for NLI is being studied?\n- \"popular neural model\"\n\n3. How are the intermediate layers of NLI models interpreted?\n- \"visualizing the saliency\"\n\n4. What are the key components being visualized in the model?\n- \"attention and LSTM gating signals\"\n\n5. Are there any notable examples presented in the paper?\n- \"several examples\"\n\n6. What insights can be gained from the visualization methods?\n- \"interesting insights\"\n\n7. How does the proposed interpretation method help in understanding model decisions?\n- \"identify the critical information\""}
{"id": "1703.04617", "title": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering", "abstract": "The last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA). In this paper, we approach the problems by closely modelling questions in a neural network framework. We first introduce syntactic information to help encode questions. We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.", "questions": "1. What are the neural-network-based models used in machine comprehension and question answering? (Phrase: \"neural-network-based models\")\n\n2. How is syntactic information introduced to help encode questions? (Phrase: \"introduce syntactic information\")\n\n3. What are the different types of questions modeled in this research? (Phrase: \"different types of questions\")\n\n4. How do the proposed adaptation models work? (Phrase: \"proposed adaptation models\")\n\n5. What improvements do these approaches show on the Stanford Question Answering Dataset? (Phrase: \"better results\")\n\n6. How does the adaptation task help in understanding and modeling questions? (Phrase: \"an adaptation task\")\n\n7. What was the competitive baseline used for comparison? (Phrase: \"competitive baseline\")"}
{"id": "1909.00578", "title": "SUM-QE: a BERT-based Summary Quality Estimation Model", "abstract": "We propose SumQE, a novel Quality Estimation model for summarization based on BERT. The model addresses linguistic quality aspects that are only indirectly captured by content-based approaches to summary evaluation, without involving comparison with human references. SumQE achieves very high correlations with human ratings, outperforming simpler models addressing these linguistic aspects. Predictions of the SumQE model can be used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text.", "questions": "1. What is SumQE?\n   - \"a novel Quality Estimation model\"\n\n2. How does SumQE differ from content-based approaches?\n   - \"addresses linguistic quality aspects\"\n\n3. How does BERT contribute to the SumQE model?\n   - \"based on BERT\"\n\n4. How well does SumQE perform compared to simpler models?\n   - \"outperforming simpler models\"\n\n5. How are human ratings used in evaluating SumQE's performance?\n   - \"correlations with human ratings\"\n\n6. In what applications can SumQE's predictions be used?\n   - \"system development\"\n\n7. How can SumQE inform users about the quality of generated text?\n   - \"inform users of the quality\"\n\n8. Can SumQE be applied to other types of generated text besides summaries?\n   - \"other types of generated text\""}
{"id": "1911.09419", "title": "Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction", "abstract": "Knowledge graph embedding, which aims to represent entities and relations as low dimensional vectors (or matrices, tensors, etc.), has been shown to be a powerful technique for predicting missing links in knowledge graphs. Existing knowledge graph embedding models mainly focus on modeling relation patterns such as symmetry/antisymmetry, inversion, and composition. However, many existing approaches fail to model semantic hierarchies, which are common in real-world applications. To address this challenge, we propose a novel knowledge graph embedding model---namely, Hierarchy-Aware Knowledge Graph Embedding (HAKE)---which maps entities into the polar coordinate system. HAKE is inspired by the fact that concentric circles in the polar coordinate system can naturally reflect the hierarchy. Specifically, the radial coordinate aims to model entities at different levels of the hierarchy, and entities with smaller radii are expected to be at higher levels; the angular coordinate aims to distinguish entities at the same level of the hierarchy, and these entities are expected to have roughly the same radii but different angles. Experiments demonstrate that HAKE can effectively model the semantic hierarchies in knowledge graphs, and significantly outperforms existing state-of-the-art methods on benchmark datasets for the link prediction task.", "questions": "1. What are the existing knowledge graph embedding models?\n- Relation patterns\n\n2. How do current approaches fail to model semantic hierarchies?\n- Fail to model\n\n3. What is the main goal of the Hierarchy-Aware Knowledge Graph Embedding (HAKE) model?\n- Model semantic hierarchies\n\n4. How does the polar coordinate system help in modeling hierarchies?\n- Concentric circles\n\n5. What is the role of the radial coordinate in the HAKE model?\n- Model entities\n\n6. How does the angular coordinate distinguish entities at the same level?\n- Different angles\n\n7. What are the benchmark datasets used for evaluating the HAKE model?\n- Benchmark datasets\n\n8. How does the performance of HAKE compare to existing state-of-the-art methods in link prediction?\n- Significantly outperforms"}
{"id": "1910.11471", "title": "Machine Translation from Natural Language to Code using Long-Short Term Memory", "abstract": "Making computer programming language more understandable and easy for the human is a longstanding problem. From assembly language to present day’s object-oriented programming, concepts came to make programming easier so that a programmer can focus on the logic and the architecture rather than the code and language itself. To go a step further in this journey of removing human-computer language barrier, this paper proposes machine learning approach using Recurrent Neural Network (RNN) and Long-Short Term Memory (LSTM) to convert human language into programming language code. The programmer will write expressions for codes in layman’s language, and the machine learning model will translate it to the targeted programming language. The proposed approach yields result with 74.40% accuracy. This can be further improved by incorporating additional techniques, which are also discussed in this paper.", "questions": "1. What is the motivation behind making computer programming language more understandable?\n   - Abstract phrase: longstanding problem\n\n2. How do Recurrent Neural Networks and Long-Short Term Memory contribute to the proposed machine learning approach?\n   - Abstract phrase: RNN and LSTM\n\n3. What are the specific expressions or examples of human language used as input for this machine learning model?\n   - Abstract phrase: layman's language\n\n4. Which programming languages are targeted for translation by the machine learning model?\n   - Abstract phrase: targeted programming language\n\n5. How was the 74.40% accuracy achieved in the machine translation model?\n   - Abstract phrase: 74.40% accuracy\n\n6. What additional techniques are suggested for improving the accuracy of the machine translation model?\n   - Abstract phrase: additional techniques\n\n7. What are the implications of this research for future programming practices?\n   - Abstract phrase: removing human-computer language barrier"}
{"id": "1910.09399", "title": "A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis", "abstract": "Text-to-image synthesis refers to computational methods which translate human written textual descriptions, in the form of keywords or sentences, into images with similar semantic meaning to the text. In earlier research, image synthesis relied mainly on word to image correlation analysis combined with supervised methods to find best alignment of the visual content matching to the text. Recent progress in deep learning (DL) has brought a new set of unsupervised deep learning methods, particularly deep generative models which are able to generate realistic visual images using suitably trained neural network models. In this paper, we review the most recent development in the text-to-image synthesis research domain. Our survey first introduces image synthesis and its challenges, and then reviews key concepts such as generative adversarial networks (GANs) and deep convolutional encoder-decoder neural networks (DCNN). After that, we propose a taxonomy to summarize GAN based text-to-image synthesis into four major categories: Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs. We elaborate the main objective of each group, and further review typical GAN architectures in each group. The taxonomy and the review outline the techniques and the evolution of different approaches, and eventually provide a clear roadmap to summarize the list of contemporaneous solutions that utilize GANs and DCNNs to generate enthralling results in categories such as human faces, birds, flowers, room interiors, object reconstruction from edge maps (games) etc. The survey will conclude with a comparison of the proposed solutions, challenges that remain unresolved, and future developments in the text-to-image synthesis domain.", "questions": "1. What are the earlier research methods for image synthesis?\n   - Phrase: word to image correlation\n\n2. How have deep learning methods impacted text-to-image synthesis?\n   - Phrase: deep learning (DL)\n\n3. What are the key concepts in text-to-image synthesis, such as GANs and DCNNs?\n   - Phrase: generative adversarial networks (GANs), deep convolutional encoder-decoder neural networks (DCNN)\n\n4. What are the four major categories of GAN based text-to-image synthesis?\n   - Phrase: Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs\n\n5. What are the main objectives of each group in the proposed taxonomy?\n   - Phrase: main objective of each group\n\n6. How do the GAN architectures differ in each group?\n   - Phrase: typical GAN architectures\n\n7. What are some examples of categories where GANs and DCNNs have generated impressive results?\n   - Phrase: human faces, birds, flowers, room interiors, object reconstruction from edge maps (games)\n\n8. What are the challenges that remain unresolved in text-to-image synthesis?\n   - Phrase: challenges that remain unresolved\n\n9. What developments can be expected in the future for the text-to-image synthesis domain?\n   - Phrase: future developments"}
{"id": "1904.05584", "title": "Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study", "abstract": "In this paper we study how different ways of combining character and word-level representations affect the quality of both final word and sentence representations. We provide strong empirical evidence that modeling characters improves the learned representations at the word and sentence levels, and that doing so is particularly useful when representing less frequent words. We further show that a feature-wise sigmoid gating mechanism is a robust method for creating representations that encode semantic similarity, as it performed reasonably well in several word similarity datasets. Finally, our findings suggest that properly capturing semantic similarity at the word level does not consistently yield improved performance in downstream sentence-level tasks. Our code is available at https://github.com/jabalazs/gating", "questions": "1. What are the different ways of combining character and word-level representations?\n   - Phrase: \"combining character and word-level representations\"\n\n2. How does modeling characters improve learned representations?\n   - Phrase: \"modeling characters improves\"\n\n3. Why is representing less frequent words particularly useful?\n   - Phrase: \"less frequent words\"\n\n4. What is a feature-wise sigmoid gating mechanism?\n   - Phrase: \"sigmoid gating mechanism\"\n\n5. How was the performance of the gating mechanism evaluated?\n   - Phrase: \"word similarity datasets\"\n\n6. Why doesn't capturing semantic similarity at the word level consistently improve sentence-level tasks?\n   - Phrase: \"semantic similarity\"\n\n7. What are the specific downstream sentence-level tasks mentioned in the study?\n   - Phrase: \"downstream sentence-level tasks\"\n\n8. What are the main findings and conclusions of the study?\n   - Phrase: \"our findings suggest\""}
{"id": "1911.09886", "title": "Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction", "abstract": "A relation tuple consists of two entities and the relation between them, and often such tuples are found in unstructured text. There may be multiple relation tuples present in a text and they may share one or both entities among them. Extracting such relation tuples from a sentence is a difficult task and sharing of entities or overlapping entities among the tuples makes it more challenging. Most prior work adopted a pipeline approach where entities were identified first followed by finding the relations among them, thus missing the interaction among the relation tuples in a sentence. In this paper, we propose two approaches to use encoder-decoder architecture for jointly extracting entities and relations. In the first approach, we propose a representation scheme for relation tuples which enables the decoder to generate one word at a time like machine translation models and still finds all the tuples present in a sentence with full entity names of different length and with overlapping entities. Next, we propose a pointer network-based decoding approach where an entire tuple is generated at every time step. Experiments on the publicly available New York Times corpus show that our proposed approaches outperform previous work and achieve significantly higher F1 scores.", "questions": "1. What are relation tuples?\n   - Phrase: relation tuple\n\n2. What challenges arise when entities are shared among relation tuples?\n   - Phrase: overlapping entities\n\n3. How do previous pipeline approaches work for entity and relation extraction?\n   - Phrase: pipeline approach\n\n4. What are the two proposed approaches for using encoder-decoder architecture in this research?\n   - Phrase: two approaches\n\n5. How does the representation scheme for relation tuples enable the decoder to generate one word at a time?\n   - Phrase: representation scheme\n\n6. How does the pointer network-based decoding approach work?\n   - Phrase: pointer network-based\n\n7. What are the main differences between the first approach and the pointer network-based approach?\n   - Phrase: two approaches\n\n8. What dataset was used for the experiments in this research?\n   - Phrase: New York Times\n\n9. How do the proposed approaches compare to previous work in terms of performance and F1 scores?\n   - Phrase: outperform previous work"}
{"id": "1611.01400", "title": "Learning to Rank Scientific Documents from the Crowd", "abstract": "Finding related published articles is an important task in any science, but with the explosion of new work in the biomedical domain it has become especially challenging. Most existing methodologies use text similarity metrics to identify whether two articles are related or not. However biomedical knowledge discovery is hypothesis-driven. The most related articles may not be ones with the highest text similarities. In this study, we first develop an innovative crowd-sourcing approach to build an expert-annotated document-ranking corpus. Using this corpus as the gold standard, we then evaluate the approaches of using text similarity to rank the relatedness of articles. Finally, we develop and evaluate a new supervised model to automatically rank related scientific articles. Our results show that authors' ranking differ significantly from rankings by text-similarity-based models. By training a learning-to-rank model on a subset of the annotated corpus, we found the best supervised learning-to-rank model (SVM-Rank) significantly surpassed state-of-the-art baseline systems.", "questions": "1. What are the challenges in finding related published articles in the biomedical domain?\n   - \"especially challenging\"\n\n2. How do existing methodologies identify related articles?\n   - \"text similarity metrics\"\n\n3. What makes biomedical knowledge discovery hypothesis-driven?\n   - \"hypothesis-driven\"\n\n4. How was the expert-annotated document-ranking corpus developed?\n   - \"crowd-sourcing approach\"\n\n5. What are the limitations of using text similarity for ranking related articles?\n   - \"text similarity\"\n\n6. What is the new supervised model for ranking related scientific articles?\n   - \"supervised model\"\n\n7. How does the authors' ranking differ from text-similarity-based models?\n   - \"authors' ranking\"\n\n8. What is the best supervised learning-to-rank model found in the study?\n   - \"SVM-Rank\"\n\n9. How does the SVM-Rank model compare to state-of-the-art baseline systems?\n   - \"significantly surpassed\""}
{"id": "1808.05077", "title": "Exploiting Deep Learning for Persian Sentiment Analysis", "abstract": "The rise of social media is enabling people to freely express their opinions about products and services. The aim of sentiment analysis is to automatically determine subject's sentiment (e.g., positive, negative, or neutral) towards a particular aspect such as topic, product, movie, news etc. Deep learning has recently emerged as a powerful machine learning technique to tackle a growing demand of accurate sentiment analysis. However, limited work has been conducted to apply deep learning algorithms to languages other than English, such as Persian. In this work, two deep learning models (deep autoencoders and deep convolutional neural networks (CNNs)) are developed and applied to a novel Persian movie reviews dataset. The proposed deep learning models are analyzed and compared with the state-of-the-art shallow multilayer perceptron (MLP) based machine learning model. Simulation results demonstrate the enhanced performance of deep learning over state-of-the-art MLP.", "questions": "1. What is the main goal of sentiment analysis?\n   Related phrase: \"determine subject's sentiment\"\n\n2. How does deep learning improve sentiment analysis accuracy?\n   Related phrase: \"powerful machine learning technique\"\n\n3. Why has limited work been done on deep learning for non-English languages?\n   Related phrase: \"limited work\"\n\n4. What are the two deep learning models implemented in this study?\n   Related phrase: \"two deep learning models\"\n\n5. How does the performance of deep autoencoders and CNNs compare to the shallow MLP model?\n   Related phrase: \"compared with the state-of-the-art\"\n\n6. What is the size and nature of the Persian movie reviews dataset?\n   Related phrase: \"novel Persian movie reviews dataset\"\n\n7. What are the specific simulation results that demonstrate deep learning's superior performance?\n   Related phrase: \"Simulation results\""}
{"id": "1807.03367", "title": "Talk the Walk: Navigating New York City through Grounded Dialogue", "abstract": "We introduce\"Talk The Walk\", the first large-scale dialogue dataset grounded in action and perception. The task involves two agents (a\"guide\"and a\"tourist\") that communicate via natural language in order to achieve a common goal: having the tourist navigate to a given target location. The task and dataset, which are described in detail, are challenging and their full solution is an open problem that we pose to the community. We (i) focus on the task of tourist localization and develop the novel Masked Attention for Spatial Convolutions (MASC) mechanism that allows for grounding tourist utterances into the guide's map, (ii) show it yields significant improvements for both emergent and natural language communication, and (iii) using this method, we establish non-trivial baselines on the full task.", "questions": "1. What is the purpose of the \"Talk The Walk\" dataset?\n   - Related phrase: large-scale dialogue dataset\n\n2. How do the two agents (guide and tourist) communicate?\n   - Related phrase: communicate via natural language\n\n3. What is the common goal of the guide and tourist?\n   - Related phrase: navigate to a given target location\n\n4. What are the specific challenges of the task and dataset?\n   - Related phrase: challenging and their full solution\n\n5. What is the Masked Attention for Spatial Convolutions (MASC) mechanism?\n   - Related phrase: Masked Attention for Spatial Convolutions\n\n6. How does MASC help in grounding tourist utterances?\n   - Related phrase: grounding tourist utterances\n\n7. What improvements are observed using the MASC mechanism?\n   - Related phrase: significant improvements\n\n8. How do the baselines for the full task compare to other methods?\n   - Related phrase: non-trivial baselines"}
{"id": "1907.02030", "title": "Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks", "abstract": "Factchecking has always been a part of the journalistic process. However with newsroom budgets shrinking it is coming under increasing pressure just as the amount of false information circulating is on the rise. We therefore propose a method to increase the efficiency of the factchecking process, using the latest developments in Natural Language Processing (NLP). This method allows us to compare incoming claims to an existing corpus and return similar, factchecked, claims in a live system-allowing factcheckers to work simultaneously without duplicating their work.", "questions": "1. What is the main motivation for developing this method?\n   - Phrase: factchecking process\n\n2. How does this method improve the efficiency of factchecking?\n   - Phrase: increase the efficiency\n\n3. What are the key components of the proposed method?\n   - Phrase: Natural Language Processing\n\n4. How does the system handle incoming claims?\n   - Phrase: compare incoming claims\n\n5. What is the basis for comparison between claims and the existing corpus?\n   - Phrase: semantically-similar\n\n6. How does the live system benefit factcheckers?\n   - Phrase: work simultaneously\n\n7. What role does NLP play in the proposed method?\n   - Phrase: latest developments in NLP\n\n8. How does the method address the issue of duplicated work among factcheckers?\n   - Phrase: without duplicating their work\n\n9. Are there any limitations or challenges mentioned in the full text of the paper?\n   - Phrase: propose a method"}
{"id": "1910.04601", "title": "RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension", "abstract": "Recent studies revealed that reading comprehension (RC) systems learn to exploit annotation artifacts and other biases in current datasets. This allows systems to \"cheat\" by employing simple heuristics to answer questions, e.g. by relying on semantic type consistency. This means that current datasets are not well-suited to evaluate RC systems. To address this issue, we introduce RC-QED, a new RC task that requires giving not only the correct answer to a question, but also the reasoning employed for arriving at this answer. For this, we release a large benchmark dataset consisting of 12,000 answers and corresponding reasoning in form of natural language derivations. Experiments show that our benchmark is robust to simple heuristics and challenging for state-of-the-art neural path ranking approaches.", "questions": "1. What are the annotation artifacts and biases in current datasets?\n   Related phrase: \"annotation artifacts\"\n\n2. How do RC systems cheat using simple heuristics?\n   Related phrase: \"simple heuristics\"\n\n3. What is semantic type consistency?\n   Related phrase: \"semantic type consistency\"\n\n4. How is RC-QED designed to evaluate reasoning in RC systems?\n   Related phrase: \"RC-QED\"\n\n5. What does the RC-QED benchmark dataset consist of?\n   Related phrase: \"benchmark dataset\"\n\n6. How many answers and natural language derivations are included in the RC-QED dataset?\n   Related phrase: \"12,000 answers\"\n\n7. How does RC-QED prevent simple heuristics from being effective?\n   Related phrase: \"robust to simple heuristics\"\n\n8. How do state-of-the-art neural path ranking approaches perform on the RC-QED benchmark?\n   Related phrase: \"neural path ranking\""}
{"id": "1912.05066", "title": "Event Outcome Prediction using Sentiment Analysis and Crowd Wisdom in Microblog Feeds", "abstract": "Sentiment Analysis of microblog feeds has attracted considerable interest in recent times. Most of the current work focuses on tweet sentiment classification. But not much work has been done to explore how reliable the opinions of the mass (crowd wisdom) in social network microblogs such as twitter are in predicting outcomes of certain events such as election debates. In this work, we investigate whether crowd wisdom is useful in predicting such outcomes and whether their opinions are influenced by the experts in the field. We work in the domain of multi-label classification to perform sentiment classification of tweets and obtain the opinion of the crowd. This learnt sentiment is then used to predict outcomes of events such as: US Presidential Debate winners, Grammy Award winners, Super Bowl Winners. We find that in most of the cases, the wisdom of the crowd does indeed match with that of the experts, and in cases where they don't (particularly in the case of debates), we see that the crowd's opinion is actually influenced by that of the experts.", "questions": "1. What is the focus of most current work in sentiment analysis of microblog feeds?\n   - Related phrase: tweet sentiment classification\n\n2. How is crowd wisdom utilized in predicting event outcomes?\n   - Related phrase: predicting outcomes\n\n3. What methods are used for sentiment classification of tweets in this research?\n   - Related phrase: multi-label classification\n\n4. Which specific events are considered for outcome prediction in this study?\n   - Related phrase: US Presidential Debate winners, Grammy Award winners, Super Bowl Winners\n\n5. How does the crowd's wisdom compare to the opinions of experts in the field?\n   - Related phrase: wisdom of the crowd\n\n6. In what cases does the crowd's opinion not match with the experts' opinions?\n   - Related phrase: particularly in the case of debates\n\n7. How is the crowd's opinion influenced by experts?\n   - Related phrase: influenced by the experts"}
{"id": "1910.03891", "title": "Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding", "abstract": "The goal of representation learning of knowledge graph is to encode both entities and relations into a low-dimensional embedding spaces. Many recent works have demonstrated the benefits of knowledge graph embedding on knowledge graph completion task, such as relation extraction. However, we observe that: 1) existing method just take direct relations between entities into consideration and fails to express high-order structural relationship between entities; 2) these methods just leverage relation triples of KGs while ignoring a large number of attribute triples that encoding rich semantic information. To overcome these limitations, this paper propose a novel knowledge graph embedding method, named KANE, which is inspired by the recent developments of graph convolutional networks (GCN). KANE can capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner under the graph convolutional networks framework. Empirical results on three datasets show that KANE significantly outperforms seven state-of-arts methods. Further analysis verify the efficiency of our method and the benefits brought by the attention mechanism.", "questions": "1. What are the limitations of existing knowledge graph embedding methods?\n   - Phrase: existing method\n\n2. How does KANE address the shortcomings of current methods?\n   - Phrase: novel knowledge graph\n\n3. What is the role of graph convolutional networks in KANE?\n   - Phrase: graph convolutional networks\n\n4. How does KANE capture high-order structural relationships?\n   - Phrase: high-order structural\n\n5. What are the benefits of incorporating attribute information into knowledge graph embeddings?\n   - Phrase: attribute information\n\n6. How does the attention mechanism contribute to KANE's performance?\n   - Phrase: attention mechanism\n\n7. What are the three datasets used for empirical evaluation?\n   - Phrase: three datasets\n\n8. How does KANE compare to the seven state-of-art methods?\n   - Phrase: outperforms seven"}
{"id": "1610.00879", "title": "A Computational Approach to Automatic Prediction of Drunk Texting", "abstract": "Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk-texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N-gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk-texting.", "questions": "1. What is the motivation behind automatic drunk-texting prediction?\n   - Related phrase: \"unsociable behavior\"\n\n2. How were the tweets labeled for the study?\n   - Related phrase: \"hashtags as distant supervision\"\n\n3. What types of features were used in the classifiers?\n   - Related phrase: \"N-gram and stylistic features\"\n\n4. How effective were the classifiers in detecting drunk tweets?\n   - Related phrase: \"detect drunk tweets\"\n\n5. What is the significance of the findings in terms of quantitative evidence?\n   - Related phrase: \"first quantitative evidence\"\n\n6. Are there any potential applications of this automatic prediction system?\n   - Related phrase: \"automatic drunk-texting prediction\"\n\n7. What are the challenges or limitations faced in this approach?\n   - Related phrase: \"computational approach\""}
{"id": "1704.05572", "title": "Answering Complex Questions Using Open Information Extraction", "abstract": "While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrieval-based methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.", "questions": "1. What is the main limitation of using Open IE knowledge for QA?\n   - Phrase: simple questions\n\n2. How does the proposed method overcome this limitation?\n   - Phrase: reasoning with Open IE\n\n3. What is the support graph optimization framework used for?\n   - Phrase: support graph optimization\n\n4. What are the key features of the new inference model for Open IE?\n   - Phrase: new inference model\n\n5. How does the model handle multiple short facts, noise, and relational structure of tuples?\n   - Phrase: short facts, noise\n\n6. How does the model's performance compare to the state-of-the-art structured solver?\n   - Phrase: significantly outperforms\n\n7. What advantages does the model have in terms of reliance on manually curated knowledge?\n   - Phrase: removing the reliance"}
{"id": "1804.10686", "title": "An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages", "abstract": "In this paper, we present Watasense, an unsupervised system for word sense disambiguation. Given a sentence, the system chooses the most relevant sense of each input word with respect to the semantic similarity between the given sentence and the synset constituting the sense of the target word. Watasense has two modes of operation. The sparse mode uses the traditional vector space model to estimate the most similar word sense corresponding to its context. The dense mode, instead, uses synset embeddings to cope with the sparsity problem. We describe the architecture of the present system and also conduct its evaluation on three different lexical semantic resources for Russian. We found that the dense mode substantially outperforms the sparse one on all datasets according to the adjusted Rand index.", "questions": "1. What is Watasense?\n   - Phrase: \"unsupervised system\"\n\n2. How does Watasense determine the most relevant sense of a word?\n   - Phrase: \"semantic similarity\"\n\n3. What are the two modes of operation for Watasense?\n   - Phrase: \"sparse mode\"\n\n4. How does the sparse mode function?\n   - Phrase: \"vector space model\"\n\n5. How does the dense mode function?\n   - Phrase: \"synset embeddings\"\n\n6. What are the three lexical semantic resources used for evaluation?\n   - Phrase: \"resources for Russian\"\n\n7. How does the dense mode compare to the sparse mode in terms of performance?\n   - Phrase: \"dense mode substantially outperforms\"\n\n8. What is the adjusted Rand index?\n   - Phrase: \"adjusted Rand index\""}
{"id": "1707.03904", "title": "Quasar: Datasets for Question Answering by Search and Reading", "abstract": "We present two new large-scale datasets aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. The Quasar-S dataset consists of 37000 cloze-style (fill-in-the-gap) queries constructed from definitions of software entity tags on the popular website Stack Overflow. The posts and comments on the website serve as the background corpus for answering the cloze questions. The Quasar-T dataset consists of 43000 open-domain trivia questions and their answers obtained from various internet sources. ClueWeb09 serves as the background corpus for extracting these answers. We pose these datasets as a challenge for two related subtasks of factoid Question Answering: (1) searching for relevant pieces of text that include the correct answer to a query, and (2) reading the retrieved text to answer the query. We also describe a retrieval system for extracting relevant sentences and documents from the corpus given a query, and include these in the release for researchers wishing to only focus on (2). We evaluate several baselines on both datasets, ranging from simple heuristics to powerful neural models, and show that these lag behind human performance by 16.4% and 32.1% for Quasar-S and -T respectively. The datasets are available at https://github.com/bdhingra/quasar .", "questions": "1. What is the purpose of Quasar-S and Quasar-T datasets? (\"two new large-scale datasets\")\n2. How were the cloze-style queries in Quasar-S created? (\"software entity tags\")\n3. What is the source of posts and comments for the Quasar-S dataset? (\"Stack Overflow\")\n4. How were the trivia questions in Quasar-T obtained? (\"various internet sources\")\n5. What serves as the background corpus for Quasar-T dataset? (\"ClueWeb09\")\n6. What are the two subtasks of factoid Question Answering? (\"searching\" and \"reading\")\n7. What does the retrieval system described in the paper do? (\"extracting relevant sentences\")\n8. How do the baseline models perform compared to human performance? (\"lag behind human performance\")\n9. Where can the datasets be accessed? (\"https://github.com/bdhingra/quasar\")"}
{"id": "1911.07228", "title": "Error Analysis for Vietnamese Named Entity Recognition on Deep Neural Network Models", "abstract": "In recent years, Vietnamese Named Entity Recognition (NER) systems have had a great breakthrough when using Deep Neural Network methods. This paper describes the primary errors of the state-of-the-art NER systems on Vietnamese language. After conducting experiments on BLSTM-CNN-CRF and BLSTM-CRF models with different word embeddings on the Vietnamese NER dataset. This dataset is provided by VLSP in 2016 and used to evaluate most of the current Vietnamese NER systems. We noticed that BLSTM-CNN-CRF gives better results, therefore, we analyze the errors on this model in detail. Our error-analysis results provide us thorough insights in order to increase the performance of NER for the Vietnamese language and improve the quality of the corpus in the future works.", "questions": "1. What are the primary errors in Vietnamese NER systems? (\"primary errors\")\n\n2. How do BLSTM-CNN-CRF and BLSTM-CRF models perform on Vietnamese NER dataset? (\"conducting experiments\")\n\n3. What are the different word embeddings used in the experiments? (\"different word embeddings\")\n\n4. What makes BLSTM-CNN-CRF model better than BLSTM-CRF model? (\"BLSTM-CNN-CRF gives better results\")\n\n5. How can the error analysis results help improve NER performance for Vietnamese language? (\"thorough insights\")\n\n6. What specific improvements can be made to the corpus for future works? (\"improve the quality\")"}
{"id": "1603.07044", "title": "Recurrent Neural Network Encoder with Attention for Community Question Answering", "abstract": "We apply a general recurrent neural network (RNN) encoder framework to community question answering (cQA) tasks. Our approach does not rely on any linguistic processing, and can be applied to different languages or domains. Further improvements are observed when we extend the RNN encoders with a neural attention mechanism that encourages reasoning over entire sequences. To deal with practical issues such as data sparsity and imbalanced labels, we apply various techniques such as transfer learning and multitask learning. Our experiments on the SemEval-2016 cQA task show 10% improvement on a MAP score compared to an information retrieval-based approach, and achieve comparable performance to a strong handcrafted feature-based method.", "questions": "1. What is the general recurrent neural network (RNN) encoder framework used for community question answering?\n- \"RNN encoder framework\"\n\n2. How does the absence of linguistic processing affect the model's applicability?\n- \"without linguistic processing\"\n\n3. How does the neural attention mechanism improve the RNN encoders?\n- \"neural attention mechanism\"\n\n4. What specific transfer learning techniques are used to address data sparsity and imbalanced labels?\n- \"transfer learning\"\n\n5. How is multitask learning implemented in the approach?\n- \"multitask learning\"\n\n6. What were the results of the experiments on the SemEval-2016 cQA task?\n- \"SemEval-2016 cQA\"\n\n7. How does the performance of the proposed method compare to an information retrieval-based approach and a handcrafted feature-based method?\n- \"comparable performance\""}
{"id": "1902.09314", "title": "Attentional Encoder Network for Targeted Sentiment Classification", "abstract": "Targeted sentiment classification aims at determining the sentimental tendency towards specific targets. Most of the previous approaches model context and target words with RNN and attention. However, RNNs are difficult to parallelize and truncated backpropagation through time brings difficulty in remembering long-term patterns. To address this issue, this paper proposes an Attentional Encoder Network (AEN) which eschews recurrence and employs attention based encoders for the modeling between context and target. We raise the label unreliability issue and introduce label smoothing regularization. We also apply pre-trained BERT to this task and obtain new state-of-the-art results. Experiments and analysis demonstrate the effectiveness and lightweight of our model.", "questions": "1. What is targeted sentiment classification?\n   - \"Targeted sentiment classification\"\n\n2. What are the limitations of RNNs in this context?\n   - \"RNNs are difficult\"\n\n3. How does the Attentional Encoder Network address the limitations of RNNs?\n   - \"Attentional Encoder Network\"\n\n4. What is label unreliability issue?\n   - \"label unreliability issue\"\n\n5. How does label smoothing regularization work?\n   - \"label smoothing regularization\"\n\n6. How is pre-trained BERT applied in this task?\n   - \"apply pre-trained BERT\"\n\n7. What are the new state-of-the-art results achieved?\n   - \"new state-of-the-art results\"\n\n8. Can you provide details of the experiments and analysis conducted?\n   - \"Experiments and analysis\"\n\n9. How does the lightweight nature of the model benefit the overall performance?\n   - \"lightweight of our model\""}
{"id": "1904.03339", "title": "ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples", "abstract": "This paper describes our system, Joint Encoders for Stable Suggestion Inference (JESSI), for the SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums. JESSI is a combination of two sentence encoders: (a) one using multiple pre-trained word embeddings learned from log-bilinear regression (GloVe) and translation (CoVe) models, and (b) one on top of word encodings from a pre-trained deep bidirectional transformer (BERT). We include a domain adversarial training module when training for out-of-domain samples. Our experiments show that while BERT performs exceptionally well for in-domain samples, several runs of the model show that it is unstable for out-of-domain samples. The problem is mitigated tremendously by (1) combining BERT with a non-BERT encoder, and (2) using an RNN-based classifier on top of BERT. Our final models obtained second place with 77.78\\% F-Score on Subtask A (i.e. in-domain) and achieved an F-Score of 79.59\\% on Subtask B (i.e. out-of-domain), even without using any additional external data.", "questions": "1. What is the main purpose of JESSI in SemEval 2019 Task 9?\n   - \"Stable Suggestion Inference\"\n\n2. What are the two sentence encoders used in JESSI?\n   - \"two sentence encoders\"\n\n3. What are the pre-trained word embeddings used in the first encoder?\n   - \"GloVe and CoVe\"\n\n4. What is the pre-trained deep bidirectional transformer used in the second encoder?\n   - \"BERT\"\n\n5. How is the domain adversarial training module used in the system?\n   - \"training for out-of-domain samples\"\n\n6. What issues were observed with BERT for out-of-domain samples?\n   - \"unstable for out-of-domain samples\"\n\n7. What are the two approaches used to mitigate the instability of BERT?\n   - \"combining BERT with a non-BERT encoder\" and \"using an RNN-based classifier\"\n\n8. What were the F-Score results for Subtask A and Subtask B?\n   - \"77.78\\% F-Score\" and \"79.59\\% F-Score\"\n\n9. Did the final models use any additional external data?\n   - \"without using any additional external data\""}
{"id": "1910.11769", "title": "DENS: A Dataset for Multi-class Emotion Analysis", "abstract": "We introduce a new dataset for multi-class emotion analysis from long-form narratives in English. The Dataset for Emotions of Narrative Sequences (DENS) was collected from both classic literature available on Project Gutenberg and modern online narratives available on Wattpad, annotated using Amazon Mechanical Turk. A number of statistics and baseline benchmarks are provided for the dataset. Of the tested techniques, we find that the fine-tuning of a pre-trained BERT model achieves the best results, with an average micro-F1 score of 60.4%. Our results show that the dataset provides a novel opportunity in emotion analysis that requires moving beyond existing sentence-level techniques.", "questions": "1. What is the purpose of the DENS dataset?\n   - \"multi-class emotion analysis\"\n\n2. What sources were used for collecting the dataset?\n   - \"Project Gutenberg\", \"Wattpad\"\n\n3. How were the narratives annotated?\n   - \"Amazon Mechanical Turk\"\n\n4. What statistics and benchmarks are provided for DENS?\n   - \"statistics and baseline benchmarks\"\n\n5. Which model achieved the best results in the experiments?\n   - \"pre-trained BERT\"\n\n6. What was the average micro-F1 score for the best performing model?\n   - \"60.4%\"\n\n7. How does the DENS dataset contribute to emotion analysis research?\n   - \"novel opportunity\"\n\n8. What is the limitation of existing sentence-level techniques?\n   - \"moving beyond existing sentence-level techniques\""}
{"id": "1702.06378", "title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition", "abstract": "Segmental conditional random fields (SCRFs) and connectionist temporal classification (CTC) are two sequence labeling methods used for end-to-end training of speech recognition models. Both models define a transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a\"continuation\"of the previous label. In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network (RNN) encoder is used for feature extraction for both outputs. We find that this multitask objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model.", "questions": "1. What are segmental conditional random fields and connectionist temporal classification used for in speech recognition models?\n- \"sequence labeling methods\"\n\n2. How do both models define transcription probability?\n- \"latent segmentation alternatives\"\n\n3. What is the difference between SCRF and CTC in defining sequence probability?\n- \"segment labels and durations\" and \"output symbol or a 'continuation'\"\n\n4. How is the recognition model trained in this study?\n- \"optimizing an interpolation\"\n\n5. What is the role of the recurrent neural network encoder in the proposed method?\n- \"feature extraction for both outputs\"\n\n6. What is the impact of the multitask objective on recognition accuracy?\n- \"improves recognition accuracy\"\n\n7. How does CTC pretraining affect the RNN encoder's convergence rate?\n- \"improves the convergence rate\"\n\n8. What are the advantages of learning the joint model with CTC pretraining?\n- \"learning the joint model\""}
{"id": "1903.03467", "title": "Filling Gender&Number Gaps in Neural Machine Translation with Black-box Context Injection", "abstract": "When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must\"guess\"this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our method on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the translation accuracy in up to 2.3 BLEU on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method.", "questions": "1. What is the main problem addressed by the study?\n   - Phrase: incorrect translations\n\n2. How does the black-box approach work?\n   - Phrase: black-box approach\n\n3. What is the specific translation task used for evaluation?\n   - Phrase: English to Hebrew\n\n4. How significant is the improvement in translation accuracy?\n   - Phrase: up to 2.3 BLEU\n\n5. What are the components of the pre-trained neural machine translation system used?\n   - Phrase: pre-trained neural machine\n\n6. How does the method control morphological variations in generated translations?\n   - Phrase: control the morphological variations\n\n7. What is the impact of supplying correct gender and number information on translation accuracy?\n   - Phrase: improves the translation accuracy\n\n8. What are the findings from the fine-grained syntactic analysis?\n   - Phrase: fine-grained syntactic analysis\n\n9. Are there any limitations or challenges in implementing the proposed method?\n   - Phrase: our method\n\n10. How does the proposed method compare to other existing methods for addressing the same problem?\n    - Phrase: our method"}
{"id": "1807.00868", "title": "Exploring End-to-End Techniques for Low-Resource Speech Recognition", "abstract": "In this work we present simple grapheme-based system for low-resource speech recognition using Babel data for Turkish spontaneous speech (80 hours). We have investigated different neural network architectures performance, including fully-convolutional, recurrent and ResNet with GRU. Different features and normalization techniques are compared as well. We also proposed CTC-loss modification using segmentation during training, which leads to improvement while decoding with small beam size. Our best model achieved word error rate of 45.8%, which is the best reported result for end-to-end systems using in-domain data for this task, according to our knowledge.", "questions": "1. What is the Babel data used for Turkish spontaneous speech?\n   - \"Babel data\"\n\n2. Which neural network architectures were investigated in this research?\n   - \"different neural network architectures\"\n\n3. How does the fully-convolutional architecture perform compared to others?\n   - \"fully-convolutional\"\n\n4. What are the advantages of using ResNet with GRU?\n   - \"ResNet with GRU\"\n\n5. Which features and normalization techniques were compared in the study?\n   - \"features and normalization techniques\"\n\n6. How does the proposed CTC-loss modification work?\n   - \"CTC-loss modification\"\n\n7. What improvements were observed when decoding with small beam size?\n   - \"small beam size\"\n\n8. How does the best model's word error rate compare to other end-to-end systems?\n   - \"word error rate\"\n\n9. Are there any limitations or challenges faced with the best model?\n   - \"best model\"\n\n10. What future improvements or research directions can be explored for low-resource speech recognition?\n    - \"low-resource speech recognition\""}
{"id": "1909.13375", "title": "Tag-based Multi-Span Extraction in Reading Comprehension", "abstract": "With models reaching human performance on many popular reading comprehension datasets in recent years, a new dataset, DROP, introduced questions that were expected to present a harder challenge for reading comprehension models. Among these new types of questions were \"multi-span questions\", questions whose answers consist of several spans from either the paragraph or the question itself. Until now, only one model attempted to tackle multi-span questions as a part of its design. In this work, we suggest a new approach for tackling multi-span questions, based on sequence tagging, which differs from previous approaches for answering span questions. We show that our approach leads to an absolute improvement of 29.7 EM and 15.1 F1 compared to existing state-of-the-art results, while not hurting performance on other question types. Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataset.", "questions": "1. What is the DROP dataset?\n   - Phrase: \"DROP dataset\"\n\n2. How do multi-span questions differ from traditional reading comprehension questions?\n   - Phrase: \"multi-span questions\"\n\n3. What is the previous approach used for answering span questions?\n   - Phrase: \"previous approaches\"\n\n4. How does the new sequence tagging approach work?\n   - Phrase: \"sequence tagging\"\n\n5. What are the specific improvements in EM and F1 scores achieved by the new approach?\n   - Phrase: \"29.7 EM and 15.1 F1\"\n\n6. How does this new approach impact performance on other question types?\n   - Phrase: \"not hurting performance\"\n\n7. What is the current state-of-the-art result on the entire DROP dataset?\n   - Phrase: \"current state-of-the-art\""}
{"id": "1909.00430", "title": "Transfer Learning Between Related Tasks Using Expected Label Proportions", "abstract": "Deep learning systems thrive on abundance of labeled training data but such data is not always available, calling for alternative methods of supervision. One such method is expectation regularization (XR) (Mann and McCallum, 2007), where models are trained based on expected label proportions. We propose a novel application of the XR framework for transfer learning between related tasks, where knowing the labels of task A provides an estimation of the label proportion of task B. We then use a model trained for A to label a large corpus, and use this corpus with an XR loss to train a model for task B. To make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure. We demonstrate the approach on the task of Aspect-based Sentiment classification, where we effectively use a sentence-level sentiment predictor to train accurate aspect-based predictor. The method improves upon fully supervised neural system trained on aspect-level data, and is also cumulative with LM-based pretraining, as we demonstrate by improving a BERT-based Aspect-based Sentiment model.", "questions": "1. What is expectation regularization (XR)?\n- \"expectation regularization (XR)\"\n\n2. How does the proposed method apply XR for transfer learning?\n- \"novel application\"\n\n3. How does knowing the labels of task A help in estimating the label proportion of task B?\n- \"knowing the labels\"\n\n4. How is the XR framework made applicable to large-scale deep-learning setups?\n- \"stochastic batched approximation\"\n\n5. What is Aspect-based Sentiment classification?\n- \"Aspect-based Sentiment classification\"\n\n6. How does the method improve upon fully supervised neural systems?\n- \"improves upon fully supervised\"\n\n7. How does the approach compare to LM-based pretraining?\n- \"cumulative with LM-based\"\n\n8. What are the results of improving a BERT-based Aspect-based Sentiment model?\n- \"improving a BERT-based\""}
{"id": "1910.11493", "title": "The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection", "abstract": "The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years' inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on lemmatization and morphological feature analysis in context. All submissions featured a neural component and built on either this year's strong baselines or highly ranked systems from previous years' shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.", "questions": "1. What is the main goal of the SIGMORPHON 2019 shared task?\n   - \"cross-lingual transfer\"\n\n2. How does the first task differ from previous years' inflection tasks?\n   - \"evolves past years\"\n\n3. What is the new second challenge presented this year?\n   - \"lemmatization and morphological feature analysis\"\n\n4. What components did all the submissions share in their methodology?\n   - \"neural component\"\n\n5. What were the baselines for the inflection task and the contextual analysis task?\n   - \"strong baselines\"\n\n6. How did the participating teams perform compared to the baselines for the inflection task?\n   - \"improved in accuracy\"\n\n7. How did the participating teams perform compared to the baselines for the contextual analysis task?\n   - \"improved on both\""}
{"id": "1910.00912", "title": "Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU", "abstract": "We present a new neural architecture for wide-coverage Natural Language Understanding in Spoken Dialogue Systems. We develop a hierarchical multi-task architecture, which delivers a multi-layer representation of sentence meaning (i.e., Dialogue Acts and Frame-like structures). The architecture is a hierarchy of self-attention mechanisms and BiLSTM encoders followed by CRF tagging layers. We describe a variety of experiments, showing that our approach obtains promising results on a dataset annotated with Dialogue Acts and Frame Semantics. Moreover, we demonstrate its applicability to a different, publicly available NLU dataset annotated with domain-specific intents and corresponding semantic roles, providing overall performance higher than state-of-the-art tools such as RASA, Dialogflow, LUIS, and Watson. For example, we show an average 4.45% improvement in entity tagging F-score over Rasa, Dialogflow and LUIS.", "questions": "1. What is the new neural architecture developed for Natural Language Understanding in Spoken Dialogue Systems?\n- \"Hierarchical Multi-Task\"\n\n2. How does the hierarchical multi-task architecture represent sentence meaning?\n- \"multi-layer representation\"\n\n3. What are the key components in the architecture?\n- \"self-attention mechanisms\"\n\n4. What type of experiments were conducted to evaluate the architecture?\n- \"variety of experiments\"\n\n5. How does the approach perform on a dataset annotated with Dialogue Acts and Frame Semantics?\n- \"promising results\"\n\n6. What is the applicability of this approach to domain-specific intents and semantic roles?\n- \"publicly available NLU dataset\"\n\n7. How does the performance of this new architecture compare to existing tools like RASA, Dialogflow, LUIS, and Watson?\n- \"higher than state-of-the-art\"\n\n8. What is the improvement in entity tagging F-score over Rasa, Dialogflow, and LUIS?\n- \"average 4.45% improvement\""}
{"id": "1908.10449", "title": "Interactive Machine Comprehension with Information Seeking Agents", "abstract": "Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA). We argue that this stems from the nature of MRC datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed. In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially observable environments. Specifically, we \"occlude\" the majority of a document's text and add context-sensitive commands that reveal \"glimpses\" of the hidden text to a model. We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making. We believe that this setting can contribute in scaling models to web-level QA scenarios.", "questions": "1. What issues do existing MRC models face in real-world applications?\n   - \"do not scale\"\n\n2. How do the characteristics of MRC datasets contribute to these issues?\n   - \"nature of MRC datasets\"\n\n3. What method is proposed to reframe existing MRC datasets as interactive environments?\n   - \"simple method\"\n\n4. How is the majority of a document's text \"occluded\" in this method?\n   - \"occlude\" the majority\n\n5. What are the context-sensitive commands that reveal glimpses of the hidden text?\n   - \"context-sensitive commands\"\n\n6. How were SQuAD and NewsQA repurposed as an initial case study?\n   - \"repurpose SQuAD and NewsQA\"\n\n7. How does the interactive corpora train a model for sequential decision making?\n   - \"train a model\"\n\n8. In what ways can this setting contribute to scaling models for web-level QA scenarios?\n   - \"scaling models to web-level\""}
{"id": "1910.03814", "title": "Exploring Hate Speech Detection in Multimodal Publications", "abstract": "In this work we target the problem of hate speech detection in multimodal publications formed by a text and an image. We gather and annotate a large scale dataset from Twitter, MMHS150K, and propose different models that jointly analyze textual and visual information for hate speech detection, comparing them with unimodal detection. We provide quantitative and qualitative results and analyze the challenges of the proposed task. We find that, even though images are useful for the hate speech detection task, current multimodal models cannot outperform models analyzing only text. We discuss why and open the field and the dataset for further research.", "questions": "1. What is the main goal of this research?\n- \"hate speech detection\"\n\n2. How was the MMHS150K dataset created and what does it consist of?\n- \"gather and annotate\"\n\n3. What types of models were proposed for this task?\n- \"propose different models\"\n\n4. How do multimodal models compare to unimodal models in detecting hate speech?\n- \"comparing them with unimodal\"\n\n5. What were the quantitative and qualitative results of the study?\n- \"quantitative and qualitative results\"\n\n6. How useful are images for hate speech detection?\n- \"images are useful\"\n\n7. Why can't current multimodal models outperform text-only models?\n- \"cannot outperform models\"\n\n8. What are the challenges faced in this task?\n- \"analyze the challenges\"\n\n9. How does this study contribute to the field of hate speech detection?\n- \"open the field\""}
{"id": "1701.00185", "title": "Self-Taught Convolutional Neural Networks for Short Text Clustering", "abstract": "Short text clustering is a challenging problem due to its sparseness of text representation. Here we propose a flexible Self-Taught Convolutional neural network framework for Short Text Clustering (dubbed STC^2), which can flexibly and successfully incorporate more useful semantic features and learn non-biased deep text representation in an unsupervised manner. In our framework, the original raw text features are firstly embedded into compact binary codes by using one existing unsupervised dimensionality reduction methods. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, meanwhile the output units are used to fit the pre-trained binary codes in the training process. Finally, we get the optimal clusters by employing K-means to cluster the learned representations. Extensive experimental results demonstrate that the proposed framework is effective, flexible and outperform several popular clustering methods when tested on three public short text datasets.", "questions": "1. What makes short text clustering challenging?\n- Abstract phrase: sparseness of text\n\n2. How does the proposed STC^2 framework incorporate useful semantic features?\n- Abstract phrase: Self-Taught Convolutional\n\n3. What unsupervised dimensionality reduction methods are used for embedding raw text features into binary codes?\n- Abstract phrase: unsupervised dimensionality reduction\n\n4. How are word embeddings explored and used in the convolutional neural networks?\n- Abstract phrase: word embeddings\n\n5. What is the role of the output units in the training process?\n- Abstract phrase: fit the pre-trained binary codes\n\n6. How are the optimal clusters obtained?\n- Abstract phrase: employing K-means\n\n7. What are the three public short text datasets used for testing the proposed framework?\n- Abstract phrase: three public short text datasets\n\n8. How does the performance of the proposed framework compare to other popular clustering methods?\n- Abstract phrase: outperform several popular clustering methods"}
{"id": "1912.00871", "title": "Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations", "abstract": "Constructing accurate and automatic solvers of math word problems has proven to be quite challenging. Prior attempts using machine learning have been trained on corpora specific to math word problems to produce arithmetic expressions in infix notation before answer computation. We find that custom-built neural networks have struggled to generalize well. This paper outlines the use of Transformer networks trained to translate math word problems to equivalent arithmetic expressions in infix, prefix, and postfix notations. In addition to training directly on domain-specific corpora, we use an approach that pre-trains on a general text corpus to provide foundational language abilities to explore if it improves performance. We compare results produced by a large number of neural configurations and find that most configurations outperform previously reported approaches on three of four datasets with significant increases in accuracy of over 20 percentage points. The best neural approaches boost accuracy by almost 10% on average when compared to the previous state of the art.", "questions": "1. What were the limitations of prior machine learning attempts in solving arithmetic word problems?\n   - Prior attempts\n\n2. How do Transformer networks help in solving arithmetic word problems?\n   - Transformer networks\n\n3. What notations are used for representing arithmetic expressions in this approach?\n   - Infix, prefix, and postfix notations\n\n4. How does pre-training on a general text corpus improve performance?\n   - Foundational language abilities\n\n5. What neural configurations were compared in this study?\n   - Neural configurations\n\n6. How significant is the increase in accuracy using the proposed method?\n   - Over 20 percentage points\n\n7. How does the best neural approach compare to the previous state of the art?\n   - Boost accuracy by almost 10%"}
{"id": "1912.03234", "title": "What Do You Mean I'm Funny? Personalizing the Joke Skill of a Voice-Controlled Virtual Assistant", "abstract": "A considerable part of the success experienced by Voice-controlled virtual assistants (VVA) is due to the emotional and personalized experience they deliver, with humor being a key component in providing an engaging interaction. In this paper we describe methods used to improve the joke skill of a VVA through personalization. The first method, based on traditional NLP techniques, is robust and scalable. The others combine self-attentional network and multi-task learning to obtain better results, at the cost of added complexity. A significant challenge facing these systems is the lack of explicit user feedback needed to provide labels for the models. Instead, we explore the use of two implicit feedback-based labelling strategies. All models were evaluated on real production data. Online results show that models trained on any of the considered labels outperform a heuristic method, presenting a positive real-world impact on user satisfaction. Offline results suggest that the deep-learning approaches can improve the joke experience with respect to the other considered methods.", "questions": "1. What is the importance of humor in the interaction with voice-controlled virtual assistants?\n   - Related phrase: emotional and personalized experience\n\n2. What are the methods used to improve the joke skill of a VVA?\n   - Related phrase: methods used\n\n3. How do traditional NLP techniques contribute to the improvement?\n   - Related phrase: traditional NLP techniques\n\n4. What benefits do the self-attentional network and multi-task learning approaches provide?\n   - Related phrase: better results\n\n5. What are the challenges faced by these systems in terms of user feedback?\n   - Related phrase: lack of explicit user feedback\n\n6. What are the two implicit feedback-based labelling strategies explored?\n   - Related phrase: two implicit feedback-based labelling strategies\n\n7. How were the models evaluated?\n   - Related phrase: real production data\n\n8. What are the online and offline results in terms of user satisfaction and joke experience?\n   - Related phrase: real-world impact on user satisfaction\n\n9. How does the deep-learning approach compare to the other methods considered?\n   - Related phrase: deep-learning approaches"}
{"id": "1911.11750", "title": "A Measure of Similarity in Textual Data Using Spearman's Rank Correlation Coefficient", "abstract": "In the last decade, many diverse advances have occurred in the field of information extraction from data. Information extraction in its simplest form takes place in computing environments, where structured data can be extracted through a series of queries. The continuous expansion of quantities of data have therefore provided an opportunity for knowledge extraction (KE) from a textual document (TD). A typical problem of this kind is the extraction of common characteristics and knowledge from a group of TDs, with the possibility to group such similar TDs in a process known as clustering. In this paper we present a technique for such KE among a group of TDs related to the common characteristics and meaning of their content. Our technique is based on the Spearman's Rank Correlation Coefficient (SRCC), for which the conducted experiments have proven to be comprehensive measure to achieve a high-quality KE.", "questions": "1. What are the diverse advances in information extraction?\n- \"diverse advances\"\n\n2. How does the technique using SRCC work for KE?\n- \"technique is based\"\n\n3. How is information extraction performed in computing environments?\n- \"computing environments\"\n\n4. What are the challenges in extracting knowledge from textual documents?\n- \"knowledge extraction (KE)\"\n\n5. How does the proposed method improve clustering of similar TDs?\n- \"group such similar\"\n\n6. What are the common characteristics considered for KE among TDs?\n- \"common characteristics\"\n\n7. How were the experiments conducted to test the SRCC-based technique?\n- \"conducted experiments\"\n\n8. What makes SRCC a comprehensive measure for high-quality KE?\n- \"comprehensive measure\""}
{"id": "1911.03894", "title": "CamemBERT: a Tasty French Language Model", "abstract": "Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models—in all languages except English—very limited. Aiming to address this issue for French, we release CamemBERT, a French version of the Bi-directional Encoders for Transformers (BERT). We measure the performance of CamemBERT compared to multilingual models in multiple downstream tasks, namely part-of-speech tagging, dependency parsing, named-entity recognition, and natural language inference. CamemBERT improves the state of the art for most of the tasks considered. We release the pretrained model for CamemBERT hoping to foster research and downstream applications for French NLP.", "questions": "1. What are pretrained language models?\n   - Related phrase: Pretrained language models\n\n2. What is the main limitation of most available models?\n   - Related phrase: very limited\n\n3. How is CamemBERT different from other language models?\n   - Related phrase: French version\n\n4. What is the Bi-directional Encoders for Transformers (BERT)?\n   - Related phrase: Transformers (BERT)\n\n5. What downstream tasks were used to measure CamemBERT's performance?\n   - Related phrase: downstream tasks\n\n6. How does CamemBERT compare to multilingual models in performance?\n   - Related phrase: compared to multilingual models\n\n7. In which tasks does CamemBERT improve the state of the art?\n   - Related phrase: state of the art\n\n8. What are the potential applications for French NLP using CamemBERT?\n   - Related phrase: downstream applications"}
{"id": "2001.09899", "title": "Vocabulary-based Method for Quantifying Controversy in Social Media", "abstract": "Identifying controversial topics is not only interesting from a social point of view, it also enables the application of methods to avoid the information segregation, creating better discussion contexts and reaching agreements in the best cases. In this paper we develop a systematic method for controversy detection based primarily on the jargon used by the communities in social media. Our method dispenses with the use of domain-specific knowledge, is language-agnostic, efficient and easy to apply. We perform an extensive set of experiments across many languages, regions and contexts, taking controversial and non-controversial topics. We find that our vocabulary-based measure performs better than state of the art measures that are based only on the community graph structure. Moreover, we shows that it is possible to detect polarization through text analysis.", "questions": "1. What is the importance of identifying controversial topics in social media?\n   Phrase: \"social point of view\"\n\n2. How does the proposed method help in avoiding information segregation?\n   Phrase: \"avoid the information segregation\"\n\n3. What are the key features of the vocabulary-based controversy detection method?\n   Phrase: \"language-agnostic, efficient\"\n\n4. How did the researchers test their method across different languages and regions?\n   Phrase: \"many languages, regions\"\n\n5. How does the vocabulary-based measure compare to other measures in terms of performance?\n   Phrase: \"performs better\"\n\n6. Can the method detect polarization through text analysis?\n   Phrase: \"detect polarization\"\n\n7. What are some examples of controversial and non-controversial topics used in the experiments?\n   Phrase: \"controversial and non-controversial topics\"\n\n8. How is the method designed to be easy to apply?\n   Phrase: \"easy to apply\"\n\n9. What are the limitations of state-of-the-art measures that rely on community graph structure?\n   Phrase: \"community graph structure\"\n\n10. How does the vocabulary-based method contribute to creating better discussion contexts?\n   Phrase: \"better discussion contexts\""}
{"id": "1710.01492", "title": "Semantic Sentiment Analysis of Twitter Data", "abstract": "Internet and the proliferation of smart mobile devices have changed the way information is created, shared, and spreads, e.g., microblogs such as Twitter, weblogs such as LiveJournal, social networks such as Facebook, and instant messengers such as Skype and WhatsApp are now commonly used to share thoughts and opinions about anything in the surrounding world. This has resulted in the proliferation of social media content, thus creating new opportunities to study public opinion at a scale that was never possible before. Naturally, this abundance of data has quickly attracted business and research interest from various fields including marketing, political science, and social studies, among many others, which are interested in questions like these: Do people like the new Apple Watch? Do Americans support ObamaCare? How do Scottish feel about the Brexit? Answering these questions requires studying the sentiment of opinions people express in social media, which has given rise to the fast growth of the field of sentiment analysis in social media, with Twitter being especially popular for research due to its scale, representativeness, variety of topics discussed, as well as ease of public access to its messages. Here we present an overview of work on sentiment analysis on Twitter.", "questions": "1. What is the significance of studying public opinion in social media?\n   Phrase: study public opinion\n\n2. How has the abundance of social media data attracted various fields of interest?\n   Phrase: attracted business and research\n\n3. What are the main questions driving sentiment analysis research in social media?\n   Phrase: questions like these\n\n4. Why is Twitter a popular platform for sentiment analysis research?\n   Phrase: especially popular for research\n\n5. What factors make Twitter suitable for sentiment analysis studies?\n   Phrase: scale, representativeness, variety\n\n6. What will be the focus of the presented overview on sentiment analysis on Twitter?\n   Phrase: present an overview"}
{"id": "1912.01673", "title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations", "abstract": "COSTRA 1.0 is a dataset of Czech complex sentence transformations. The dataset is intended for the study of sentence-level embeddings beyond simple word alternations or standard paraphrasing.  ::: The dataset consist of 4,262 unique sentences with average length of 10 words, illustrating 15 types of modifications such as simplification, generalization, or formal and informal language variation.  ::: The hope is that with this dataset, we should be able to test semantic properties of sentence embeddings and perhaps even to find some topologically interesting “skeleton” in the sentence embedding space.", "questions": "1. What is the purpose of COSTRA 1.0?\n   - \"sentence-level embeddings\"\n\n2. How many sentences are included in the dataset?\n   - \"4,262 unique sentences\"\n\n3. What is the average length of sentences in the dataset?\n   - \"average length of 10 words\"\n\n4. How many types of modifications are illustrated in the dataset?\n   - \"15 types of modifications\"\n\n5. What are some examples of modifications included in the dataset?\n   - \"simplification, generalization\"\n\n6. What are the potential applications of this dataset?\n   - \"test semantic properties\"\n\n7. What is the significance of finding a \"skeleton\" in the sentence embedding space?\n   - \"topologically interesting 'skeleton'\"\n\n8. Does the dataset include both formal and informal language variations?\n   - \"formal and informal language variation\""}
{"id": "1909.12231", "title": "Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization", "abstract": "Linking facts across documents is a challenging task, as the language used to express the same information in a sentence can vary significantly, which complicates the task of multi-document summarization. Consequently, existing approaches heavily rely on hand-crafted features, which are domain-dependent and hard to craft, or additional annotated data, which is costly to gather. To overcome these limitations, we present a novel method, which makes use of two types of sentence embeddings: universal embeddings, which are trained on a large unrelated corpus, and domain-specific embeddings, which are learned during training.  ::: To this end, we develop SemSentSum, a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of summary, consisting of 665 bytes and 100 words. Unlike other state-of-the-art models, neither hand-crafted features nor additional annotated data are necessary, and the method is easily adaptable for other tasks. To our knowledge, we are the first to use multiple sentence embeddings for the task of multi-document summarization.", "questions": "1. What are the challenges in multi-document summarization?\n   - Abstract phrase: \"language used to express\"\n\n2. What are the limitations of existing approaches?\n   - Abstract phrase: \"heavily rely on\"\n\n3. How does the novel method overcome these limitations?\n   - Abstract phrase: \"two types of\"\n\n4. What are universal embeddings?\n   - Abstract phrase: \"universal embeddings\"\n\n5. What are domain-specific embeddings?\n   - Abstract phrase: \"domain-specific embeddings\"\n\n6. How is SemSentSum developed?\n   - Abstract phrase: \"we develop SemSentSum\"\n\n7. What type of sentence embeddings does SemSentSum leverage?\n   - Abstract phrase: \"both types of\"\n\n8. What are the results of using SemSentSum on two types of summaries?\n   - Abstract phrase: \"competitive results\"\n\n9. How does SemSentSum compare to state-of-the-art models?\n   - Abstract phrase: \"Unlike other state-of-the-art\"\n\n10. In what other tasks can the method be adaptable?\n    - Abstract phrase: \"easily adaptable for\"\n\n11. Are there any other approaches using multiple sentence embeddings for multi-document summarization?\n    - Abstract phrase: \"first to use\""}
{"id": "1706.08032", "title": "A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking", "abstract": "This paper introduces a novel deep learning framework including a lexicon-based approach for sentence-level prediction of sentiment label distribution. We propose to first apply semantic rules and then use a Deep Convolutional Neural Network (DeepCNN) for character-level embeddings in order to increase information for word-level embedding. After that, a Bidirectional Long Short-Term Memory Network (Bi-LSTM) produces a sentence-wide feature representation from the word-level embedding. We evaluate our approach on three Twitter sentiment classification datasets. Experimental results show that our model can improve the classification accuracy of sentence-level sentiment analysis in Twitter social networking.", "questions": "1. What is the novel deep learning framework introduced in this paper?\n   Phrase: deep learning framework\n\n2. How does the lexicon-based approach contribute to sentiment label prediction?\n   Phrase: lexicon-based approach\n\n3. What semantic rules are applied in the framework?\n   Phrase: semantic rules\n\n4. How do Deep Convolutional Neural Networks help in increasing information for word-level embedding?\n   Phrase: DeepCNN\n\n5. What is the role of Bidirectional Long Short-Term Memory Network in the framework?\n   Phrase: Bi-LSTM\n\n6. Which three Twitter sentiment classification datasets were used for evaluation?\n   Phrase: three Twitter datasets\n\n7. How much improvement in classification accuracy was observed in the experiments?\n   Phrase: improve classification accuracy\n\n8. How does this framework compare to other existing methods for sentence-level sentiment analysis in Twitter social networking?\n   Phrase: sentiment analysis in Twitter"}
{"id": "1811.01399", "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding", "abstract": "Knowledge graph embedding aims at modeling entities and relations with low-dimensional vectors. Most previous methods require that all entities should be seen during training, which is unpractical for real-world knowledge graphs with new entities emerging on a daily basis. Recent efforts on this issue suggest training a neighborhood aggregator in conjunction with the conventional entity and relation embeddings, which may help embed new entities inductively via their existing neighbors. However, their neighborhood aggregators neglect the unordered and unequal natures of an entity's neighbors. To this end, we summarize the desired properties that may lead to effective neighborhood aggregators. We also introduce a novel aggregator, namely, Logic Attention Network (LAN), which addresses the properties by aggregating neighbors with both rules- and network-based attention weights. By comparing with conventional aggregators on two knowledge graph completion tasks, we experimentally validate LAN's superiority in terms of the desired properties.", "questions": "1. What are the limitations of previous knowledge graph embedding methods?\n   - Phrase: all entities should be seen\n\n2. How do neighborhood aggregators help in embedding new entities?\n   - Phrase: embed new entities inductively\n\n3. What are the issues with existing neighborhood aggregators?\n   - Phrase: neglect unordered and unequal\n\n4. What are the desired properties for effective neighborhood aggregators?\n   - Phrase: desired properties\n\n5. How does the Logic Attention Network (LAN) improve neighborhood aggregation?\n   - Phrase: Logic Attention Network\n\n6. What are the rules- and network-based attention weights in LAN?\n   - Phrase: rules- and network-based attention weights\n\n7. How does LAN's performance compare to conventional aggregators in knowledge graph completion tasks?\n   - Phrase: LAN's superiority\n\n8. What are the specific knowledge graph completion tasks used in the experiments?\n   - Phrase: two knowledge graph completion tasks"}
{"id": "1909.00124", "title": "Learning with Noisy Labels for Sentence-level Sentiment Classification", "abstract": "Deep neural networks (DNNs) can fit (or even over-fit) the training data very well. If a DNN model is trained using data with noisy labels and tested on data with clean labels, the model may perform poorly. This paper studies the problem of learning with noisy labels for sentence-level sentiment classification. We propose a novel DNN model called NetAb (as shorthand for convolutional neural Networks with Ab-networks) to handle noisy labels during training. NetAb consists of two convolutional neural networks, one with a noise transition layer for dealing with the input noisy labels and the other for predicting 'clean' labels. We train the two networks using their respective loss functions in a mutual reinforcement manner. Experimental results demonstrate the effectiveness of the proposed model.", "questions": "1. What are the challenges of training deep neural networks with noisy labels?\n   - Phrase: noisy labels\n\n2. How does the proposed NetAb model address the issue of learning with noisy labels?\n   - Phrase: NetAb model\n\n3. What are the key components of the NetAb model?\n   - Phrase: two convolutional neural networks\n\n4. How does the noise transition layer function within the NetAb model?\n   - Phrase: noise transition layer\n\n5. What are the specific loss functions used for training the two networks in the NetAb model?\n   - Phrase: respective loss functions\n\n6. How does the mutual reinforcement training process work for the two networks in the NetAb model?\n   - Phrase: mutual reinforcement manner\n\n7. What are the experimental results that demonstrate the effectiveness of the proposed model?\n   - Phrase: Experimental results\n\n8. How does the performance of the NetAb model compare to other DNN models for sentence-level sentiment classification?\n   - Phrase: sentence-level sentiment classification"}
{"id": "1909.00088", "title": "Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange", "abstract": "In this paper, we present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency, a task we call semantic text exchange. This is useful for text data augmentation and the semantic correction of text generated by chatbots and virtual assistants. We introduce a pipeline called SMERTI that combines entity replacement, similarity masking, and text infilling. We measure our pipeline's success by its Semantic Text Exchange Score (STES): the ability to preserve the original text's sentiment and fluency while adjusting semantic content. We propose to use masking (replacement) rate threshold as an adjustable parameter to control the amount of semantic change in the text. Our experiments demonstrate that SMERTI can outperform baseline models on Yelp reviews, Amazon reviews, and news headlines.", "questions": "1. What is the purpose of semantic text exchange?\n   - Phrase: text data augmentation\n\n2. How can semantic text exchange benefit chatbots and virtual assistants?\n   - Phrase: semantic correction\n\n3. What are the main components of the SMERTI pipeline?\n   - Phrase: entity replacement, similarity masking, and text infilling\n\n4. What is the Semantic Text Exchange Score (STES)?\n   - Phrase: Semantic Text Exchange Score\n\n5. How is the masking (replacement) rate threshold used in this method?\n   - Phrase: adjustable parameter\n\n6. In what domains were the experiments conducted?\n   - Phrase: Yelp reviews, Amazon reviews, and news headlines\n\n7. How does SMERTI compare to the baseline models in terms of performance?\n   - Phrase: outperform baseline models"}
{"id": "1911.01799", "title": "CN-CELEB: a challenging Chinese speaker recognition dataset", "abstract": "Recently, researchers set an ambitious goal of conducting speaker recognition in unconstrained conditions where the variations on ambient, channel and emotion could be arbitrary. However, most publicly available datasets are collected under constrained environments, i.e., with little noise and limited channel variation. These datasets tend to deliver over optimistic performance and do not meet the request of research on speaker recognition in unconstrained conditions. In this paper, we present CN-Celeb, a large-scale speaker recognition dataset collected `in the wild'. This dataset contains more than 130,000 utterances from 1,000 Chinese celebrities, and covers 11 different genres in real world. Experiments conducted with two state-of-the-art speaker recognition approaches (i-vector and x-vector) show that the performance on CN-Celeb is far inferior to the one obtained on VoxCeleb, a widely used speaker recognition dataset. This result demonstrates that in real-life conditions, the performance of existing techniques might be much worse than it was thought. Our database is free for researchers and can be downloaded from this http URL.", "questions": "1. What is the main goal of conducting speaker recognition in unconstrained conditions?\nRelated phrase: \"unconstrained conditions\"\n\n2. Why do most publicly available datasets not meet the requirements for unconstrained speaker recognition research?\nRelated phrase: \"constrained environments\"\n\n3. What are the specific features of the CN-Celeb dataset in terms of size and diversity?\nRelated phrase: \"1,000 Chinese celebrities\"\n\n4. How many different genres are included in the CN-Celeb dataset?\nRelated phrase: \"11 different genres\"\n\n5. What are the two state-of-the-art speaker recognition approaches used in the experiments?\nRelated phrase: \"i-vector and x-vector\"\n\n6. How does the performance of these approaches on CN-Celeb compare to their performance on VoxCeleb?\nRelated phrase: \"far inferior\"\n\n7. What is the potential impact of the real-life conditions on the performance of existing speaker recognition techniques?\nRelated phrase: \"much worse\"\n\n8. How can researchers access the CN-Celeb dataset?\nRelated phrase: \"free for researchers\""}
{"id": "1812.06705", "title": "Conditional BERT Contextual Augmentation", "abstract": "We propose a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation. Data augmentation methods are often applied to prevent overfitting and improve generalization of deep neural network models. Recently proposed contextual augmentation augments labeled sentences by randomly replacing words with more varied substitutions predicted by language model. BERT demonstrates that a deep bidirectional language model is more powerful than either an unidirectional language model or the shallow concatenation of a forward and backward model. We retrofit BERT to conditional BERT by introducing a new conditional masked language model\\footnote{The term\"conditional masked language model\"appeared once in original BERT paper, which indicates context-conditional, is equivalent to term\"masked language model\". In our paper,\"conditional masked language model\"indicates we apply extra label-conditional constraint to the\"masked language model\".} task. The well trained conditional BERT can be applied to enhance contextual augmentation. Experiments on six various different text classification tasks show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain obvious improvement.", "questions": "1. What is the main goal of the proposed data augmentation method?\n   - \"conditional BERT contextual augmentation\"\n\n2. How does data augmentation help deep neural network models?\n   - \"improve generalization\"\n\n3. What is the difference between BERT and conditional BERT in this paper?\n   - \"retrofit BERT\"\n\n4. How does the conditional masked language model task relate to BERT?\n   - \"conditional masked language model\"\n\n5. What are the benefits of using the well-trained conditional BERT for contextual augmentation?\n   - \"enhance contextual augmentation\"\n\n6. On which text classification tasks were the experiments conducted?\n   - \"six various different text classification tasks\"\n\n7. How does the method improve convolutional or recurrent neural networks classifiers?\n   - \"obvious improvement\""}
