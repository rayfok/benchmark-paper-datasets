{"id": "1909.00694", "questions": [["What are the natural language processing applications of recognizing affective events?", "natural language processing applications"], ["Why is the polarity of an event not necessarily predictable from constituent words?", "not necessarily predictable"], ["How does the method propagate affective polarity using discourse relations?", "propagate affective polarity"], ["What is the composition of the small seed lexicon used in the method?", "small seed lexicon"], ["What are the characteristics of the large raw corpus used in the study?", "large raw corpus"], ["How does the method perform on Japanese data when compared to other approaches?", "Japanese data"], ["In what ways does the method improve supervised learning results with small labeled data?", "improves supervised learning"]]}
{"id": "2003.07723", "questions": [["What are the basic emotion categories defined by Ekman or Plutchik?", "basic emotion categories"], ["How are aesthetic emotions conceptualized in this study?", "conceptualize a set"], ["What methodology was used for annotation of multiple labels per line?", "allow the annotation"], ["How were the trained experts and crowdsourcing participants selected for the annotation experiment?", "trained experts and via crowdsourcing"], ["What was the specific annotation process followed in the experiment?", "annotation experiment"], ["Can you provide details on the BERT-based emotion classification experiments?", "emotion classification experiments"], ["What were the main challenges in identifying aesthetic emotions in the data?", "identifying aesthetic emotions"], ["How was the performance of the classification model measured?", ".52 F1-micro"], ["Are there any specific findings or insights about the German and English poetry in relation to aesthetic emotions?", "German and English poetry"], ["Does the paper discuss potential applications or future work related to the analysis of aesthetic emotions in literature?", "future large scale analysis"]]}
{"id": "1705.09665", "questions": [["What is the quantitative, language-based typology introduced in this study?", "language-based typology"], ["How were the 300 Reddit communities chosen for this study?", "300 Reddit communities"], ["What are the specific characteristics of a community's identity that were analyzed?", "distinctive, and temporally dynamic"], ["How do user engagement patterns differ in communities with varying characteristics?", "patterns of user engagement"], ["How is the collective identity of a community related to user engagement and retention?", "collective identity"], ["What challenges do niche communities face when integrating newcomers?", "acculturation gaps"], ["What are the implications of the study's findings for community maintainers?", "community maintainers"], ["How can the methodology used in this study help to understand social phenomena across communities?", "our methodology reveals"], ["Are there any specific examples or case studies of communities in this research?", "individual communities"]]}
{"id": "1908.06606", "questions": [["What are the limitations of traditional methods in clinical text structuring?", "lack of dataset and error propagation"], ["How does the question answering based clinical text structuring (QA-CTS) task unify different specific tasks?", "unify different specific tasks"], ["What is the novel model proposed for the QA-CTS task?", "novel model"], ["How are domain-specific features integrated into the pre-trained language model?", "domain-specific features"], ["What are the specific tasks the proposed model competes favorably with strong baseline models in?", "strong baseline models"], ["How does the QA-CTS task improve performance on specific tasks?", "improve the performance"], ["What type of dataset was used in the experiments of this paper?", "Chinese pathology reports"], ["What is the source of the pathological reports used in the experiments?", "Ruijing Hospital"]]}
{"id": "1811.00942", "questions": [["What are the specific techniques driven by neural networks for NLP tasks mentioned in the paper?", "neural networks"], ["How much has perplexity been reduced by neural language models?", "reduced perplexity"], ["What are the performance tradeoffs when using neural language models on mobile devices?", "performance tradeoff"], ["What are the differences in energy usage, latency, perplexity, and prediction accuracy between NLMs and Kneser-Ney LMs?", "energy usage, latency"], ["How do the benchmarks compare NLMs and KN LMs on Raspberry Pi and desktop environments?", "Raspberry Pi"], ["What are the orders of increase in latency and energy usage for NLMs on Raspberry Pi and desktops?", "orders of increase"], ["How significant is the difference in perplexity between NLMs and classic Kneser-Ney LMs?", "less change in perplexity"], ["What are the two standard benchmarks used to compare NLMs and KN LMs?", "two standard benchmarks"]]}
{"id": "1805.02400", "questions": [["What is char-LSTM?", "char-LSTM"], ["How does the neural machine translation technique improve context-specificity?", "neural machine translation"], ["What are the different variants of the NMT technique tested in this study?", "multiple variants"], ["How was the undetectability of generated reviews assessed?", "class-averaged F-score"], ["What was the user study conducted with skeptical users?", "user study"], ["How does the state-of-the-art method compare to the proposed technique in terms of evasion?", "average evasion"], ["What are the developed detection tools that achieved an average F-score of 97%?", "effective detection tools"], ["What are the implications of these findings for the online review systems?", "fake restaurant reviews"]]}
{"id": "1907.05664", "questions": [["What are the applications of saliency map generation techniques in explainable AI?", "explainable AI literature"], ["How does Layer-Wise Relevance Propagation (LRP) work?", "Layer-Wise Relevance Propagation"], ["What is the sequence-to-sequence attention model used for the text summarization dataset?", "sequence-to-sequence attention model"], ["What are some examples of unexpected saliency maps generated?", "unexpected saliency maps"], ["How can the counterfactual case be tested quantitatively?", "quantitative way"], ["What is the proposed protocol for checking the validity of importance attributed to input?", "suggest a protocol"], ["In what cases do the saliency maps capture the real use of input features by the network?", "capture the real use"], ["How can one be more cautious when accepting saliency maps as explanations?", "how careful we need"]]}
{"id": "1910.14497", "questions": [["What are the biases present in word embeddings derived from large corpora?", "incorporate biases"], ["What are the existing methods for mitigating biases in word embeddings?", "Various methods"], ["How do recent works show that existing methods fail to truly remove biases?", "recent work"], ["What is the proposed probabilistic view of word embedding bias?", "probabilistic view"], ["How does the novel method for mitigating bias work?", "novel method"], ["What are the three separate measures of bias used for evaluating the effectiveness of the proposed method?", "three separate measures"], ["How does the proposed method maintain embedding quality across popular benchmark semantic tasks?", "maintaining embedding quality"]]}
{"id": "1912.02481", "questions": [["What are the several architectures used for learning semantic representations from unannotated text?", "several architectures"], ["How is the evaluation done for high-resourced languages compared to low-resourced languages?", "evaluation of such resources"], ["How do deep learning architectures perform in low-resourced settings?", "deep learning architectures"], ["What are the specific differences in word embeddings obtained from curated corpora and language-dependent processing?", "word embeddings obtained"], ["How was the noise in publicly available corpora analyzed?", "analyse the noise"], ["What kind of high quality and noisy data were collected for Yorùbá and Twi?", "collect high quality and noisy data"], ["What improvements were seen based on the amount and quality of the data?", "quantify the improvements"], ["What were the different architectures that learned word representations from surface forms and characters?", "different architectures"], ["How was the wordsim-353 dataset translated into Yorùbá and Twi for evaluation?", "manually translate"], ["What are the details of the provided corpora, embeddings, and test suits for Yorùbá and Twi?", "provide corpora, embeddings and the test suits"]]}
{"id": "1810.04528", "questions": [["What are word embeddings?", "Portuguese word embeddings"], ["How is gender bias analyzed?", "analysis of...gender bias"], ["What professions are considered stereotyped?", "stereotyped professions"], ["What are the specific gender implications examined?", "gender implications"], ["What methodology is used for studying gender bias in word embeddings?", "study gender implications"], ["Are there any previous studies or works related to this topic?", "propose an analysis"], ["How does the Portuguese language context differ from other languages in terms of gender bias?", "context of the Portuguese language"], ["What are the main findings of the research?", "presence of gender bias"], ["Are there any recommendations to mitigate the identified biases?", "objective of this work"]]}
{"id": "2002.02224", "questions": [["What is the purpose of extracting citation data from Czech apex courts? \n2. How were the court decisions organized in the CzCDC 1.0 corpus? \n3. What are the specific components of the natural language processing pipeline? \n4. How did the document segmentation model work in the extraction process? \n5. What was the role of the reference recognition model in the pipeline? \n6. What manual processing steps were taken to ensure high-quality citation data? \n7. What are some examples of qualitative and quantitative analyses that can be performed using this dataset? \n8. How can the general public access the dataset?", "citation data"]]}
{"id": "2003.07433", "questions": [["What is the main issue with existing PTSD assessment frameworks for veterans?", "lack of clinical explainability"], ["How does LAXARY aim to address the trust issue with clinicians?", "Explainable Artificial Intelligent"], ["What is the modified Linguistic Inquiry and Word Count (LIWC) analysis used for in the LAXARY model?", "LIWC analysis"], ["How is the PTSD Linguistic Dictionary developed?", "PTSD assessment survey results"], ["How is the PTSD Linguistic Dictionary used in the machine learning model?", "fill up the survey tools"], ["What is the sample size and source for the experimental evaluation?", "210 clinically validated veteran twitter users"], ["What were the results of the experimental evaluation on PTSD classification and intensity estimation?", "promising accuracies"], ["How was the reliability and validity of the PTSD Linguistic Dictionary evaluated?", "reliability and validity"]]}
{"id": "2003.12218", "questions": [["What is the CORD-19-NER dataset?", "CORD-19-NER dataset"], ["How many fine-grained named entity types does the dataset cover?", "74 fine-grained"], ["What are the four sources used to generate the dataset?", "four sources"], ["What are the 18 general entity types from Spacy?", "18 general entity"], ["What are the 18 biomedical entity types from SciSpacy?", "18 biomedical entity"], ["What is the distantly-supervised NER method?", "distantly-supervised NER"], ["What are the 127 biomedical entity types from the knowledge base-guided NER model?", "127 biomedical entity"], ["What is the weakly-supervised NER method?", "weakly-supervised NER"], ["What are the 8 new entity types related to COVID-19 studies?", "8 new entity types"], ["What are the potential downstream applications of this dataset?", "downstream applications"], ["How can this dataset contribute to the understanding of COVID-19 studies?", "COVID-19 studies"]]}
{"id": "1904.09678", "questions": [["What is UniSent?", "universal sentiment lexica"], ["How many languages does UniSent cover?", "1000 languages"], ["What domain is the parallel corpus from?", "Bible domain"], ["What is the Adapted Sentiment Pivot method?", "Adapted Sentiment Pivot"], ["How does Adapted Sentiment Pivot combine annotation projection, vocabulary expansion, and unsupervised domain adaptation?", "combines annotation projection"], ["Which languages were used for evaluation?", "Macedonian, Czech, German, Spanish, and French"], ["How does UniSent's quality compare to other sentiment resources?", "comparable to manually"], ["What resources are being released along with this paper?", "release UniSent lexica"]]}
{"id": "2003.06651", "questions": [["What are the limitations of supervised and knowledge-based models for word sense disambiguation?", "supervised and knowledge-based models"], ["How does the Zipfian distribution affect supervised training instances for a given word?", "inherent Zipfian distribution"], ["How does the quality of linguistic knowledge representations impact word sense disambiguation?", "quality of linguistic knowledge representations"], ["What are the benefits of unsupervised and knowledge-free approaches to WSD, especially for under-resourced languages?", "under-resourced languages"], ["How does the presented method induce a fully-fledged word sense inventory from a pre-trained word embedding model?", "induces a fully-fledged word sense inventory"], ["What are the main features of the fastText word embeddings by Grave et al. (2018)?", "fastText word embeddings"], ["How is the induced collection of sense inventories used for disambiguation in context across 158 languages?", "disambiguation in context"], ["Are the induced sense inventories and models available online, and if so, where can they be accessed?", "available online"]]}
{"id": "1910.04269", "questions": [["What are the existing models in speech LI research?", "benchmarks existing models"], ["How does the new attention-based model work?", "attention based model"], ["Why are log-Mel spectrogram images used as input?", "log-Mel spectrogram"], ["What is the effectiveness of raw waveforms as features for LI tasks?", "raw waveforms"], ["What are the details of the VoxForge dataset used in this study?", "VoxForge dataset"], ["How can this approach be scaled to include more languages?", "scaled to incorporate"], ["What are the specific results and comparisons between the proposed model and the benchmarked models?", "benchmarks existing models"], ["Were any challenges or limitations encountered during the experiment?", "accuracy of 95.4% (as an indicator of potential challenges)"], ["What are the potential applications or improvements for voice-based assistants resulting from this research?", "voice-based assistants"]]}
{"id": "1906.00378", "questions": [["What are the limitations of previous vision-based approaches for bilingual lexicon induction?", "vision-based approaches"], ["How does the proposed multi-lingual caption model work?", "multi-lingual caption model"], ["What are the two types of word representation induced from the multi-lingual caption model?", "two types"], ["How do linguistic features help in learning translation for less visual-relevant words?", "linguistic features"], ["How do localized visual features alleviate image restrictions for salient visual representation?", "localized visual features"], ["What were the experimental results for the proposed method on multiple language pairs?", "experimental results"], ["How does the proposed method compare to previous vision-based approaches in terms of performance?", "substantially outperforms"]]}
{"id": "1912.13072", "questions": [["What are the specific social media datasets used for training the bidirectional encoders?", "social media datasets"], ["How does AraNet's performance compare to other existing methods on the mentioned tasks?", "state-of-the-art performance"], ["What deep learning framework is AraNet based on?", "deep learning framework"], ["How does the absence of feature engineering benefit AraNet?", "feature engineering free"], ["What are the detailed results of AraNet on each of the tasks (age, dialect, gender, emotion, irony, and sentiment)?", "predict age, dialect, gender, emotion, irony, and sentiment"], ["How can researchers access and use AraNet for their own work?", "publicly release AraNet"], ["Are there any limitations or challenges faced while developing AraNet?", "AraNet delivers"], ["What are some potential applications of AraNet in Arabic NLP research?", "Arabic NLP"], ["Are there any plans for future improvements or additions to AraNet?", "AraNet is the first"], ["How does AraNet handle variations in Arabic dialects and regional differences?", "predict dialect"]]}
{"id": "1712.09127", "questions": [["What are generative adversarial nets (GANs)?", "Generative adversarial nets"], ["How have GANs been applied to image data?", "artificial generation of image data"], ["What progress has been made in generating natural language from a single corpus?", "artificial generation of natural language"], ["What are the two applications of GANs for multiple text corpora?", "two applications of GANs"], ["How do the GAN models create consistent cross-corpus word embeddings?", "consistent cross-corpus word embeddings"], ["How are robust bag-of-words document embeddings generated for each corpus?", "robust bag-of-words document embeddings"], ["What real-world text data sets were used for the demonstration of GAN models?", "real-world text data sets"], ["How do the embeddings from both models improve supervised learning problems?", "improvements in supervised learning problems"]]}
{"id": "2001.00137", "questions": [["What is Stacked DeBERT?", "Stacked DeBERT"], ["How does Stacked DeBERT improve robustness in incomplete data?", "novel encoding scheme"], ["How is incomplete data defined in this context?", "missing or incorrect words"], ["Why do current models struggle with incomplete data?", "clean and complete data"], ["What are the main components of the proposed approach?", "denoising transformers"], ["How do multilayer perceptrons contribute to the reconstruction of missing words' embeddings?", "multilayer perceptrons"], ["What is the role of bidirectional transformers in the proposed model?", "bidirectional transformers"], ["Which datasets were used for training and evaluation?", "Chatbot Natural Language Understanding Evaluation Corpus and Kaggle's Twitter Sentiment Corpus"], ["What performance metrics were used to evaluate the model?", "improved F1-scores"], ["How does the model perform on informal/incorrect texts and texts with Speech-to-Text errors?", "better robustness"]]}
{"id": "1910.03042", "questions": []}
{"id": "2002.06644", "questions": [["What are the applications of subjective bias detection?", "propaganda detection, content recommendation, sentiment analysis, and bias neutralization"], ["How is subjective bias introduced in natural language?", "inflammatory words and phrases"], ["What is the Wiki Neutrality Corpus (WNC)?", "Wiki Neutrality Corpus"], ["How many labeled instances are in the WNC dataset?", "360k labeled instances"], ["What kind of Wikipedia edits does the WNC dataset consist of?", "Wikipedia edits"], ["What BERT-based models were used for subjective bias detection?", "BERT-based models"], ["How do the proposed BERT-based ensembles perform compared to state-of-the-art methods?", "outperform state-of-the-art methods"], ["What is the improvement in F1 score achieved by the BERT-based ensembles?", "5.6 F1 score"]]}
{"id": "1809.08731", "questions": [["What are the recent findings on probabilistic modeling of acceptability judgments?", "recent findings"], ["How does syntactic log-odds ratio", "SLOR"], ["What are the differences between SLOR and WPSLOR?", "WordPiece-based version"], ["How do referenceless methods perform compared to word-overlap metrics like ROUGE?", "significantly higher correlation"], ["What is the benchmark dataset used for evaluating compressed sentences?", "benchmark dataset"], ["How is ROUGE-LM a natural extension of WPSLOR?", "ROUGE-LM"], ["How does ROUGE-LM compare to other baseline metrics in terms of correlation with human judgments?", "significantly higher correlation"], ["What are the implications of these findings for natural language generation evaluation?", "fluency evaluation"]]}
{"id": "1707.00995", "questions": [["What is the attention mechanism in Neural Machine Translation?", "attention mechanism"], ["How does the decoder utilize the attention mechanism during translation?", "during decoding"], ["What are the different attention mechanisms compared in the study?", "several attention mechanism"], ["How does the model make use of images to improve translation?", "use of images"], ["What are the results of the comparison on the multimodal translation tasks?", "compare several attention"], ["What is the Multi30k dataset and how is it used in the study?", "Multi30k data set"], ["What are the specific misbehaviors observed in the machine during translation?", "different misbehavior"]]}
{"id": "1809.04960", "questions": [["What is the main goal of the unsupervised automatic article commenting model?", "automatically commenting"], ["How does the proposed model differ from previous models?", "completely remove"], ["What is the basis of the retrieval-based commenting framework?", "retrieval-based commenting"], ["How is the topic representation obtained in the neural variational topic model?", "topic representation"], ["How was the model evaluated?", "news comment dataset"], ["How does the proposed model compare to lexicon-based models?", "significantly outperforms"], ["How does the model perform under semi-supervised scenarios?", "state-of-the-art performance"]]}
{"id": "1909.08402", "questions": [["What is the main goal of the research?", "classification of books"], ["How are cover blurbs and metadata utilized?", "combine text representations"], ["What is the role of knowledge graph embeddings in the study?", "encode author information"], ["How does the enriched BERT model compare to the standard BERT approach?", "achieve considerably better results"], ["What are the F1-scores for the coarse-grained and detailed classifications?", "F1-score of 87.20 and F1-score of 64.70"], ["Are the source code and trained models accessible?", "publicly available"], ["What is the basis of the deep neural language model used in the research?", "Building upon BERT"], ["How many labels are used for the more coarse-grained classification?", "eight labels"], ["How many labels are used for the detailed classification?", "343 labels"]]}
{"id": "1909.11189", "questions": [["What are the benefits of using statistical topic models in Digital Humanities research?", "Statistical topic models"], ["How does Latent Dirichlet Allocation (LDA) work, and what makes it a useful method for analyzing literary data?", "Latent Dirichlet Allocation"], ["What is the size and scope of the New High German poetry corpus used in the study?", "51k poems, 8m token"], ["How is the distribution of topics over documents used for classification purposes in this study?", "distribution of topics"], ["What are the specific methods used for classifying poems into time periods and for authorship attribution?", "classification of poems"], ["What were the main findings of the preliminary study, and how might they inform further research in the field?", "preliminary study"], ["Are there any limitations or challenges faced in applying LDA to a corpus of New High German poetry?", "apply LDA"], ["How does this research contribute to the understanding of diachronic topics in New High German poetry?", "Diachronic Topics"]]}
{"id": "1810.05320", "questions": [["What are the various natural language processing tasks that KGs are applied to?", "application scenarios"], ["How does the proposed method evaluate the relative importance of an entity's attributes?", "external user generated"], ["What are the word/sub-word embedding techniques used for matching?", "embedding techniques"], ["How does the vector-based semantic matching approach compare to traditional methods?", "outperforms the previous"], ["What specific language generation task was used to test the outcome of detected important attributes?", "language generation task"], ["How do the generated messages differ from those produced by previous methods?", "more customized and informative"]]}
{"id": "2003.08529", "questions": [["What are the limitations of current characteristic metrics for text collections?", "insufficient characteristic metrics"], ["How are the proposed metrics of diversity, density, and homogeneity defined?", "metrics of diversity, density, and homogeneity"], ["What are the desired properties of these metrics?", "desired properties"], ["How do these metrics resonate with human intuitions?", "human intuitions"], ["What kind of simulations were conducted to verify the metrics?", "series of simulations"], ["Which real-world datasets were used for the experiments?", "real-world datasets"], ["How were the correlations between the proposed metrics and BERT's performance measured?", "highly correlated"], ["What are the potential future applications of these characteristic metrics?", "future applications"]]}
{"id": "1708.05873", "questions": [["What is the significance of agenda setting in the United Nations for international development?", "significant influence"], ["What are the natural language processing techniques used in this study?", "natural language processing"], ["How have the UN General Debate statements been overlooked in the study of global politics?", "largely been overlooked"], ["What are the main international development topics identified in the UN General Debate speeches between 1970 and 2016?", "main international development"], ["What country-specific drivers influence international development rhetoric?", "country-specific drivers"], ["How do the annual statements from UN member states provide insight into state preferences on international development?", "state preferences"], ["What is the overall aim of analyzing the UN General Debate speeches from 1970 to 2016?", "addresses this shortcoming"]]}
{"id": "2003.08553", "questions": [["What is the main purpose of QnAMaker?", "creates a conversational layer"], ["How does QnAMaker help reduce traffic to human support?", "lower traffic to human support"], ["What types of data does QnAMaker work with?", "semi-structured data"], ["What are some examples of data sources for QnAMaker?", "FAQ pages, product manuals, and support documents"], ["How popular is QnAMaker among bots in production?", "used by over 15,000 bots"], ["What are other applications of QnAMaker besides bots?", "used by search interfaces"]]}
{"id": "1909.09491", "questions": [["What are margin infused relaxed algorithms (MIRAs) used for in statistical machine translation?", "MIRAs dominate"], ["What is the complexity associated with MIRAs in implementation?", "complexity in implementation"], ["How does the new method treat an N-best list in the context of machine translation?", "N-best list"], ["What is the Plackett-Luce loss and how is it used in the new method?", "Plackett-Luce loss"], ["How does the new method compare to MERT in terms of robustness?", "more robust than MERT"], ["In what way is the new method only matchable with MIRAs?", "matchable with MIRAs"], ["What is the main advantage of the new method compared to MIRAs?", "easier to implement"], ["What kind of experiments were conducted to test the new method with large-scale features?", "Experiments with large-scale features"]]}
{"id": "2001.05284", "questions": [["What is the main goal of the presented models?", "improving the understanding"], ["How does an SLU system typically process speech input?", "first best interpretation"], ["What is the main issue with relying on the first best interpretation?", "erroneous and noisy"], ["How do the proposed models address the issue of misrecognized speeches?", "collectively exploiting"], ["What specific downstream tasks are mentioned in the abstract?", "domain and intent classification"], ["What is the role of the ASR module in the SLU system?", "takes interpretations"], ["Are the introduced models complex or simple?", "simple yet efficient"]]}
{"id": "1909.12140", "questions": [["What is the purpose of DisSim?", "transform syntactically complex sentences"], ["How does DisSim create an intermediate representation?", "two-layered semantic hierarchy"], ["What are the two layers in the semantic hierarchy?", "core facts and accompanying contexts"], ["How does DisSim identify rhetorical relations?", "identifying the rhetorical relations"], ["How does DisSim preserve the input's coherence structure?", "preserve the coherence structure"], ["What are the downstream semantic applications for DisSim?", "downstream semantic applications"], ["What challenges does DisSim address in English and German languages?", "English and German"], ["How does DisSim contribute to the interpretability of input for downstream tasks?", "interpretability for downstream tasks"]]}
{"id": "1709.00947", "questions": [["What are the main challenges in producing and distributing a large-scale database of embeddings from the Portuguese Twitter stream?", "challenges: volume, vocabulary"], ["How was the experiment conducted with a relatively small sample?", "small sample"], ["What were the results of scaling up vocabulary size and training examples?", "scale up vocabulary"], ["How did the training time per epoch change when scaling up vocabulary size?", "approximately linear trend"], ["What is the impact of using less than 50% of the available training examples for each vocabulary size?", "result in overfitting"], ["What were the results of the intrinsic evaluation for a vocabulary size of 32768 words?", "promising performance"], ["What issues were observed with intrinsic evaluation metrics?", "over-sensitivity"], ["What suggestions might the authors have for developing a wider range of metrics for evaluation?", "wider range of metrics"]]}
{"id": "1909.08859", "questions": [["What is the main problem addressed in this research paper?", "comprehending procedural commonsense knowledge"], ["What are the challenges in comprehending procedural commonsense knowledge?", "identifying key entities"], ["How does this study differ from previous work?", "do not rely on strong inductive bias"], ["How does the proposed model exploit multimodality?", "multimodality can be exploited"], ["What is the structure of the introduced entity-aware neural comprehension model?", "external relational memory units"], ["How does the model dynamically update entity states?", "dynamically update entity states"], ["What is the RecipeQA dataset, and how is it used in this study?", "RecipeQA dataset"], ["How does the proposed approach improve the accuracy of previously reported models?", "improves the accuracy"], ["What kind of supervision is not used in the model?", "supervision at the level of entity states"], ["What are the implications of the model's effective dynamic representations of entities?", "effective dynamic representations"]]}
{"id": "1908.08419", "questions": [["What is the importance of Chinese word segmentation in medical texts?", "fundamental and important"], ["Why is manual annotation time-consuming and expensive in the medical field?", "time-consuming and expensive"], ["How does the active learning method for CWS work?", "active learning method"], ["What is the new scoring model in the sampling strategy?", "new scoring model"], ["How does the combination of information entropy and neural network contribute to the method's effectiveness?", "information entropy"], ["What role do K-means clustering features play in the word segmenter?", "K-means clustering"], ["How was the proposed method experimentally evaluated?", "experimentally evaluate"], ["What were the results of the experiments based on EHRs from Shuguang Hospital?", "experimental results"], ["How does the proposed method compare with other reference methods?", "outperforms other reference"], ["How much cost savings can the proposed method offer in terms of manual annotation?", "save the cost"]]}
{"id": "1703.05260", "questions": [["What is the main purpose of the InScript corpus?", "unique resource"], ["How many stories are included in the InScript corpus?", "1,000 stories"], ["What are the different scenarios covered by the InScript corpus?", "10 different scenarios"], ["How are verbs and noun phrases annotated in the corpus?", "event and participant types"], ["What additional information is annotated in the text?", "coreference information"], ["What is the significance of the rich lexical variation in the InScript corpus?", "rich lexical variation"], ["How can the InScript corpus contribute to the study of script knowledge in natural language processing?", "role of script knowledge"]]}
{"id": "1905.00563", "questions": [["What is the main goal of the proposed adversarial modifications for link prediction models?", "adversarial modifications"], ["How does the paper estimate the effect of modifications on the knowledge graph?", "efficient approach"], ["How does the proposed approach avoid combinatorial search over all possible facts?", "gradient-based optimization"], ["What are the main applications of the techniques presented in the paper?", "evaluate robustness, study interpretability, detect incorrect facts"], ["How do the authors measure the sensitivity of the models to additional fake facts?", "sensitivity to additional facts"], ["What is the method used to identify the most influential neighbors?", "most influential neighbors"], ["How do the authors decode embeddings to their corresponding graph components?", "train a network"]]}
{"id": "1808.05902", "questions": [["What are the proposed supervised topic models for classification and regression problems?", "supervised topic models"], ["How do these models account for heterogeneity and biases among annotators?", "heterogeneity and biases"], ["What is the stochastic variational inference algorithm developed in the paper?", "stochastic variational inference"], ["How does the proposed model scale to very large datasets?", "scale to very large datasets"], ["In what ways do the proposed models outperform the state-of-the-art approaches?", "advantages of the proposed model"], ["What are some real-world applications of these supervised topic models?", "real-world applications"], ["How does the single-annotator assumption limit the effectiveness of other supervised topic models?", "single-annotator assumption"], ["What are the specific challenges posed by annotation tasks with respect to ambiguity and noise?", "ambiguity and noise"]]}
{"id": "2002.11893", "questions": [["What is the purpose of creating CrossWOZ?", "advance multi-domain"], ["How many dialogue sessions and utterances are included in CrossWOZ?", "6K dialogue sessions"], ["Which domains are covered in CrossWOZ?", "5 domains"], ["What type of annotations does CrossWOZ contain?", "rich annotation"], ["What is the percentage of dialogues with cross-domain user goals?", "60% of the dialogues"], ["What tasks can be investigated using CrossWOZ?", "cross-domain dialogue modeling"], ["What is the purpose of the provided user simulator and benchmark models?", "compare and evaluate"], ["What specific dialogue modeling tasks are suitable for CrossWOZ?", "dialogue state tracking, policy learning, user simulation"]]}
{"id": "1910.07181", "questions": [["What are the performance gains observed in NLP tasks due to pretraining deep contextualized representations?", "large performance gains"], ["How do contextualized models struggle with rare words, as suggested by Schick and Schutze?", "struggle to understand"], ["How does separately learning representations for infrequent words help context-independent word embeddings?", "infrequent words"], ["How does applying the idea of learning representations for infrequent words improve the downstream task performance of contextualized models?", "clearly improves"], ["Why are simple bag-of-words models not suitable for inducing word embeddings into existing embedding spaces?", "not a suitable"], ["What are the components and structure of the BERTRAM architecture?", "introduce BERTRAM"], ["How does BERTRAM enable high-quality representations for rare words?", "inferring high-quality"], ["How do surface form and contexts of a word interact in the BERTRAM architecture?", "directly interact"], ["What are the results of the rare word probing task and the three downstream task datasets when using BERTRAM?", "considerably improves"], ["How does BERTRAM compare to both a standalone BERT model and previous work in terms of rare and medium frequency word representations?", "compared to both"]]}
{"id": "1902.00330", "questions": [["What is entity linking?", "Entity linking"], ["What are the common weaknesses in previous global models?", "common weaknesses"], ["How do these weaknesses introduce noise data and increase model complexity?", "introduce noise data"], ["How does the proposed reinforcement learning model address the problems?", "reinforcement learning model"], ["In what way does the model make use of previous referred entities?", "previous referred entities"], ["How does the model explore the long-term influence of current selection on subsequent decisions?", "long-term influence"], ["What types of datasets were used in the experiments?", "different types of datasets"], ["How does the model's performance compare to state-of-the-art systems?", "outperforms state-of-the-art"], ["What is the model's generalization performance like?", "better generalization performance"]]}
{"id": "1909.00542", "questions": [["What is the focus of Task B Phase B in the 2019 BioASQ challenge?", "biomedical question answering"], ["How does Macquarie University's participation differ from previous approaches in this challenge?", "classification approaches"], ["What are the deep learning architectures used in regression approaches for this task?", "deep learning architectures"], ["What are the advantages of using classification approaches over regression approaches in this context?", "betters regression"], ["How does reinforcement learning impact the performance of classification approaches?", "reinforcement learning"], ["What are the various ROUGE metrics used in the correlation analysis?", "ROUGE metrics"], ["How do the BioASQ human evaluation scores compare to the performance of the proposed methods?", "BioASQ human evaluation scores"]]}
{"id": "1810.06743", "questions": [["What are the main goals of Universal Dependencies and Universal Morphology projects?", "morphosyntactic details"], ["How do the projects differ in terms of annotation levels?", "token level, type level"], ["What issues do language-specific decisions cause in achieving universal schemata?", "language-specific decisions"], ["What are the potential benefits of compatibility between UD and UniMorph tags?", "validate the other's"], ["What is the deterministic mapping's purpose in this study?", "deterministic mapping"], ["How effective is the proposed mapping between UD v2 features and UniMorph schema?", "64.13% recall"], ["What are some incompatibilities observed due to data limitations?", "paucity of data"], ["What insights are provided by the critical evaluation of the two annotation projects?", "critical evaluation"]]}
{"id": "1909.02764", "questions": [["What is the purpose of the in-car experiment for emotion recognition?", "in-car experiment"], ["How many modalities were studied for emotion recognition from speech interactions?", "three modalities"], ["What are the specific emotions analyzed in this study?", "emotions joy, annoyance and insecurity"], ["Which off-the-shelf tools were used for emotion detection in audio and face?", "off-the-shelf tools"], ["How does the neural transfer learning approach work for emotion recognition from text?", "neural transfer learning approach"], ["What are the performance improvements achieved by using transfer learning?", "10 percentage points"], ["What are the limitations of the off-the-shelf-tools for emotion detection in in-car speech interactions?", "off-the-shelf-tools analyzing face and audio are not ready yet"], ["What were the adjustments needed for off-the-shelf-tools to improve emotion detection in in-car speech interactions?", "without further adjustments"], ["Which existing resources from other domains were utilized in the transfer learning approach?", "existing resources from other domains"]]}
{"id": "1905.11901", "questions": [["What causes the performance drop in low-resource NMT systems?", "low-resource conditions"], ["How does PBSMT perform in comparison to low-resource NMT systems?", "underperforming PBSMT"], ["What are some pitfalls when training low-resource NMT systems?", "lack of system adaptation"], ["What recent techniques have been helpful in low-resource settings?", "recent techniques"], ["What best practices are recommended for low-resource NMT?", "best practices"], ["How much IWSLT14 training data was used in the experiments?", "different amounts"], ["How does the optimized NMT system compare to PBSMT in terms of data requirements?", "far less data"], ["What were the results of applying these techniques to the Korean-English dataset?", "4 BLEU"]]}
{"id": "1912.01252", "questions": [["What are the challenges in studying cultural and societal conflicts through technological means?", "study such cultural"], ["How does the experimental observatory work for mining and analyzing opinions?", "experimental observatory"], ["What are the specific methods used in computational text analysis for opinion mining?", "computational text analysis"], ["How are causal mapping methods combined with computational text analysis?", "causal mapping methods"], ["What news website is the source of climate change-related user comments?", "this http URL"], ["How are beliefs and opinions visualized in the opinion landscape?", "visualize opinion landscapes"], ["What digital methods and open infrastructures are introduced for data exploration and analysis?", "introducing digital methods"], ["What are the implications of using these methods for observing and facilitating debates?", "debate facilitation"], ["How does the article contribute to the study of opinion dynamics and conflict in new media environments?", "opinion dynamics and conflict"]]}
{"id": "1912.13109", "questions": [["What is the significance of the rise in Hinglish fluency and users in India?", "Hinglish in linguistically diverse country"], ["Why is it important to analyze social content written in Hinglish?", "analyze social content"], ["Which platforms are being studied for Hinglish content?", "Twitter, Reddit, Facebook"], ["What are the three categories for classifying Hinglish social content?", "Abusive, Hate-Inducing, Not offensive"], ["What deep learning techniques are used in this project?", "bi-directional sequence models"], ["What text augmentation techniques are employed in the study?", "synonym replacement, random insertion, random swap, random deletion"], ["How does the state of the art classifier compare to previous work?", "outperforms the previous work"], ["What dataset is used for the analysis?", "analyzing this dataset"]]}
{"id": "1911.03310", "questions": [["What is mBERT's purpose?", "sentence representations"], ["How did previous work test mBERT's cross-linguality?", "zero-shot transfer"], ["What is the focus of this research paper?", "semantic properties"], ["How can mBERT representations be divided?", "language-specific component"], ["What tasks can the language-neutral component handle with high accuracy?", "word-alignment and sentence retrieval"], ["Which task is the language-neutral component not yet good enough for?", "MT quality estimation"], ["What challenges do better language-neutral representations face?", "linguistic transfer of semantics"]]}
{"id": "1907.12108", "questions": [["What is TransferTransfo learning approach? \n2. How does CAiRE fine-tune the large-scale pre-trained language model? \n3. What are the multi-task objectives in CAiRE's training? \n4. What is the structure of the empathetic-dialogues dataset? \n5. How does CAiRE perform on response language modeling? \n6. What is the methodology behind response prediction? \n7. How is dialogue emotion detection achieved in CAiRE? \n8. In what ways does CAiRE outperform previous state-of-the-art models? \n9. Are there any limitations or challenges in CAiRE's empathetic response generation?", "TransferTransfo"]]}
{"id": "2004.03685", "questions": [["What is interpretability in NLP systems?", "what is interpretability"], ["How is faithfulness criteria defined?", "faithfulness criteria"], ["What are the different desired criteria for interpretation?", "different desired criteria"], ["What are the current approaches to faithfulness evaluation?", "faithfulness evaluation"], ["What are the three assumptions mentioned in the literature survey?", "three assumptions"], ["What are the concrete guidelines for evaluating interpretation methods?", "concrete guidelines"], ["Why is the current binary definition of faithfulness potentially unrealistic?", "binary definition"], ["How can a more graded notion of faithfulness be beneficial?", "graded one"]]}
{"id": "1808.03894", "questions": [["What is the main goal of this study?", "explaining such deep learning based models"], ["Which specific neural model for NLI is being studied?", "popular neural model"], ["How are the intermediate layers of NLI models interpreted?", "visualizing the saliency"], ["What are the key components being visualized in the model?", "attention and LSTM gating signals"], ["Are there any notable examples presented in the paper?", "several examples"], ["What insights can be gained from the visualization methods?", "interesting insights"], ["How does the proposed interpretation method help in understanding model decisions?", "identify the critical information"]]}
{"id": "1703.04617", "questions": [["What are the neural-network-based models used in machine comprehension and question answering?", "neural-network-based models"], ["How is syntactic information introduced to help encode questions?", "introduce syntactic information"], ["What are the different types of questions modeled in this research?", "different types of questions"], ["How do the proposed adaptation models work?", "proposed adaptation models"], ["What improvements do these approaches show on the Stanford Question Answering Dataset?", "better results"], ["How does the adaptation task help in understanding and modeling questions?", "an adaptation task"], ["What was the competitive baseline used for comparison?", "competitive baseline"]]}
{"id": "1909.00578", "questions": [["What is SumQE?", "a novel Quality Estimation model"], ["How does SumQE differ from content-based approaches?", "addresses linguistic quality aspects"], ["How does BERT contribute to the SumQE model?", "based on BERT"], ["How well does SumQE perform compared to simpler models?", "outperforming simpler models"], ["How are human ratings used in evaluating SumQE's performance?", "correlations with human ratings"], ["In what applications can SumQE's predictions be used?", "system development"], ["How can SumQE inform users about the quality of generated text?", "inform users of the quality"], ["Can SumQE be applied to other types of generated text besides summaries?", "other types of generated text"]]}
{"id": "1911.09419", "questions": [["What are the existing knowledge graph embedding models?", "Relation patterns"], ["How do current approaches fail to model semantic hierarchies?", "Fail to model"], ["What is the main goal of the Hierarchy-Aware Knowledge Graph Embedding (HAKE) model?", "Model semantic hierarchies"], ["How does the polar coordinate system help in modeling hierarchies?", "Concentric circles"], ["What is the role of the radial coordinate in the HAKE model?", "Model entities"], ["How does the angular coordinate distinguish entities at the same level?", "Different angles"], ["What are the benchmark datasets used for evaluating the HAKE model?", "Benchmark datasets"], ["How does the performance of HAKE compare to existing state-of-the-art methods in link prediction?", "Significantly outperforms"]]}
{"id": "1910.11471", "questions": [["What is the motivation behind making computer programming language more understandable?", "Abstract longstanding problem"], ["How do Recurrent Neural Networks and Long-Short Term Memory contribute to the proposed machine learning approach?", "Abstract RNN and LSTM"], ["What are the specific expressions or examples of human language used as input for this machine learning model?", "Abstract layman's language"], ["Which programming languages are targeted for translation by the machine learning model?", "Abstract targeted programming language"], ["How was the 74.40% accuracy achieved in the machine translation model?", "Abstract 74.40% accuracy"], ["What additional techniques are suggested for improving the accuracy of the machine translation model?", "Abstract additional techniques"], ["What are the implications of this research for future programming practices?", "Abstract removing human-computer language barrier"]]}
{"id": "1910.09399", "questions": [["What are the earlier research methods for image synthesis?", "word to image correlation"], ["How have deep learning methods impacted text-to-image synthesis?", "deep learning (DL)"], ["What are the key concepts in text-to-image synthesis, such as GANs and DCNNs?", "generative adversarial networks (GANs), deep convolutional encoder-decoder neural networks (DCNN)"], ["What are the four major categories of GAN based text-to-image synthesis?", "Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs"], ["What are the main objectives of each group in the proposed taxonomy?", "main objective of each group"], ["How do the GAN architectures differ in each group?", "typical GAN architectures"], ["What are some examples of categories where GANs and DCNNs have generated impressive results?", "human faces, birds, flowers, room interiors, object reconstruction from edge maps (games)"], ["What are the challenges that remain unresolved in text-to-image synthesis?", "challenges that remain unresolved"], ["What developments can be expected in the future for the text-to-image synthesis domain?", "future developments"]]}
{"id": "1904.05584", "questions": [["What are the different ways of combining character and word-level representations?", "combining character and word-level representations"], ["How does modeling characters improve learned representations?", "modeling characters improves"], ["Why is representing less frequent words particularly useful?", "less frequent words"], ["What is a feature-wise sigmoid gating mechanism?", "sigmoid gating mechanism"], ["How was the performance of the gating mechanism evaluated?", "word similarity datasets"], ["Why doesn't capturing semantic similarity at the word level consistently improve sentence-level tasks?", "semantic similarity"], ["What are the specific downstream sentence-level tasks mentioned in the study?", "downstream sentence-level tasks"], ["What are the main findings and conclusions of the study?", "our findings suggest"]]}
{"id": "1911.09886", "questions": [["What are relation tuples?", "relation tuple"], ["What challenges arise when entities are shared among relation tuples?", "overlapping entities"], ["How do previous pipeline approaches work for entity and relation extraction?", "pipeline approach"], ["What are the two proposed approaches for using encoder-decoder architecture in this research?", "two approaches"], ["How does the representation scheme for relation tuples enable the decoder to generate one word at a time?", "representation scheme"], ["How does the pointer network-based decoding approach work?", "pointer network-based"], ["What are the main differences between the first approach and the pointer network-based approach?", "two approaches"], ["What dataset was used for the experiments in this research?", "New York Times"], ["How do the proposed approaches compare to previous work in terms of performance and F1 scores?", "outperform previous work"]]}
{"id": "1611.01400", "questions": [["What are the challenges in finding related published articles in the biomedical domain?", "especially challenging"], ["How do existing methodologies identify related articles?", "text similarity metrics"], ["What makes biomedical knowledge discovery hypothesis-driven?", "hypothesis-driven"], ["How was the expert-annotated document-ranking corpus developed?", "crowd-sourcing approach"], ["What are the limitations of using text similarity for ranking related articles?", "text similarity"], ["What is the new supervised model for ranking related scientific articles?", "supervised model"], ["How does the authors' ranking differ from text-similarity-based models?", "authors' ranking"], ["What is the best supervised learning-to-rank model found in the study?", "SVM-Rank"], ["How does the SVM-Rank model compare to state-of-the-art baseline systems?", "significantly surpassed"]]}
{"id": "1808.05077", "questions": [["What is the main goal of sentiment analysis?", "determine subject's sentiment"], ["How does deep learning improve sentiment analysis accuracy?", "powerful machine learning technique"], ["Why has limited work been done on deep learning for non-English languages?", "limited work"], ["What are the two deep learning models implemented in this study?", "two deep learning models"], ["How does the performance of deep autoencoders and CNNs compare to the shallow MLP model?", "compared with the state-of-the-art"], ["What is the size and nature of the Persian movie reviews dataset?", "novel Persian movie reviews dataset"], ["What are the specific simulation results that demonstrate deep learning's superior performance?", "Simulation results"]]}
{"id": "1807.03367", "questions": [["What is the purpose of the \"Talk The Walk\" dataset?", "large-scale dialogue dataset"], ["How do the two agents (guide and tourist) communicate?", "communicate via natural language"], ["What is the common goal of the guide and tourist?", "navigate to a given target location"], ["What are the specific challenges of the task and dataset?", "challenging and their full solution"], ["What is the Masked Attention for Spatial Convolutions (MASC) mechanism?", "Masked Attention for Spatial Convolutions"], ["How does MASC help in grounding tourist utterances?", "grounding tourist utterances"], ["What improvements are observed using the MASC mechanism?", "significant improvements"], ["How do the baselines for the full task compare to other methods?", "non-trivial baselines"]]}
{"id": "1907.02030", "questions": [["What is the main motivation for developing this method?", "factchecking process"], ["How does this method improve the efficiency of factchecking?", "increase the efficiency"], ["What are the key components of the proposed method?", "Natural Language Processing"], ["How does the system handle incoming claims?", "compare incoming claims"], ["What is the basis for comparison between claims and the existing corpus?", "semantically-similar"], ["How does the live system benefit factcheckers?", "work simultaneously"], ["What role does NLP play in the proposed method?", "latest developments in NLP"], ["How does the method address the issue of duplicated work among factcheckers?", "without duplicating their work"], ["Are there any limitations or challenges mentioned in the full text of the paper?", "propose a method"]]}
{"id": "1910.04601", "questions": [["What are the annotation artifacts and biases in current datasets?", "annotation artifacts"], ["How do RC systems cheat using simple heuristics?", "simple heuristics"], ["What is semantic type consistency?", "semantic type consistency"], ["How is RC-QED designed to evaluate reasoning in RC systems?", "RC-QED"], ["What does the RC-QED benchmark dataset consist of?", "benchmark dataset"], ["How many answers and natural language derivations are included in the RC-QED dataset?", "12,000 answers"], ["How does RC-QED prevent simple heuristics from being effective?", "robust to simple heuristics"], ["How do state-of-the-art neural path ranking approaches perform on the RC-QED benchmark?", "neural path ranking"]]}
{"id": "1912.05066", "questions": [["What is the focus of most current work in sentiment analysis of microblog feeds?", "tweet sentiment classification"], ["How is crowd wisdom utilized in predicting event outcomes?", "predicting outcomes"], ["What methods are used for sentiment classification of tweets in this research?", "multi-label classification"], ["Which specific events are considered for outcome prediction in this study?", "US Presidential Debate winners, Grammy Award winners, Super Bowl Winners"], ["How does the crowd's wisdom compare to the opinions of experts in the field?", "wisdom of the crowd"], ["In what cases does the crowd's opinion not match with the experts' opinions?", "particularly in the case of debates"], ["How is the crowd's opinion influenced by experts?", "influenced by the experts"]]}
{"id": "1910.03891", "questions": [["What are the limitations of existing knowledge graph embedding methods?", "existing method"], ["How does KANE address the shortcomings of current methods?", "novel knowledge graph"], ["What is the role of graph convolutional networks in KANE?", "graph convolutional networks"], ["How does KANE capture high-order structural relationships?", "high-order structural"], ["What are the benefits of incorporating attribute information into knowledge graph embeddings?", "attribute information"], ["How does the attention mechanism contribute to KANE's performance?", "attention mechanism"], ["What are the three datasets used for empirical evaluation?", "three datasets"], ["How does KANE compare to the seven state-of-art methods?", "outperforms seven"]]}
{"id": "1610.00879", "questions": [["What is the motivation behind automatic drunk-texting prediction?", "unsociable behavior"], ["How were the tweets labeled for the study?", "hashtags as distant supervision"], ["What types of features were used in the classifiers?", "N-gram and stylistic features"], ["How effective were the classifiers in detecting drunk tweets?", "detect drunk tweets"], ["What is the significance of the findings in terms of quantitative evidence?", "first quantitative evidence"], ["Are there any potential applications of this automatic prediction system?", "automatic drunk-texting prediction"], ["What are the challenges or limitations faced in this approach?", "computational approach"]]}
{"id": "1704.05572", "questions": [["What is the main limitation of using Open IE knowledge for QA?", "simple questions"], ["How does the proposed method overcome this limitation?", "reasoning with Open IE"], ["What is the support graph optimization framework used for?", "support graph optimization"], ["What are the key features of the new inference model for Open IE?", "new inference model"], ["How does the model handle multiple short facts, noise, and relational structure of tuples?", "short facts, noise"], ["How does the model's performance compare to the state-of-the-art structured solver?", "significantly outperforms"], ["What advantages does the model have in terms of reliance on manually curated knowledge?", "removing the reliance"]]}
{"id": "1804.10686", "questions": [["What is Watasense?", "unsupervised system"], ["How does Watasense determine the most relevant sense of a word?", "semantic similarity"], ["What are the two modes of operation for Watasense?", "sparse mode"], ["How does the sparse mode function?", "vector space model"], ["How does the dense mode function?", "synset embeddings"], ["What are the three lexical semantic resources used for evaluation?", "resources for Russian"], ["How does the dense mode compare to the sparse mode in terms of performance?", "dense mode substantially outperforms"], ["What is the adjusted Rand index?", "adjusted Rand index"]]}
{"id": "1707.03904", "questions": [["What is the purpose of Quasar-S and Quasar-T datasets? \n2. How were the cloze-style queries in Quasar-S created? \n3. What is the source of posts and comments for the Quasar-S dataset? \n4. How were the trivia questions in Quasar-T obtained? \n5. What serves as the background corpus for Quasar-T dataset? \n6. What are the two subtasks of factoid Question Answering? \n7. What does the retrieval system described in the paper do? \n8. How do the baseline models perform compared to human performance? \n9. Where can the datasets be accessed?", "two new large-scale datasets"]]}
{"id": "1911.07228", "questions": [["What are the primary errors in Vietnamese NER systems?", "primary errors"], ["How do BLSTM-CNN-CRF and BLSTM-CRF models perform on Vietnamese NER dataset?", "conducting experiments"], ["What are the different word embeddings used in the experiments?", "different word embeddings"], ["What makes BLSTM-CNN-CRF model better than BLSTM-CRF model?", "BLSTM-CNN-CRF gives better results"], ["How can the error analysis results help improve NER performance for Vietnamese language?", "thorough insights"], ["What specific improvements can be made to the corpus for future works?", "improve the quality"]]}
{"id": "1603.07044", "questions": [["What is the general recurrent neural network (RNN) encoder framework used for community question answering?", "RNN encoder framework"], ["How does the absence of linguistic processing affect the model's applicability?", "without linguistic processing"], ["How does the neural attention mechanism improve the RNN encoders?", "neural attention mechanism"], ["What specific transfer learning techniques are used to address data sparsity and imbalanced labels?", "transfer learning"], ["How is multitask learning implemented in the approach?", "multitask learning"], ["What were the results of the experiments on the SemEval-2016 cQA task?", "SemEval-2016 cQA"], ["How does the performance of the proposed method compare to an information retrieval-based approach and a handcrafted feature-based method?", "comparable performance"]]}
{"id": "1902.09314", "questions": [["What is targeted sentiment classification?", "Targeted sentiment classification"], ["What are the limitations of RNNs in this context?", "RNNs are difficult"], ["How does the Attentional Encoder Network address the limitations of RNNs?", "Attentional Encoder Network"], ["What is label unreliability issue?", "label unreliability issue"], ["How does label smoothing regularization work?", "label smoothing regularization"], ["How is pre-trained BERT applied in this task?", "apply pre-trained BERT"], ["What are the new state-of-the-art results achieved?", "new state-of-the-art results"], ["Can you provide details of the experiments and analysis conducted?", "Experiments and analysis"], ["How does the lightweight nature of the model benefit the overall performance?", "lightweight of our model"]]}
{"id": "1904.03339", "questions": [["What is the main purpose of JESSI in SemEval 2019 Task 9?", "Stable Suggestion Inference"], ["What are the two sentence encoders used in JESSI?", "two sentence encoders"], ["What are the pre-trained word embeddings used in the first encoder?", "GloVe and CoVe"], ["What is the pre-trained deep bidirectional transformer used in the second encoder?", "BERT"], ["How is the domain adversarial training module used in the system?", "training for out-of-domain samples"], ["What issues were observed with BERT for out-of-domain samples?", "unstable for out-of-domain samples"], ["What are the two approaches used to mitigate the instability of BERT?", "combining BERT with a non-BERT encoder and using an RNN-based classifier"], ["What were the F-Score results for Subtask A and Subtask B?", "77.78\\% F-Score and 79.59\\% F-Score"], ["Did the final models use any additional external data?", "without using any additional external data"]]}
{"id": "1910.11769", "questions": [["What is the purpose of the DENS dataset?", "multi-class emotion analysis"], ["What sources were used for collecting the dataset?", "Project Gutenberg, Wattpad"], ["How were the narratives annotated?", "Amazon Mechanical Turk"], ["What statistics and benchmarks are provided for DENS?", "statistics and baseline benchmarks"], ["Which model achieved the best results in the experiments?", "pre-trained BERT"], ["What was the average micro-F1 score for the best performing model?", "60.4%"], ["How does the DENS dataset contribute to emotion analysis research?", "novel opportunity"], ["What is the limitation of existing sentence-level techniques?", "moving beyond existing sentence-level techniques"]]}
{"id": "1702.06378", "questions": [["What are segmental conditional random fields and connectionist temporal classification used for in speech recognition models?", "sequence labeling methods"], ["How do both models define transcription probability?", "latent segmentation alternatives"], ["What is the difference between SCRF and CTC in defining sequence probability?", "segment labels and durations and output symbol or a 'continuation'"], ["How is the recognition model trained in this study?", "optimizing an interpolation"], ["What is the role of the recurrent neural network encoder in the proposed method?", "feature extraction for both outputs"], ["What is the impact of the multitask objective on recognition accuracy?", "improves recognition accuracy"], ["How does CTC pretraining affect the RNN encoder's convergence rate?", "improves the convergence rate"], ["What are the advantages of learning the joint model with CTC pretraining?", "learning the joint model"]]}
{"id": "1903.03467", "questions": [["What is the main problem addressed by the study?", "incorrect translations"], ["How does the black-box approach work?", "black-box approach"], ["What is the specific translation task used for evaluation?", "English to Hebrew"], ["How significant is the improvement in translation accuracy?", "up to 2.3 BLEU"], ["What are the components of the pre-trained neural machine translation system used?", "pre-trained neural machine"], ["How does the method control morphological variations in generated translations?", "control the morphological variations"], ["What is the impact of supplying correct gender and number information on translation accuracy?", "improves the translation accuracy"], ["What are the findings from the fine-grained syntactic analysis?", "fine-grained syntactic analysis"], ["Are there any limitations or challenges in implementing the proposed method?", "our method"], ["How does the proposed method compare to other existing methods for addressing the same problem?", "our method"]]}
{"id": "1807.00868", "questions": [["What is the Babel data used for Turkish spontaneous speech?", "Babel data"], ["Which neural network architectures were investigated in this research?", "different neural network architectures"], ["How does the fully-convolutional architecture perform compared to others?", "fully-convolutional"], ["What are the advantages of using ResNet with GRU?", "ResNet with GRU"], ["Which features and normalization techniques were compared in the study?", "features and normalization techniques"], ["How does the proposed CTC-loss modification work?", "CTC-loss modification"], ["What improvements were observed when decoding with small beam size?", "small beam size"], ["How does the best model's word error rate compare to other end-to-end systems?", "word error rate"], ["Are there any limitations or challenges faced with the best model?", "best model"], ["What future improvements or research directions can be explored for low-resource speech recognition?", "low-resource speech recognition"]]}
{"id": "1909.13375", "questions": [["What is the DROP dataset?", "DROP dataset"], ["How do multi-span questions differ from traditional reading comprehension questions?", "multi-span questions"], ["What is the previous approach used for answering span questions?", "previous approaches"], ["How does the new sequence tagging approach work?", "sequence tagging"], ["What are the specific improvements in EM and F1 scores achieved by the new approach?", "29.7 EM and 15.1 F1"], ["How does this new approach impact performance on other question types?", "not hurting performance"], ["What is the current state-of-the-art result on the entire DROP dataset?", "current state-of-the-art"]]}
{"id": "1909.00430", "questions": [["What is expectation regularization (XR)?", "expectation regularization (XR)"], ["How does the proposed method apply XR for transfer learning?", "novel application"], ["How does knowing the labels of task A help in estimating the label proportion of task B?", "knowing the labels"], ["How is the XR framework made applicable to large-scale deep-learning setups?", "stochastic batched approximation"], ["What is Aspect-based Sentiment classification?", "Aspect-based Sentiment classification"], ["How does the method improve upon fully supervised neural systems?", "improves upon fully supervised"], ["How does the approach compare to LM-based pretraining?", "cumulative with LM-based"], ["What are the results of improving a BERT-based Aspect-based Sentiment model?", "improving a BERT-based"]]}
{"id": "1910.11493", "questions": [["What is the main goal of the SIGMORPHON 2019 shared task?", "cross-lingual transfer"], ["How does the first task differ from previous years' inflection tasks?", "evolves past years"], ["What is the new second challenge presented this year?", "lemmatization and morphological feature analysis"], ["What components did all the submissions share in their methodology?", "neural component"], ["What were the baselines for the inflection task and the contextual analysis task?", "strong baselines"], ["How did the participating teams perform compared to the baselines for the inflection task?", "improved in accuracy"], ["How did the participating teams perform compared to the baselines for the contextual analysis task?", "improved on both"]]}
{"id": "1910.00912", "questions": [["What is the new neural architecture developed for Natural Language Understanding in Spoken Dialogue Systems?", "Hierarchical Multi-Task"], ["How does the hierarchical multi-task architecture represent sentence meaning?", "multi-layer representation"], ["What are the key components in the architecture?", "self-attention mechanisms"], ["What type of experiments were conducted to evaluate the architecture?", "variety of experiments"], ["How does the approach perform on a dataset annotated with Dialogue Acts and Frame Semantics?", "promising results"], ["What is the applicability of this approach to domain-specific intents and semantic roles?", "publicly available NLU dataset"], ["How does the performance of this new architecture compare to existing tools like RASA, Dialogflow, LUIS, and Watson?", "higher than state-of-the-art"], ["What is the improvement in entity tagging F-score over Rasa, Dialogflow, and LUIS?", "average 4.45% improvement"]]}
{"id": "1908.10449", "questions": [["What issues do existing MRC models face in real-world applications?", "do not scale"], ["How do the characteristics of MRC datasets contribute to these issues?", "nature of MRC datasets"], ["What method is proposed to reframe existing MRC datasets as interactive environments?", "simple method"], ["How is the majority of a document's text \"occluded\" in this method?", "occlude the majority"], ["What are the context-sensitive commands that reveal glimpses of the hidden text?", "context-sensitive commands"], ["How were SQuAD and NewsQA repurposed as an initial case study?", "repurpose SQuAD and NewsQA"], ["How does the interactive corpora train a model for sequential decision making?", "train a model"], ["In what ways can this setting contribute to scaling models for web-level QA scenarios?", "scaling models to web-level"]]}
{"id": "1910.03814", "questions": [["What is the main goal of this research?", "hate speech detection"], ["How was the MMHS150K dataset created and what does it consist of?", "gather and annotate"], ["What types of models were proposed for this task?", "propose different models"], ["How do multimodal models compare to unimodal models in detecting hate speech?", "comparing them with unimodal"], ["What were the quantitative and qualitative results of the study?", "quantitative and qualitative results"], ["How useful are images for hate speech detection?", "images are useful"], ["Why can't current multimodal models outperform text-only models?", "cannot outperform models"], ["What are the challenges faced in this task?", "analyze the challenges"], ["How does this study contribute to the field of hate speech detection?", "open the field"]]}
{"id": "1701.00185", "questions": [["What makes short text clustering challenging?", "Abstract sparseness of text"], ["How does the proposed STC^2 framework incorporate useful semantic features?", "Abstract Self-Taught Convolutional"], ["What unsupervised dimensionality reduction methods are used for embedding raw text features into binary codes?", "Abstract unsupervised dimensionality reduction"], ["How are word embeddings explored and used in the convolutional neural networks?", "Abstract word embeddings"], ["What is the role of the output units in the training process?", "Abstract fit the pre-trained binary codes"], ["How are the optimal clusters obtained?", "Abstract employing K-means"], ["What are the three public short text datasets used for testing the proposed framework?", "Abstract three public short text datasets"], ["How does the performance of the proposed framework compare to other popular clustering methods?", "Abstract outperform several popular clustering methods"]]}
{"id": "1912.00871", "questions": [["What were the limitations of prior machine learning attempts in solving arithmetic word problems?", "Prior attempts"], ["How do Transformer networks help in solving arithmetic word problems?", "Transformer networks"], ["What notations are used for representing arithmetic expressions in this approach?", "Infix, prefix, and postfix notations"], ["How does pre-training on a general text corpus improve performance?", "Foundational language abilities"], ["What neural configurations were compared in this study?", "Neural configurations"], ["How significant is the increase in accuracy using the proposed method?", "Over 20 percentage points"], ["How does the best neural approach compare to the previous state of the art?", "Boost accuracy by almost 10%"]]}
{"id": "1912.03234", "questions": [["What is the importance of humor in the interaction with voice-controlled virtual assistants?", "emotional and personalized experience"], ["What are the methods used to improve the joke skill of a VVA?", "methods used"], ["How do traditional NLP techniques contribute to the improvement?", "traditional NLP techniques"], ["What benefits do the self-attentional network and multi-task learning approaches provide?", "better results"], ["What are the challenges faced by these systems in terms of user feedback?", "lack of explicit user feedback"], ["What are the two implicit feedback-based labelling strategies explored?", "two implicit feedback-based labelling strategies"], ["How were the models evaluated?", "real production data"], ["What are the online and offline results in terms of user satisfaction and joke experience?", "real-world impact on user satisfaction"], ["How does the deep-learning approach compare to the other methods considered?", "deep-learning approaches"]]}
{"id": "1911.11750", "questions": [["What are the diverse advances in information extraction?", "diverse advances"], ["How does the technique using SRCC work for KE?", "technique is based"], ["How is information extraction performed in computing environments?", "computing environments"], ["What are the challenges in extracting knowledge from textual documents?", "knowledge extraction (KE)"], ["How does the proposed method improve clustering of similar TDs?", "group such similar"], ["What are the common characteristics considered for KE among TDs?", "common characteristics"], ["How were the experiments conducted to test the SRCC-based technique?", "conducted experiments"], ["What makes SRCC a comprehensive measure for high-quality KE?", "comprehensive measure"]]}
{"id": "1911.03894", "questions": [["What are pretrained language models?", "Pretrained language models"], ["What is the main limitation of most available models?", "very limited"], ["How is CamemBERT different from other language models?", "French version"], ["What is the Bi-directional Encoders for Transformers (BERT)?", "Transformers (BERT)"], ["What downstream tasks were used to measure CamemBERT's performance?", "downstream tasks"], ["How does CamemBERT compare to multilingual models in performance?", "compared to multilingual models"], ["In which tasks does CamemBERT improve the state of the art?", "state of the art"], ["What are the potential applications for French NLP using CamemBERT?", "downstream applications"]]}
{"id": "2001.09899", "questions": [["What is the importance of identifying controversial topics in social media?", "social point of view"], ["How does the proposed method help in avoiding information segregation?", "avoid the information segregation"], ["What are the key features of the vocabulary-based controversy detection method?", "language-agnostic, efficient"], ["How did the researchers test their method across different languages and regions?", "many languages, regions"], ["How does the vocabulary-based measure compare to other measures in terms of performance?", "performs better"], ["Can the method detect polarization through text analysis?", "detect polarization"], ["What are some examples of controversial and non-controversial topics used in the experiments?", "controversial and non-controversial topics"], ["How is the method designed to be easy to apply?", "easy to apply"], ["What are the limitations of state-of-the-art measures that rely on community graph structure?", "community graph structure"], ["How does the vocabulary-based method contribute to creating better discussion contexts?", "better discussion contexts"]]}
{"id": "1710.01492", "questions": [["What is the significance of studying public opinion in social media?", "study public opinion"], ["How has the abundance of social media data attracted various fields of interest?", "attracted business and research"], ["What are the main questions driving sentiment analysis research in social media?", "questions like these"], ["Why is Twitter a popular platform for sentiment analysis research?", "especially popular for research"], ["What factors make Twitter suitable for sentiment analysis studies?", "scale, representativeness, variety"], ["What will be the focus of the presented overview on sentiment analysis on Twitter?", "present an overview"]]}
{"id": "1912.01673", "questions": [["What is the purpose of COSTRA 1.0?", "sentence-level embeddings"], ["How many sentences are included in the dataset?", "4,262 unique sentences"], ["What is the average length of sentences in the dataset?", "average length of 10 words"], ["How many types of modifications are illustrated in the dataset?", "15 types of modifications"], ["What are some examples of modifications included in the dataset?", "simplification, generalization"], ["What are the potential applications of this dataset?", "test semantic properties"], ["What is the significance of finding a \"skeleton\" in the sentence embedding space?", "topologically interesting 'skeleton'"], ["Does the dataset include both formal and informal language variations?", "formal and informal language variation"]]}
{"id": "1909.12231", "questions": [["What are the challenges in multi-document summarization?", "Abstract language used to express"], ["What are the limitations of existing approaches?", "Abstract heavily rely on"], ["How does the novel method overcome these limitations?", "Abstract two types of"], ["What are universal embeddings?", "Abstract universal embeddings"], ["What are domain-specific embeddings?", "Abstract domain-specific embeddings"], ["How is SemSentSum developed?", "Abstract we develop SemSentSum"], ["What type of sentence embeddings does SemSentSum leverage?", "Abstract both types of"], ["What are the results of using SemSentSum on two types of summaries?", "Abstract competitive results"], ["How does SemSentSum compare to state-of-the-art models?", "Abstract Unlike other state-of-the-art"], ["In what other tasks can the method be adaptable?", "Abstract easily adaptable for"], ["Are there any other approaches using multiple sentence embeddings for multi-document summarization?", "Abstract first to use"]]}
{"id": "1706.08032", "questions": [["What is the novel deep learning framework introduced in this paper?", "deep learning framework"], ["How does the lexicon-based approach contribute to sentiment label prediction?", "lexicon-based approach"], ["What semantic rules are applied in the framework?", "semantic rules"], ["How do Deep Convolutional Neural Networks help in increasing information for word-level embedding?", "DeepCNN"], ["What is the role of Bidirectional Long Short-Term Memory Network in the framework?", "Bi-LSTM"], ["Which three Twitter sentiment classification datasets were used for evaluation?", "three Twitter datasets"], ["How much improvement in classification accuracy was observed in the experiments?", "improve classification accuracy"], ["How does this framework compare to other existing methods for sentence-level sentiment analysis in Twitter social networking?", "sentiment analysis in Twitter"]]}
{"id": "1811.01399", "questions": [["What are the limitations of previous knowledge graph embedding methods?", "all entities should be seen"], ["How do neighborhood aggregators help in embedding new entities?", "embed new entities inductively"], ["What are the issues with existing neighborhood aggregators?", "neglect unordered and unequal"], ["What are the desired properties for effective neighborhood aggregators?", "desired properties"], ["How does the Logic Attention Network (LAN) improve neighborhood aggregation?", "Logic Attention Network"], ["What are the rules- and network-based attention weights in LAN?", "rules- and network-based attention weights"], ["How does LAN's performance compare to conventional aggregators in knowledge graph completion tasks?", "LAN's superiority"], ["What are the specific knowledge graph completion tasks used in the experiments?", "two knowledge graph completion tasks"]]}
{"id": "1909.00124", "questions": [["What are the challenges of training deep neural networks with noisy labels?", "noisy labels"], ["How does the proposed NetAb model address the issue of learning with noisy labels?", "NetAb model"], ["What are the key components of the NetAb model?", "two convolutional neural networks"], ["How does the noise transition layer function within the NetAb model?", "noise transition layer"], ["What are the specific loss functions used for training the two networks in the NetAb model?", "respective loss functions"], ["How does the mutual reinforcement training process work for the two networks in the NetAb model?", "mutual reinforcement manner"], ["What are the experimental results that demonstrate the effectiveness of the proposed model?", "Experimental results"], ["How does the performance of the NetAb model compare to other DNN models for sentence-level sentiment classification?", "sentence-level sentiment classification"]]}
{"id": "1909.00088", "questions": [["What is the purpose of semantic text exchange?", "text data augmentation"], ["How can semantic text exchange benefit chatbots and virtual assistants?", "semantic correction"], ["What are the main components of the SMERTI pipeline?", "entity replacement, similarity masking, and text infilling"], ["What is the Semantic Text Exchange Score (STES)?", "Semantic Text Exchange Score"], ["How is the masking (replacement) rate threshold used in this method?", "adjustable parameter"], ["In what domains were the experiments conducted?", "Yelp reviews, Amazon reviews, and news headlines"], ["How does SMERTI compare to the baseline models in terms of performance?", "outperform baseline models"]]}
{"id": "1911.01799", "questions": [["What is the main goal of conducting speaker recognition in unconstrained conditions?", "unconstrained conditions"], ["Why do most publicly available datasets not meet the requirements for unconstrained speaker recognition research?", "constrained environments"], ["What are the specific features of the CN-Celeb dataset in terms of size and diversity?", "1,000 Chinese celebrities"], ["How many different genres are included in the CN-Celeb dataset?", "11 different genres"], ["What are the two state-of-the-art speaker recognition approaches used in the experiments?", "i-vector and x-vector"], ["How does the performance of these approaches on CN-Celeb compare to their performance on VoxCeleb?", "far inferior"], ["What is the potential impact of the real-life conditions on the performance of existing speaker recognition techniques?", "much worse"], ["How can researchers access the CN-Celeb dataset?", "free for researchers"]]}
{"id": "1812.06705", "questions": [["What is the main goal of the proposed data augmentation method?", "conditional BERT contextual augmentation"], ["How does data augmentation help deep neural network models?", "improve generalization"], ["What is the difference between BERT and conditional BERT in this paper?", "retrofit BERT"], ["How does the conditional masked language model task relate to BERT?", "conditional masked language model"], ["What are the benefits of using the well-trained conditional BERT for contextual augmentation?", "enhance contextual augmentation"], ["On which text classification tasks were the experiments conducted?", "six various different text classification tasks"], ["How does the method improve convolutional or recurrent neural networks classifiers?", "obvious improvement"]]}
